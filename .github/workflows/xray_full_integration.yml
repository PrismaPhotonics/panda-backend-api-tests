name: Xray Full Integration - Test Execution

on:
  workflow_dispatch:
    inputs:
      test_plan:
        description: 'Test Plan Key (e.g., PZ-14024)'
        required: false
        default: 'PZ-14024'
      environment:
        description: 'Test Environment'
        required: false
        default: 'Staging'
        type: choice
        options:
          - Staging
          - Production
      run_all_tests:
        description: 'Run all tests (ignore Test Plan filter)'
        required: false
        default: false
        type: boolean
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    # Run nightly at 2 AM Israel time (midnight UTC)
    - cron: '0 0 * * *'

env:
  TEST_PLAN: ${{ github.event.inputs.test_plan || 'PZ-14024' }}
  TEST_ENV: ${{ github.event.inputs.environment || (github.ref == 'refs/heads/main' && 'Production' || 'Staging') }}
  PROJECT_KEY: PZ
  RUN_ALL_TESTS: ${{ github.event.inputs.run_all_tests || 'false' }}

jobs:
  test-execution:
    runs-on: ubuntu-latest
    name: Test Execution - ${{ env.TEST_ENV }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for revision tracking
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install --no-cache-dir --prefer-binary -r requirements.txt
          # Install pytest-xray if available (optional)
          pip install pytest-xray || echo "pytest-xray not available, using manual markers"
      
      - name: Get Xray authentication token
        id: xray-auth
        env:
          XRAY_CLIENT_ID: ${{ secrets.XRAY_CLIENT_ID }}
          XRAY_CLIENT_SECRET: ${{ secrets.XRAY_CLIENT_SECRET }}
        run: |
          TOKEN=$(python - << 'PY'
import os
import requests
import json

client_id = os.getenv("XRAY_CLIENT_ID")
client_secret = os.getenv("XRAY_CLIENT_SECRET")

response = requests.post(
    "https://xray.cloud.getxray.app/api/v2/authenticate",
    json={"client_id": client_id, "client_secret": client_secret}
)
response.raise_for_status()
token = response.text.strip('"')
print(token)
PY
)
          echo "token=$TOKEN" >> $GITHUB_OUTPUT
          echo "‚úÖ Xray authentication successful"
      
      - name: Fetch test keys from Test Plan
        id: test-plan
        if: env.RUN_ALL_TESTS != 'true'
        env:
          TOKEN: ${{ steps.xray-auth.outputs.token }}
          TEST_PLAN: ${{ env.TEST_PLAN }}
        run: |
          echo "Fetching tests from Test Plan: $TEST_PLAN"
          
          RESPONSE=$(python - << 'PY'
import os
import requests
import json

token = os.getenv("TOKEN")
test_plan = os.getenv("TEST_PLAN")

query = """
query GetTestPlanTests($testPlanKey: String!, $limit: Int!) {
    getTestPlan(issueIdOrKey: $testPlanKey) {
        issueId
        tests(limit: 2000) {
            total
            results {
                issue {
                    key
                }
            }
        }
    }
}
"""

response = requests.post(
    "https://xray.cloud.getxray.app/api/v2/graphql",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    },
    json={
        "query": query,
        "variables": {"testPlanKey": test_plan, "limit": 2000}
    }
)
response.raise_for_status()
result = response.json()

if "errors" in result:
    print(json.dumps({"error": result["errors"]}))
    exit(1)

tests = result.get("data", {}).get("getTestPlan", {}).get("tests", {}).get("results", [])
test_keys = [t["issue"]["key"] for t in tests if t.get("issue", {}).get("key")]

print(json.dumps({"test_keys": test_keys, "count": len(test_keys)}))
PY
)
          
          echo "$RESPONSE" > test_plan_response.json
          TEST_COUNT=$(echo "$RESPONSE" | python -c "import sys, json; print(json.load(sys.stdin)['count'])")
          echo "Found $TEST_COUNT tests in Test Plan"
          
          # Generate pytest expression
          if [ "$TEST_COUNT" -gt 0 ]; then
            TEST_KEYS=$(echo "$RESPONSE" | python -c "import sys, json; print(' or '.join(json.load(sys.stdin)['test_keys']))")
            echo "pytest_expr=$TEST_KEYS" >> $GITHUB_OUTPUT
            echo "test_count=$TEST_COUNT" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è  No tests found in Test Plan, running all tests"
            echo "pytest_expr=True" >> $GITHUB_OUTPUT
            echo "test_count=0" >> $GITHUB_OUTPUT
          fi
      
      - name: Prepare test execution
        id: prep
        run: |
          REVISION="${{ github.sha }}"
          BUILD_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          COMMIT_MSG="${{ github.event.head_commit.message || 'Automated test run' }}"
          
          echo "revision=$REVISION" >> $GITHUB_OUTPUT
          echo "build_url=$BUILD_URL" >> $GITHUB_OUTPUT
          echo "commit_msg<<EOF" >> $GITHUB_OUTPUT
          echo "$COMMIT_MSG" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Run tests
        id: run-tests
        env:
          ENVIRONMENT: ${{ env.TEST_ENV == 'Production' && 'production' || 'staging' }}
        run: |
          mkdir -p reports logs screenshots
          
          # Determine pytest command
          if [ "${{ env.RUN_ALL_TESTS }}" == "true" ] || [ "${{ steps.test-plan.outputs.test_count }}" == "0" ]; then
            echo "Running all tests"
            PYTEST_CMD="pytest be_focus_server_tests/ -v"
          else
            echo "Running tests from Test Plan: ${{ steps.test-plan.outputs.pytest_expr }}"
            # Use pytest-xray if available, otherwise use -k with test keys
            if python -c "import pytest_xray" 2>/dev/null; then
              PYTEST_CMD="pytest be_focus_server_tests/ -v -m '${{ steps.test-plan.outputs.pytest_expr }}'"
            else
              # Fallback: run tests matching Xray markers
              PYTEST_CMD="pytest be_focus_server_tests/ -v -k '${{ steps.test-plan.outputs.pytest_expr }}'"
            fi
          fi
          
          echo "Command: $PYTEST_CMD"
          
          # Run tests with JUnit XML output
          $PYTEST_CMD \
            --junitxml=reports/junit.xml \
            --html=reports/report.html \
            --self-contained-html \
            --tb=short \
            || TEST_EXIT_CODE=$?
          
          # Always continue (we'll upload results even if tests failed)
          exit ${TEST_EXIT_CODE:-0}
      
      - name: Upload test results to Xray
        id: xray-upload
        env:
          XRAY_CLIENT_ID: ${{ secrets.XRAY_CLIENT_ID }}
          XRAY_CLIENT_SECRET: ${{ secrets.XRAY_CLIENT_SECRET }}
          TEST_PLAN: ${{ env.TEST_PLAN }}
          TEST_ENV: ${{ env.TEST_ENV }}
          REVISION: ${{ steps.prep.outputs.revision }}
        run: |
          python scripts/xray_upload.py \
            --format junit \
            --file reports/junit.xml \
            --test-plan "$TEST_PLAN" \
            --environment "$TEST_ENV" \
            --revision "$REVISION" \
            > xray_upload_response.json 2>&1 || true
          
          # Extract Test Execution key from response
          if [ -f xray_upload_response.json ]; then
            TEST_EXEC_KEY=$(python - << 'PY'
import json
import re

try:
    with open('xray_upload_response.json', 'r') as f:
        content = f.read()
        # Try to extract key from JSON response or log output
        match = re.search(r'PZ-EXE-\d+', content)
    if match:
        print(match.group(0))
except:
    pass
PY
)
            if [ -n "$TEST_EXEC_KEY" ]; then
              echo "test_exec_key=$TEST_EXEC_KEY" >> $GITHUB_OUTPUT
              echo "‚úÖ Test Execution created: $TEST_EXEC_KEY"
            fi
          fi
      
      - name: Attach evidence to Test Execution
        if: steps.xray-upload.outputs.test_exec_key != ''
        env:
          TEST_EXEC_KEY: ${{ steps.xray-upload.outputs.test_exec_key }}
        run: |
          if [ -n "$TEST_EXEC_KEY" ]; then
            echo "Attaching evidence to $TEST_EXEC_KEY..."
            
            # Attach logs
            if [ -d "logs" ] && [ "$(ls -A logs)" ]; then
              python scripts/xray/attach_evidence.py \
                --test-exec "$TEST_EXEC_KEY" \
                --evidence logs/ \
                || echo "‚ö†Ô∏è  Failed to attach logs"
            fi
            
            # Attach screenshots
            if [ -d "screenshots" ] && [ "$(ls -A screenshots)" ]; then
              python scripts/xray/attach_evidence.py \
                --test-exec "$TEST_EXEC_KEY" \
                --evidence screenshots/ \
                || echo "‚ö†Ô∏è  Failed to attach screenshots"
            fi
            
            # Attach HTML report
            if [ -f "reports/report.html" ]; then
              python scripts/xray/attach_evidence.py \
                --test-exec "$TEST_EXEC_KEY" \
                --file reports/report.html \
                || echo "‚ö†Ô∏è  Failed to attach HTML report"
            fi
            
            echo "‚úÖ Evidence attachment complete"
          fi
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_id }}
          path: |
            reports/
            logs/
            screenshots/
            xray_upload_response.json
          retention-days: 30
      
      - name: Comment PR with test results
        if: github.event_name == 'pull_request' && steps.xray-upload.outputs.test_exec_key != ''
        uses: actions/github-script@v7
        env:
          TEST_EXEC_KEY: ${{ steps.xray-upload.outputs.test_exec_key }}
          TEST_PLAN: ${{ env.TEST_PLAN }}
          TEST_ENV: ${{ env.TEST_ENV }}
        with:
          script: |
            const fs = require('fs');
            const { execSync } = require('child_process');
            
            // Parse JUnit XML to get test results
            let passed = 0;
            let failed = 0;
            let total = 0;
            
            try {
              const junitXml = fs.readFileSync('reports/junit.xml', 'utf8');
              const testsuites = junitXml.match(/testsuites[^>]*/);
              if (testsuites) {
                const failures = junitXml.match(/failures="(\d+)"/);
                const errors = junitXml.match(/errors="(\d+)"/);
                const tests = junitXml.match(/tests="(\d+)"/);
                
                failed = parseInt(failures?.[1] || 0) + parseInt(errors?.[1] || 0);
                total = parseInt(tests?.[1] || 0);
                passed = total - failed;
              }
            } catch (e) {
              console.log('Could not parse JUnit XML:', e.message);
            }
            
            const jiraBaseUrl = 'https://prismaphotonics.atlassian.net';
            const testExecUrl = `${jiraBaseUrl}/browse/${process.env.TEST_EXEC_KEY}`;
            const testPlanUrl = `${jiraBaseUrl}/browse/${process.env.TEST_PLAN}`;
            
            const comment = `## üß™ Test Execution Results
            
**Environment:** ${process.env.TEST_ENV}
**Test Plan:** [${process.env.TEST_PLAN}](${testPlanUrl})
**Revision:** \`${context.sha.substring(0, 7)}\`

### Results
- ‚úÖ **Passed:** ${passed}
- ‚ùå **Failed:** ${failed}
- üìä **Total:** ${total}

### Links
- üìã [View Test Execution in Xray](${testExecUrl})
- üìù [View Test Plan](${testPlanUrl})
- üîó [View Build](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

---
*Automated by GitHub Actions*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Fail job if tests failed
        if: steps.run-tests.outcome == 'failure'
        run: |
          echo "‚ùå Tests failed - check Test Execution in Xray for details"
          exit 1

