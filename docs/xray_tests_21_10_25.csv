Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project lead id,Project description,Project url,Priority,Resolution,Assignee,Assignee Id,Reporter,Reporter Id,Creator,Creator Id,Created,Updated,Last Viewed,Resolved,Due date,Votes,Description,Environment,Watchers,Watchers Id,Original estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Cloners),Outward issue link (Cloners),Custom field (Actual Results),Custom field (Affected hardware),Custom field (Affected services),Custom field (Analysis),Custom field (Annotation status),Custom field (Approvals),Custom field (Atlassian project),Custom field (Atlassian project status),Custom field (Backout plan),Custom field (Bad Recordings),Custom field (Category),Custom field (Class),Custom field (Comments),Custom field (Countrycode),Custom field (Customer),Custom field (Customer code name),Custom field (Customer real name),Custom field (DataOps:),Custom field (Delivery progress),Custom field (Delivery status),Custom field (Department),Custom field (Department),Custom field (Development),Custom field (Disk Name),Custom field (Due),Custom field (Due Date),Custom field (Entry date),Custom field (Epic Color),Custom field (Epic Name),Custom field (Epic Status),Custom field (Escalated in R&D),Custom field (Expected Result),Custom field (Experiment/Incident End Date),Custom field (Experiment/Incident End Date Time),Custom field (Experiment/Incident Start Date),Custom field (Experiment/Incident Start Date Time),Custom field (Fiber length),Custom field (Focus Areas),Custom field (Folder/s to upload),Custom field (Found by),Custom field (Goals),Custom field (IP),Custom field (Idea archived),Custom field (Idea archived on),Custom field (Implementation plan),Custom field (Insights),Custom field (Installed Version),Custom field (Investigation reason),Custom field (Issue color),Custom field (Known Locations),Custom field (Link to Confluence ),Custom field (Linked items),Custom field (Locked forms),Custom field (Machine Name),Custom field (Major incident),Custom field (Metadata attached),Custom field (Open forms),Custom field (PRR),Custom field (Pending reason),Custom field (Pixel Number),Custom field (Planned end),Custom field (Planned start),Custom field (Prisma Project),Custom field (Processed Data Path),Custom field (Processed Data Path),Custom field (Project ID),Custom field (Project overview key),Custom field (Project overview status),Custom field (ProjectID),Custom field (Rank),Custom field (Raw Data Path),Custom field (Real site name (geographical)),Custom field (Recording Links),Custom field (Region),Custom field (Reproduction Steps),Custom field (Request Type),Custom field (Request language),Custom field (Request participants),Custom field (Responders),Custom field (Review Explanation),Custom field (Reviewed by DataOps),Custom field (Reviewed by dataops),Custom field (Root cause),Satisfaction rating,Custom field (Satisfaction date),Custom field (Secondary Project ID),Custom field (Secondary ProjectID),Custom field (Sentiment),Custom field (Severity),Custom field (Site),Custom field (Site),Custom field (Site code name),Custom field (Site1),Custom field (Skip annotation),Custom field (Source),Custom field (Start date),Custom field (Story Points),Custom field (Story point estimate),Custom field (Submitted forms),Custom field (Target date),Custom field (Target end),Custom field (Target start),Custom field (Task Duration),Custom field (Task progress),Team Id,Team Name,Custom field (Test plan),Custom field (Time to close after resolution),Custom field (Time to done),Custom field (Time to first response),Custom field (Time to resolution),Custom field (Time to review normal change),Custom field (Timezone),Custom field (Total forms),Custom field (Urgency),Custom field (Vulnerability),Custom field (Webapp version),Custom field (Work category),Custom field (Workaround),Custom field (mongoDB file name),Comment,Status Category,Status Category Changed
Stress - Configuration with Extreme Values ,PZ-13880,43430,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:45 PM,21/Oct/25 12:46 PM,21/Oct/25 12:46 PM,,,0,"h3. Summary

Validates that Focus Server can handle configuration requests with extreme (but technically valid) parameter values, such as very large channel ranges, very high NFFT, or very large canvas heights.

h3. Objective

Verify that the server can accept and process configurations with boundary/extreme values without crashes or errors, demonstrating robustness under stress conditions.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{stress-test}}, {{extreme-values}}, {{robustness}}
* *Test Type*: Integration Test (Stress)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-EXTREME-VALUES
* *Description*: Server must handle extreme (but valid) configuration values

h3. Pre-Conditions

# Focus Server is running
# System has sufficient resources to handle large configurations

h3. Test Data

{code:json}{
  ""task_id"": ""config_extreme_values_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 8192,
    ""displayInfo"": {
      ""height"": 5000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 200
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 2000
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Extreme values*: {{nfft=8192}}, {{height=5000}}, {{channels=0-200}}, {{freq=0-2000}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with extreme values|Pydantic validation passes|
|2|Send POST /configure|Status 200, task configured OR status 400 if values exceed system limits|
|3|If accepted, poll GET /waterfall|Status 200/201, server handles request without crash|
|4|Monitor server response time|Response time may be longer but within acceptable limits|
|5|Verify no server errors or crashes|Server remains stable|

h3. Expected Result

* *Option A*: Configuration accepted, server processes successfully (may be slow)
* *Option B*: Configuration rejected with ""exceeds system limits"" error (acceptable)
* *No crashes*: Server remains stable and responsive

h3. Post-Conditions

* Server stable, no resource exhaustion

h3. Assertions (Python Code)

{code:python}# Test function: test_configuration_with_extreme_values

task_id = generate_task_id(""extreme_values"")
logger.info(f""Test: Configuration with extreme values for {task_id}"")

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 8192,  # Very high NFFT
    ""displayInfo"": {""height"": 5000},  # Very tall canvas
    ""channels"": {""min"": 0, ""max"": 200},  # Many channels
    ""frequencyRange"": {""min"": 0, ""max"": 2000},  # Wide frequency range
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

try:
    config_request = ConfigTaskRequest(**config_payload)
    response = focus_server_api.config_task(task_id, config_request)
    
    # If accepted, verify server stability
    assert response.status == ""Config received successfully""
    logger.info(""Extreme values configuration accepted"")
    
    # Try to poll data
    time.sleep(2.0)
    waterfall_response = focus_server_api.get_waterfall(task_id, 5)
    assert waterfall_response.status_code in [200, 201], \
        f""Unexpected status: {waterfall_response.status_code}""
    
    logger.info(""Server stable with extreme values"")

except Exception as e:
    # If rejected, that's also acceptable (system limits)
    error_msg = str(e).lower()
    if ""limit"" in error_msg or ""exceed"" in error_msg or ""too large"" in error_msg:
        logger.info(f""Extreme values rejected (system limits): {e}"")
    else:
        # Unexpected error
        raise{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_configuration_with_extreme_values}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@915fe8d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u6n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:45 PM
Integration – Missing Required Fields,PZ-13879,43429,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:43 PM,21/Oct/25 12:44 PM,21/Oct/25 12:43 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests that are missing required fields (e.g., missing {{channels}}, {{frequencyRange}}, or {{nfftSelection}}).

h3. Objective

Verify proper validation of required fields, ensuring that incomplete configurations are rejected with appropriate error messages.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{required-fields}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-REQUIRED-FIELDS
* *Description*: All required configuration fields must be present

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_missing_fields_<timestamp>"",
  ""config_payload_missing_nfft"": {
    ""displayTimeAxisDuration"": 10,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  },
  ""config_payload_missing_channels"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Missing fields*: {{nfftSelection}} or {{channels}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create payload missing {{nfftSelection}}|Incomplete payload|
|2|Attempt to create {{ConfigureRequest}}|Pydantic {{ValidationError}}|
|3|Verify error message|Error indicates ""field required: nfftSelection""|
|4|Repeat with payload missing {{channels}}|Pydantic {{ValidationError}}|
|5|Verify error message|Error indicates ""field required: channels""|

h3. Expected Result

* *Pydantic ValidationError*: Missing required fields detected before API call
* *Error messages*: Clearly indicate which fields are missing

h3. Post-Conditions

* No API call made (Pydantic validation fails first)

h3. Assertions (Python Code)

{code:python}# Test function: test_missing_required_fields

# Test 1: Missing nfftSelection
config_payload_missing_nfft = {
    ""displayTimeAxisDuration"": 10,
    # ""nfftSelection"": 1024,  # MISSING
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

with pytest.raises(ValidationError) as exc_info:
    ConfigTaskRequest(**config_payload_missing_nfft)

error_msg = str(exc_info.value).lower()
assert ""nfft"" in error_msg or ""required"" in error_msg
logger.info(f""Missing nfftSelection detected: {exc_info.value}"")

# Test 2: Missing channels
config_payload_missing_channels = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {""height"": 1000},
    # ""channels"": {""min"": 0, ""max"": 50},  # MISSING
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

with pytest.raises(ValidationError) as exc_info:
    ConfigTaskRequest(**config_payload_missing_channels)

error_msg = str(exc_info.value).lower()
assert ""channel"" in error_msg or ""required"" in error_msg
logger.info(f""Missing channels detected: {exc_info.value}""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_missing_required_fields}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b1cb5b8,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u6f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:43 PM
Integration – Invalid View Type - Out of Range,PZ-13878,43428,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:42 PM,21/Oct/25 12:42 PM,21/Oct/25 12:42 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests with invalid {{view_type}} values (outside the defined enum range).

h3. Objective

Verify proper validation of {{view_type}}, ensuring it is one of the defined valid values (0=MULTICHANNEL, 1=SINGLECHANNEL).

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{view-type}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-VIEW-TYPE
* *Description*: View type must be a valid enum value (0 or 1)

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_invalid_view_type_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 999
  }
}{code}

*Invalid parameter*: {{view_type = 999}} (valid values are 0, 1)

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{view_type = 999}}|Validation error expected|
|2|Attempt POST /configure|Status 400/422 or Pydantic exception|
|3|Verify error message|Error indicates ""invalid view_type"" or ""not a valid enum""|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400/422 or {{ValidationError}}
* *Error message*: ""view_type must be 0 or 1"" or ""not a valid ViewType""

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_invalid_view_type_out_of_range

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 999  # Invalid: out of range
}

with pytest.raises(Exception) as exc_info:
    config_request = ConfigTaskRequest(**config_payload)
    focus_server_api.config_task(task_id, config_request)

error_msg = str(exc_info.value).lower()
assert ""view_type"" in error_msg or ""viewtype"" in error_msg or ""enum"" in error_msg or ""999"" in error_msg

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_invalid_view_type_out_of_range}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@55818086,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u67:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:42 PM
Integration – Invalid Frequency Range - Min > Max,PZ-13877,43427,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:41 PM,21/Oct/25 12:41 PM,21/Oct/25 12:41 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests where {{frequencyRange.min > frequencyRange.max}}.

h3. Objective

Verify proper validation of frequency range, ensuring that {{min}} must be less than or equal to {{max}}.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{frequency-range}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-FREQ-RANGE
* *Description*: Frequency range must have min <= max

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_invalid_freq_range_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 1000,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Invalid parameter*: {{frequencyRange.min = 1000 > frequencyRange.max = 500}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{frequencyRange.min=1000, frequencyRange.max=500}}|Validation error expected|
|2|Attempt POST /configure|Status 400/422 or exception|
|3|Verify error message|Error indicates ""frequency min > max"" or ""invalid range""|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400/422 or {{ValidationError}}
* *Error message*: ""Frequency min must be <= max"" or similar

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_invalid_frequency_range_min_greater_than_max

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 1000, ""max"": 500},  # Invalid: min > max
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

with pytest.raises(Exception) as exc_info:
    config_request = ConfigTaskRequest(**config_payload)
    focus_server_api.config_task(task_id, config_request)

error_msg = str(exc_info.value).lower()
assert ""frequency"" in error_msg or ""min"" in error_msg or ""max"" in error_msg or ""range"" in error_msg

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_invalid_frequency_range_min_greater_than_max}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1ecfca58,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u5z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:41 PM
Integration – Invalid Channel Range - Min > Max,PZ-13876,43426,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:40 PM,21/Oct/25 12:40 PM,21/Oct/25 12:40 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests where {{channels.min > channels.max}}, which is an invalid range.

h3. Objective

Verify proper validation of channel range, ensuring that {{min}} must be less than or equal to {{max}}.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{channel-range}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-CHANNEL-RANGE
* *Description*: Channel range must have min <= max

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_invalid_channel_range_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 50,
      ""max"": 10
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Invalid parameter*: {{channels.min = 50 > channels.max = 10}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{channels.min=50, channels.max=10}}|Validation error expected|
|2|Attempt POST /configure|Status 400/422 or exception|
|3|Verify error message|Error indicates ""min > max"" or ""invalid range""|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400/422 or {{ValidationError}}
* *Error message*: ""Channel min must be <= max"" or similar

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_invalid_channel_range_min_greater_than_max

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 50, ""max"": 10},  # Invalid: min > max
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

with pytest.raises(Exception) as exc_info:
    config_request = ConfigTaskRequest(**config_payload)
    focus_server_api.config_task(task_id, config_request)

error_msg = str(exc_info.value).lower()
assert ""channel"" in error_msg or ""min"" in error_msg or ""max"" in error_msg or ""range"" in error_msg

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_invalid_channel_range_min_greater_than_max}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2b1cc47c,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u5r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:40 PM
Integration – Invalid NFFT - Negative Value,PZ-13875,43425,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:39 PM,21/Oct/25 12:39 PM,21/Oct/25 12:39 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests with negative {{nfftSelection}} values.

h3. Objective

Verify proper validation and error handling when attempting to configure a task with a negative NFFT value.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{nfft}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-NFFT-VALIDATION
* *Description*: NFFT must be a positive value

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_invalid_nfft_negative_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": -512,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Invalid parameter*: {{nfftSelection = -512}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{nfftSelection = -512}}|Validation error expected|
|2|Attempt POST /configure|Status 400/422 or Pydantic exception|
|3|Verify error message|Error indicates ""negative"" or ""must be positive""|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400/422 or {{ValidationError}}
* *Error message*: ""nfftSelection cannot be negative"" or similar

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_invalid_nfft_negative_value

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": -512,  # Invalid: negative
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

with pytest.raises(Exception) as exc_info:
    config_request = ConfigTaskRequest(**config_payload)
    focus_server_api.config_task(task_id, config_request)

error_msg = str(exc_info.value).lower()
assert ""nfft"" in error_msg or ""negative"" in error_msg or ""positive"" in error_msg

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_invalid_nfft_negative_value}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5cc9a6c9,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u5j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:39 PM
Integration – Invalid NFFT - Zero Value,PZ-13874,43424,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:36 PM,21/Oct/25 12:38 PM,21/Oct/25 12:38 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects configuration requests with {{nfftSelection = 0}}, which is invalid for FFT processing.

h3. Objective

Verify proper validation and error handling when attempting to configure a task with zero NFFT value.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{error-handling}}, {{nfft}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-NFFT-VALIDATION
* *Description*: NFFT must be a positive, non-zero value (typically power of 2)

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""config_invalid_nfft_zero_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 0,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

*Invalid parameter*: {{nfftSelection = 0}}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{nfftSelection = 0}}|Pydantic validation may fail OR server validation fails|
|2|Attempt to send POST /configure|Status 400 or 422 (Validation Error) OR Pydantic exception|
|3|Verify error message|Error indicates ""nfft must be positive"" or similar|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400 or 422 for /configure (or Pydantic {{ValidationError}})
* *Error message*: ""nfftSelection must be greater than 0"" or similar
* *Behavior*: Request rejected

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_invalid_nfft_zero_value

task_id = generate_task_id(""invalid_nfft_zero"")
logger.info(f""Test: Invalid NFFT (zero) for {task_id}"")

config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 0,  # Invalid: zero
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": 0
}

# Expect validation error (Pydantic or server)
with pytest.raises(Exception) as exc_info:
    config_request = ConfigTaskRequest(**config_payload)
    focus_server_api.config_task(task_id, config_request)

error_msg = str(exc_info.value).lower()
assert ""nfft"" in error_msg or ""zero"" in error_msg or ""positive"" in error_msg or ""greater"" in error_msg

logger.info(f""Validation error as expected: {exc_info.value}"")

# Verify task not created
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404, ""Task should not have been created""{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_invalid_nfft_zero_value}}
* *Test File*: {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@47fd29d4,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u5b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:36 PM
integration - Valid Configuration - All Parameters,PZ-13873,43423,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:35 PM,21/Oct/25 12:36 PM,21/Oct/25 12:36 PM,,,0,"h3. Summary

Validates that Focus Server correctly accepts and processes a fully valid configuration request with all parameters properly set.

h3. Objective

Verify that a well-formed configuration request with all required and optional parameters correctly configured is accepted by the server and results in a successful task creation.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{config-validation}}, {{happy-path}}, {{integration}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-CONFIG-VALID
* *Description*: Server must accept valid configurations and create tasks

h3. Pre-Conditions

# Focus Server is running and accessible
# All API endpoints are functional
# Baby Analyzer can process valid configurations

h3. Test Data

{code:json}{
  ""task_id"": ""config_valid_all_params_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 0
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Generate unique task_id|Task ID created|
|2|Create {{ConfigureRequest}} with all valid parameters|Pydantic validation passes|
|3|Validate payload structure|All required fields present, correct types|
|4|Send POST /configure|Status 200, {{status: ""Config received successfully""}}|
|5|Verify response contains expected fields|{{stream_amount}}, {{channel_amount}}, {{channel_to_stream_index}} present|
|6|Send GET /waterfall/{task_id}/10|Status 200 (no data yet) or 201 (data available)|
|7|Verify task was created in server memory|Task exists, consumer registered|

h3. Expected Result

* *Status code*: 200 for /configure
* *Response status*: ""Config received successfully""
* *Task created*: GET /waterfall returns 200 or 201 (not 404)
* *No errors*: No validation errors or server exceptions

h3. Post-Conditions

* Task active and ready to deliver data
* Configuration stored in server memory

h3. Assertions (Python Code)

{code:python}# Test function: test_valid_configuration_all_parameters

task_id = generate_task_id(""valid_config"")
logger.info(f""Test: Valid configuration with all parameters for {task_id}"")

# Create valid payload
config_payload = {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {""height"": 1000},
    ""channels"": {""min"": 0, ""max"": 50},
    ""frequencyRange"": {""min"": 0, ""max"": 500},
    ""start_time"": None,
    ""end_time"": None,
    ""view_type"": ViewType.MULTICHANNEL  # 0
}

# Validate with Pydantic
config_request = ConfigTaskRequest(**config_payload)
assert config_request is not None

# Send configuration
response = focus_server_api.config_task(task_id, config_request)

# Assertions
assert isinstance(response, ConfigTaskResponse)
assert response.status == ""Config received successfully""
assert response.stream_amount is not None
assert response.channel_amount is not None
assert response.channel_to_stream_index is not None

logger.info(f""Task configured successfully: stream_amount={response.stream_amount}"")

# Verify task exists
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code in [200, 201], \
    f""Expected status 200/201, got {waterfall_response.status_code}""

logger.info(""Valid configuration test passed""){code}

h3. Environment

* *Environment Name*: new_production
* *Focus Server*: [https://10.10.100.100/focus-server/|https://10.10.100.100/focus-server/]

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_valid_configuration_all_parameters}}
* *Test File*: {{tests/integration/api/test_config_validation.py}} (to be created) or integrated in existing test files
* *Execution*: {{pytest -m ""integration and api"" -k ""valid_configuration""}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1c66064b,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u53:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:35 PM
Integration – Historic Playback Complete End-to-End Flow,PZ-13872,43422,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:33 PM,21/Oct/25 12:34 PM,21/Oct/25 12:34 PM,,,0,"h3. Summary

Comprehensive end-to-end test for historic playback, covering configuration, polling, data collection, metadata retrieval, and completion verification.

h3. Objective

Verify that a complete historic playback session works correctly from start (configuration) to finish (status 208), demonstrating a full lifecycle workflow.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{end-to-end}}, {{full-flow}}, {{lifecycle}}
* *Test Type*: Integration Test (End-to-End)

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-E2E
* *Description*: Complete historic playback lifecycle must function correctly

h3. Pre-Conditions

# Focus Server is running
# MongoDB contains recorded data
# Baby Analyzer functional

h3. Test Data

{code:json}{
  ""task_id"": ""historic_e2e_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|*Phase 1: Configuration*|*|
|2|Calculate 5-minute time range|Time range calculated|
|3|Send POST /configure|Status 200, task configured|
|4|*Phase 2: Data Polling*|*|
|5|Poll GET /waterfall continuously|Status transitions 200 → 201 → 208|
|6|Collect all data blocks during status 201|Multiple data blocks collected|
|7|Track total rows received|Row count tracked|
|8|*Phase 3: Data Validation*|*|
|9|Verify all timestamps within specified range|All timestamps valid|
|10|Verify timestamp ordering|Sequential, no overlap|
|11|Verify sensor data completeness|All sensors have intensity data|
|12|*Phase 4: Completion*|*|
|13|Wait for status 208|Status 208 received|
|14|Verify playback duration reasonable|Completed within expected time|
|15|*Phase 5: Post-Completion*|*|
|16|Verify total rows > 0|Data was delivered|
|17|Log summary statistics|Statistics logged|

h3. Expected Result

* *Configuration*: Successful (status 200)
* *Data delivery*: Multiple blocks, > 50 rows total
* *Data quality*: All timestamps valid, ordered, sensors complete
* *Completion*: Status 208 within 200 seconds
* *Summary*: Full lifecycle successful

h3. Post-Conditions

* Historic playback completed successfully
* All data quality checks passed

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_playback_complete_e2e_flow

task_id = generate_task_id(""historic_e2e"")

# Phase 1: Configuration
logger.info(""Phase 1: Configuring historic playback"")
end_time_dt = datetime.now()
start_time_dt = end_time_dt - timedelta(minutes=5)
start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(sensors_min=0, sensors_max=50, live=False, duration_minutes=5)
payload[""start_time""] = start_time
payload[""end_time""] = end_time

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**payload))
assert response.status == ""Config received successfully""

# Phase 2: Data Polling
logger.info(""Phase 2: Polling historic data"")
all_rows = []
status_transitions = []
start_poll_time = time.time()

for attempt in range(100):
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    
    if not status_transitions or status_transitions[-1] != waterfall_response.status_code:
        status_transitions.append(waterfall_response.status_code)
    
    if waterfall_response.status_code == 201 and waterfall_response.data:
        for block in waterfall_response.data:
            for row in block.rows:
                all_rows.append(row)
    
    elif waterfall_response.status_code == 208:
        poll_duration = time.time() - start_poll_time
        logger.info(f""Playback completed after {poll_duration:.1f} seconds"")
        break
    
    time.sleep(2.0)

# Phase 3: Data Validation
logger.info(""Phase 3: Validating data quality"")
assert len(all_rows) > 0, ""No rows received""

# Verify timestamp ordering
for i in range(len(all_rows) - 1):
    assert all_rows[i].endTimestamp <= all_rows[i+1].startTimestamp

# Verify sensor data
for row in all_rows:
    assert len(row.sensors) > 0
    for sensor in row.sensors:
        assert len(sensor.intensity) > 0

# Phase 4: Summary
logger.info(f""E2E Historic Playback Summary:"")
logger.info(f""  - Total rows: {len(all_rows)}"")
logger.info(f""  - Status transitions: {status_transitions}"")
logger.info(f""  - Duration: {poll_duration:.1f}s"")
logger.info(""  - All validations passed""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_playback_complete_e2e_flow}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@473e5b16,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u4v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:33 PM
Integration – Historic Playback - Timestamp Ordering Validation,PZ-13871,43421,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:32 PM,21/Oct/25 12:33 PM,21/Oct/25 12:33 PM,,,0,"h3. Summary

Validates that all timestamps in historic playback data are strictly ordered, with no out-of-sequence or overlapping time ranges.

h3. Objective

Verify that timestamps in all waterfall rows are monotonically increasing, ensuring proper temporal ordering of historic data.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{timestamps}}, {{ordering}}, {{quality}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-TIMESTAMP-ORDER
* *Description*: All waterfall rows must have sequential, non-overlapping timestamps

h3. Pre-Conditions

# Focus Server is running
# Historic data available

h3. Test Data

{code:json}{
  ""task_id"": ""historic_timestamp_order_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure historic task|Task configured|
|2|Poll and collect ALL rows|Rows collected|
|3|For each consecutive pair of rows:|*|
|4|→ Verify {{row[i].endTimestamp <= row[i+1].startTimestamp}}|No overlap, strict ordering|
|5|→ Track any violations|No violations found|
|6|Verify all rows passed ordering check|All timestamps valid|

h3. Expected Result

* *Strict ordering*: Every row's endTimestamp <= next row's startTimestamp
* *No violations*: 0 timestamp ordering errors

h3. Post-Conditions

* Timestamp ordering verified

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_timestamp_ordering_validation

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**historic_config_payload))

all_rows = []

for attempt in range(100):
    waterfall_response = focus_server_api.get_waterfall(task_id, 20)
    
    if waterfall_response.status_code == 201 and waterfall_response.data:
        for block in waterfall_response.data:
            for row in block.rows:
                all_rows.append(row)
    
    elif waterfall_response.status_code == 208:
        break
    
    time.sleep(2.0)

# Verify timestamp ordering
violations = 0
for i in range(len(all_rows) - 1):
    if all_rows[i].endTimestamp > all_rows[i+1].startTimestamp:
        logger.error(f""Timestamp violation at row {i}: {all_rows[i].endTimestamp} > {all_rows[i+1].startTimestamp}"")
        violations += 1

assert violations == 0, f""Found {violations} timestamp ordering violations""
logger.info(f""All {len(all_rows)} rows have correct timestamp ordering""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_timestamp_ordering_validation}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@254a5d13,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u4n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:32 PM
Integration – Historic Playback - Future Timestamps,PZ-13870,43420,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:31 PM,21/Oct/25 12:31 PM,21/Oct/25 12:31 PM,,,0,"h3. Summary

Validates that Focus Server properly handles historic playback requests with future timestamps, either rejecting them or gracefully completing with no data.

h3. Objective

Verify error handling or graceful behavior when requesting historic data for a time range in the future (which cannot exist).

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{error-handling}}, {{future-timestamps}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-FUTURE
* *Description*: Historic playback must handle future timestamps gracefully

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""historic_future_timestamps_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_tomorrow>"",
    ""end_time"": ""<yymmddHHMMSS_tomorrow_plus_5min>"",
    ""view_type"": 0
  }
}{code}

*Time Range*: Tomorrow (future)

h3. Steps

||#||Step Description||Expected Result||
|1|Set {{start_time = now() + 1 day}}, {{end_time = now() + 1 day + 5 minutes}}|Future time range|
|2|Send POST /configure|Status 200 (accepted) or 400 (rejected)|
|3|If accepted, poll GET /waterfall|Status 200 (no data) or 208 (no data, completed)|
|4|Verify no data returned|{{data_blocks_received == 0}}|

h3. Expected Result

* *Option A*: Configuration rejected (status 400)
* *Option B*: Configuration accepted, but no data (status 208 quickly)

h3. Post-Conditions

* No data returned for future timestamps

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_future_timestamps

# Create future time range
start_time_dt = datetime.now() + timedelta(days=1)
end_time_dt = start_time_dt + timedelta(minutes=5)

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(sensors_min=0, sensors_max=50, live=False, duration_minutes=5)
payload[""start_time""] = start_time
payload[""end_time""] = end_time

try:
    response = focus_server_api.config_task(task_id, ConfigTaskRequest(**payload))
    
    # If accepted, verify no data
    data_found = False
    for attempt in range(30):
        waterfall_response = focus_server_api.get_waterfall(task_id, 10)
        
        if waterfall_response.status_code == 201:
            data_found = True
            break
        elif waterfall_response.status_code == 208:
            logger.info(""Future timestamps: no data (expected)"")
            break
        
        time.sleep(1.0)
    
    assert not data_found, ""Data should not exist for future timestamps""
    
except Exception as e:
    # Configuration rejected (also acceptable)
    logger.info(f""Future timestamps rejected: {e}""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_future_timestamps}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@93253af,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u4f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:31 PM
Integration – Historic Playback - Invalid Time Range (End Before Start),PZ-13869,43419,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:30 PM,21/Oct/25 12:30 PM,21/Oct/25 12:30 PM,,,0,"h3. Summary

Validates that Focus Server properly rejects historic playback requests where {{end_time}} is before {{start_time}}, returning an appropriate error.

h3. Objective

Verify proper validation and error handling when attempting to configure a historic playback with an invalid time range (end before start).

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{error-handling}}, {{validation}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-VALIDATION
* *Description*: Historic playback must validate that start_time < end_time

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""historic_invalid_range_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_later>"",
    ""end_time"": ""<yymmddHHMMSS_earlier>"",
    ""view_type"": 0
  }
}{code}

*Note*: {{start_time > end_time}} (intentionally invalid)

h3. Steps

||#||Step Description||Expected Result||
|1|Set {{start_time = now()}}, {{end_time = now() - 10 minutes}} (inverted)|Invalid time range created|
|2|Send POST /configure|Status 400 or 422 (Validation Error)|
|3|Verify error message|Error indicates ""start_time must be before end_time"" or similar|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400 or 422 for /configure
* *Error message*: ""Invalid time range: start_time must be before end_time""
* *Behavior*: Request rejected

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_invalid_time_range_end_before_start

# Create invalid time range (end before start)
start_time_dt = datetime.now()
end_time_dt = datetime.now() - timedelta(minutes=10)  # Earlier than start

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(sensors_min=0, sensors_max=50, live=False, duration_minutes=5)
payload[""start_time""] = start_time
payload[""end_time""] = end_time

# Expect validation error
with pytest.raises(Exception) as exc_info:
    focus_server_api.config_task(task_id, ConfigTaskRequest(**payload))

error_msg = str(exc_info.value).lower()
assert ""time"" in error_msg or ""range"" in error_msg or ""invalid"" in error_msg

# Verify task not created
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_invalid_time_range_end_before_start}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@104376f1,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u47:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:30 PM
Integration – Historic Playback - Status 208 Completion,PZ-13868,43418,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:29 PM,21/Oct/25 12:29 PM,21/Oct/25 12:29 PM,,,0,"h3. Summary

Validates that historic playback tasks correctly reach status 208 (baby analyzer exited) upon completion, signaling end-of-data.

h3. Objective

Verify that the server properly signals playback completion by returning status 208 when all historical data for the specified range has been delivered.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{status-208}}, {{completion}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-STATUS-208
* *Description*: Historic playback must signal completion with status 208

h3. Pre-Conditions

# Focus Server is running
# Historic data available for specified range

h3. Test Data

{code:json}{
  ""task_id"": ""historic_status_208_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure historic task (5-minute range)|Task configured|
|2|Poll GET /waterfall until status 208|Status transitions: 200 → 201 → 208|
|3|Track status transitions|Status codes logged|
|4|Verify status 208 eventually received|Final status = 208|
|5|Verify status 208 message indicates completion|Message: ""Baby analyzer exited"" or similar|

h3. Expected Result

* *Status 208 received*: Playback completes with status 208
* *Message*: ""Baby analyzer exited"" or ""playback complete""

h3. Post-Conditions

* Task completed, no longer active

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_playback_status_208_completion

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**historic_config_payload))
assert response.status == ""Config received successfully""

status_transitions = []
status_208_received = False

for attempt in range(100):
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    
    if not status_transitions or status_transitions[-1] != waterfall_response.status_code:
        status_transitions.append(waterfall_response.status_code)
        logger.info(f""Status: {waterfall_response.status_code} - {waterfall_response.message}"")
    
    if waterfall_response.status_code == 208:
        status_208_received = True
        logger.info(""Status 208 received - playback complete"")
        break
    
    time.sleep(2.0)

assert status_208_received, ""Status 208 not received within timeout""
logger.info(f""Status transitions: {status_transitions}""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_playback_status_208_completion}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6c8b318f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u3z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:29 PM
Data Quality – Historic Playback - Data Integrity Validation,PZ-13867,43417,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:23 PM,21/Oct/25 12:24 PM,21/Oct/25 12:24 PM,,,0,"h3. Summary

Validates data integrity during historic playback by checking timestamp ordering, sensor data completeness, and absence of corrupted data.

h3. Objective

Verify that all data returned during historic playback has sequential timestamps, complete sensor arrays, non-empty intensity data, and no missing or corrupted values.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{data-integrity}}, {{quality}}, {{validation}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-DATA-INTEGRITY
* *Description*: All historic data must be complete, ordered, and free of corruption

h3. Pre-Conditions

# Focus Server is running
# MongoDB contains clean, recorded data
# Historic playback configured for 5-minute range

h3. Test Data

{code:json}{
  ""task_id"": ""historic_integrity_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure historic task (5-minute range)|Task configured|
|2|Poll GET /waterfall, collect ALL data blocks|Data collected|
|3|For each row in all blocks:|*|
|4|→ Verify {{row.startTimestamp <= row.endTimestamp}}|Timestamps valid|
|5|→ Verify timestamps are sequential (increasing)|{{row[i].endTimestamp <= row[i+1].startTimestamp}}|
|6|→ Verify {{len(row.sensors) > 0}}|Row has sensor data|
|7|→ For each sensor: {{assert sensor.id >= 0}}|Valid sensor IDs|
|8|→ For each sensor: {{assert len(sensor.intensity) > 0}}|Non-empty intensity arrays|
|9|→ Track last_timestamp, ensure no duplicates|No duplicate timestamps|
|10|Wait for status 208|Playback completes|
|11|Verify total rows collected > 0|Data received|
|12|Verify all integrity checks passed|No corrupted data found|

h3. Expected Result

* *All rows valid*: Every row passes timestamp, sensor, and intensity checks
* *Sequential timestamps*: No out-of-order or duplicate timestamps
* *Complete data*: No missing sensors or empty intensity arrays

h3. Post-Conditions

* Data integrity verified for historic playback

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_playback_data_integrity

# Configure task
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**historic_config_payload))
assert response.status == ""Config received successfully""

all_rows = []
last_timestamp = 0

for attempt in range(100):
    waterfall_response = focus_server_api.get_waterfall(task_id, 20)
    
    if waterfall_response.status_code == 201 and waterfall_response.data:
        # Collect rows
        for block in waterfall_response.data:
            for row in block.rows:
                all_rows.append(row)
                
                # Check timestamp ordering
                assert row.startTimestamp <= row.endTimestamp, \
                    ""Start timestamp > end timestamp""
                assert row.startTimestamp >= last_timestamp, \
                    ""Timestamps not sequential""
                last_timestamp = row.endTimestamp
                
                # Check sensor data
                assert len(row.sensors) > 0, ""Row has no sensor data""
                
                for sensor in row.sensors:
                    assert sensor.id >= 0, ""Invalid sensor ID""
                    assert len(sensor.intensity) > 0, ""Sensor has no intensity data""
    
    elif waterfall_response.status_code == 208:
        # Playback complete
        logger.info(f""Data integrity verified: {len(all_rows)} rows collected"")
        break
    
    time.sleep(2.0)

# Final assertions
assert len(all_rows) > 0, ""No rows collected during playback""
logger.info(f""All {len(all_rows)} rows passed integrity checks""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_playback_data_integrity}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7d959e5,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u3r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:23 PM
Integration – Historic Playback - Very Old Timestamps (No Data),PZ-13866,43416,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:21 PM,21/Oct/25 12:22 PM,21/Oct/25 12:22 PM,,,0,"h3. Summary

Validates that Focus Server correctly handles historic playback requests for time ranges where no data exists (e.g., 1 year ago), returning appropriate status and messages.

h3. Objective

Verify error handling or graceful completion when requesting historic data from a time period with no recorded data in MongoDB.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{error-handling}}, {{no-data}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-NODATA
* *Description*: Historic playback must handle ""no data available"" scenarios gracefully

h3. Pre-Conditions

# Focus Server is running
# MongoDB does NOT contain data from 1 year ago

h3. Test Data

{code:json}{
  ""task_id"": ""historic_old_nodata_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_1_year_ago>"",
    ""end_time"": ""<yymmddHHMMSS_1_year_ago_plus_5min>"",
    ""view_type"": 0
  }
}{code}

*Time Range*: 1 year ago, 5-minute window

h3. Steps

||#||Step Description||Expected Result||
|1|Calculate time range from 1 year ago|Time range calculated|
|2|Send POST /configure|Status 200, task configured (server accepts config)|
|3|Poll GET /waterfall (max 30 attempts)|Status 200 (no data yet) or 208 (no data found, exited)|
|4|Verify either: (a) Status 208 quickly (no data), or (b) Status 200 persists (no data available)|Expected behavior for ""no data"" scenario|
|5|Verify no data blocks received|{{data_blocks_received == 0}}|

h3. Expected Result

* *Configuration accepted*: Status 200 for /configure
* *No data*: Either status 208 (completed with no data) or persistent status 200
* *No errors*: No crashes or unexpected errors

h3. Post-Conditions

* Task completes or remains in ""no data"" state
* No errors logged

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_with_very_old_timestamps

# Calculate time range from 1 year ago
end_time_dt = datetime.now() - timedelta(days=365)
start_time_dt = end_time_dt - timedelta(minutes=5)

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(
    sensors_min=0,
    sensors_max=50,
    live=False,
    duration_minutes=5
)
payload[""start_time""] = start_time
payload[""end_time""] = end_time

# Configure task
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**payload))
assert response.status == ""Config received successfully""

# Poll and expect no data
data_found = False
max_attempts = 30

for attempt in range(max_attempts):
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    
    if waterfall_response.status_code == 201:
        data_found = True
        logger.warning(""Unexpected data found for 1-year-old timestamps"")
        break
    
    elif waterfall_response.status_code == 208:
        logger.info(""Playback completed with no data (expected)"")
        break
    
    time.sleep(1.0)

# No data expected
assert not data_found, ""Data should not exist for 1-year-old timestamps""
logger.info(""No data scenario handled correctly""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_with_very_old_timestamps}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4fa0369d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u3j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:21 PM
Integration – Historic Playback - Short Duration (1 Minute),PZ-13865,43415,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:20 PM,21/Oct/25 12:21 PM,21/Oct/25 12:21 PM,,,0,"h3. Summary

Validates that Focus Server can handle a very short historic playback request (1 minute), completing quickly and efficiently.

h3. Objective

Verify that historic playback works correctly for minimal time ranges (1 minute), demonstrating that the system can handle short-duration queries without issues.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{short-duration}}, {{edge-case}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-SHORT
* *Description*: Historic playback must support short time ranges (minimum 1 minute)

h3. Pre-Conditions

# Focus Server is running
# MongoDB contains data from the last hour
# Baby Analyzer can process 1-minute playback

h3. Test Data

{code:json}{
  ""task_id"": ""historic_1min_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 20
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

*Time Range*: 1 minute (end_time - 1 minute to end_time)

h3. Steps

||#||Step Description||Expected Result||
|1|Calculate 1-minute time range|{{start_time = now() - 1 minute}}, {{end_time = now()}}|
|2|Create {{ConfigureRequest}} with 1-minute range|Payload validated|
|3|Send POST /configure|Status 200, task configured|
|4|Poll GET /waterfall every 1 second (max 30 attempts)|Status transitions to 201, then 208|
|5|Verify status 208 received within 30 seconds|Playback completes quickly|
|6|Verify some data received|Data blocks > 0|

h3. Expected Result

* *Quick completion*: Status 208 received within 30 polls (30 seconds)
* *Data received*: At least some data blocks returned
* *No errors*: Clean completion

h3. Post-Conditions

* Short playback completed successfully

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_playback_with_short_duration

# Create 1-minute historic range
end_time_dt = datetime.now()
start_time_dt = end_time_dt - timedelta(minutes=1)

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(
    sensors_min=0,
    sensors_max=20,
    live=False,
    duration_minutes=1
)

# Configure task
config_request = ConfigTaskRequest(**payload)
response = focus_server_api.config_task(task_id, config_request)
assert response.status == ""Config received successfully""

# Poll until completion (with shorter timeout)
max_attempts = 30
completed = False

for attempt in range(max_attempts):
    response = focus_server_api.get_waterfall(task_id, 10)
    
    if response.status_code == 208:
        completed = True
        logger.info(f""Short playback completed after {attempt + 1} polls"")
        break
    
    time.sleep(1.0)

assert completed, ""Short historic playback did not complete in time""{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_playback_with_short_duration}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}

----

h2. Test: PZ-HISTORIC-EXT-003

*Test Name*: Historic Playback - Long Duration (30 Minutes)

h3. Summary

Validates that Focus Server can handle longer historic playback requests (30 minutes), ensuring stability and data integrity over extended processing.

h3. Objective

Verify that historic playback works correctly for longer time ranges (30 minutes), demonstrating that the system can sustain extended playback sessions without memory leaks or performance degradation.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{long-duration}}, {{stability}}
* *Test Type*: Integration Test (Long-Running)

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-LONG
* *Description*: Historic playback must support extended time ranges (up to 60 minutes)

h3. Pre-Conditions

# Focus Server is running
# MongoDB contains at least 30 minutes of recorded data from recent period
# Sufficient server resources for long playback

h3. Test Data

{code:json}{
  ""task_id"": ""historic_30min_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

*Time Range*: 30 minutes

h3. Steps

||#||Step Description||Expected Result||
|1|Calculate 30-minute time range|Time range calculated|
|2|Send POST /configure|Status 200, task configured|
|3|Poll GET /waterfall every 2 seconds (max 600 attempts = 20 minutes timeout)|Status 201 data received continuously|
|4|Track total data blocks received|Large number of data blocks|
|5|Wait for status 208|Playback completes eventually|
|6|Verify large amount of data received|Data blocks > 100|
|7|Verify no memory issues or timeouts|No server errors|

h3. Expected Result

* *Long playback*: Completes within 20 minutes (600 polls)
* *Large data volume*: > 100 data blocks received
* *Stability*: No errors, timeouts, or resource exhaustion

h3. Post-Conditions

* Server stable after long playback
* Memory usage returns to normal

h3. Assertions (Python Code)

{code:python}# Test function: test_historic_playback_long_duration

# Create 30-minute historic range
end_time_dt = datetime.now()
start_time_dt = end_time_dt - timedelta(minutes=30)

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

payload = generate_config_payload(
    sensors_min=0,
    sensors_max=50,
    live=False,
    duration_minutes=30
)

# Configure task
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**payload))
assert response.status == ""Config received successfully""

# Poll with longer timeout
max_attempts = 600  # 20 minutes
data_blocks_received = 0

for attempt in range(max_attempts):
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    
    if waterfall_response.status_code == 201 and waterfall_response.data:
        data_blocks_received += len(waterfall_response.data)
        
        if attempt % 50 == 0:
            logger.info(f""Poll {attempt}, data blocks: {data_blocks_received}"")
    
    elif waterfall_response.status_code == 208:
        logger.info(f""Long playback completed after {attempt + 1} polls"")
        logger.info(f""Total data blocks: {data_blocks_received}"")
        assert data_blocks_received > 100, f""Expected >100 blocks, got {data_blocks_received}""
        return  # Test passed
    
    time.sleep(2.0)

pytest.fail(""Long playback did not complete in time""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_historic_playback_long_duration}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3e785997,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u3b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:20 PM
Integration – Historic Playback - Short Duration (1 Minute),PZ-13864,43414,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:19 PM,21/Oct/25 12:19 PM,21/Oct/25 12:19 PM,,,0,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@28187437,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u33:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:19 PM
Integration – Historic Playback - Standard 5-Minute Range,PZ-13863,43413,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:17 PM,21/Oct/25 12:18 PM,21/Oct/25 12:18 PM,,,0,"h3. Summary

Validates that Focus Server correctly handles a standard historic playback request for a 5-minute time range, returning data from the specified historical period and completing with status 208.

h3. Objective

Verify that Focus Server can process a historic playback configuration with {{start_time}} and {{end_time}} set to a 5-minute range, poll waterfall data until completion, and return all available data for the specified period.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{historic-playback}}, {{time-range}}, {{integration}}, {{api}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-HISTORIC-PLAYBACK
* *Description*: Historic playback must support time-bound data retrieval with start/end timestamps

h3. Pre-Conditions

# Focus Server is running and accessible
# MongoDB contains recorded data for the specified time range
# Baby Analyzer can access and process historic recordings
# Recordings collection in MongoDB has data within the last 24 hours

h3. Test Data

{code:json}{
  ""task_id"": ""historic_5min_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 50
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": ""<yymmddHHMMSS_start>"",
    ""end_time"": ""<yymmddHHMMSS_end>"",
    ""view_type"": 0
  }
}{code}

*Time Range*: {{end_time = now()}}, {{start_time = end_time - 5 minutes}}  
*Format*: {{yymmddHHMMSS}} (e.g., ""251020120000"" for 2025-10-20 12:00:00)

h3. Steps

||#||Step Description||Expected Result||
|1|Calculate time range: {{end_time = now()}}, {{start_time = end_time - 5 minutes}}|Time range calculated in {{yymmddHHMMSS}} format|
|2|Validate time format using {{validate_time_format_yymmddHHMMSS()}}|Time strings validated|
|3|Create {{ConfigureRequest}} with {{start_time}} and {{end_time}}|Payload validated by Pydantic|
|4|Send POST /configure|Status 200, {{status: ""Config received successfully""}}|
|5|Immediately send GET /waterfall/{task_id}/10|Status 200 (no data yet) or 201 (data available)|
|6|Poll GET /waterfall every 2 seconds (max 100 attempts)|Status transitions from 200 → 201 → 208|
|7|Collect all data blocks during status 201 phases|Multiple data blocks with waterfall rows|
|8|Wait for status 208 (baby analyzer exited)|Status 208 received, playback complete|
|9|Verify total data blocks received > 0|At least some historic data retrieved|
|10|Verify all timestamps fall within specified range|{{start_time <= row.timestamp <= end_time}}|
|11|Verify timestamps are sequential and increasing|Each row's timestamp >= previous row's timestamp|

h3. Expected Result

* *Status code*: 200 for /configure, status transitions 200 → 201 → 208 for /waterfall
* *Data received*: Multiple data blocks with valid waterfall rows
* *Timestamps*: All within specified 5-minute range, sequential
* *Completion*: Status 208 received within reasonable time (< 200 seconds)

h3. Post-Conditions

* Historic playback task completed successfully
* Baby analyzer process for this task exited cleanly
* No error messages in Focus Server logs

h3. Assertions (Python Code)

{code:python}# Test function: test_configure_historic_task_success (from test_historic_playback_flow.py)

# Generate time range
from src.utils.helpers import generate_time_range, datetime_to_yymmddHHMMSS
end_time_dt = datetime.now()
start_time_dt = end_time_dt - timedelta(minutes=5)

start_time = datetime_to_yymmddHHMMSS(start_time_dt)
end_time = datetime_to_yymmddHHMMSS(end_time_dt)

historic_config_payload[""start_time""] = start_time
historic_config_payload[""end_time""] = end_time

logger.info(f""Time range: {start_time} to {end_time}"")

# Validate time format
from src.utils.validators import validate_time_format_yymmddHHMMSS
assert validate_time_format_yymmddHHMMSS(start_time)
assert validate_time_format_yymmddHHMMSS(end_time)

# Configure task
config_request = ConfigTaskRequest(**historic_config_payload)
response = focus_server_api.config_task(task_id, config_request)

# Assertions
assert isinstance(response, ConfigTaskResponse)
assert response.status == ""Config received successfully""

# Poll until completion
status_transitions = []
data_blocks_received = 0
max_poll_attempts = 100
poll_interval = 2.0

for attempt in range(max_poll_attempts):
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    
    # Track status transitions
    if not status_transitions or status_transitions[-1] != waterfall_response.status_code:
        status_transitions.append(waterfall_response.status_code)
        logger.info(f""Status transition: {waterfall_response.status_code}"")
    
    if waterfall_response.status_code == 201:
        # Data available
        validation_result = validate_waterfall_response(waterfall_response)
        assert validation_result[""is_valid""]
        
        if waterfall_response.data:
            data_blocks_received += len(waterfall_response.data)
    
    elif waterfall_response.status_code == 208:
        # Playback complete
        logger.info(f""Playback completed after {attempt + 1} polls"")
        logger.info(f""Total data blocks received: {data_blocks_received}"")
        assert data_blocks_received > 0, ""No data blocks received during playback""
        return  # Test passed
    
    time.sleep(poll_interval)

pytest.fail(""Playback did not complete after 100 poll attempts""){code}

h3. Environment

* *Environment Name*: new_production
* *Focus Server*: [https://10.10.100.100/focus-server/|https://10.10.100.100/focus-server/]
* *MongoDB*: 10.10.100.108:27017 (contains historic recordings)

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_configure_historic_task_success}}, {{test_poll_historic_playback_until_completion}}
* *Test File*: {{tests/integration/api/test_historic_playback_flow.py}}
* *Execution*: {{pytest -m ""integration and api"" tests/integration/api/test_historic_playback_flow.py::TestHistoricPlaybackHappyPath::test_configure_historic_task_success}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5e99b761,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u2v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:17 PM
Integration - SingleChannel Complete Flow End-to-End,PZ-13862,43412,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:05 PM,21/Oct/25 12:05 PM,21/Oct/25 12:05 PM,,,0,"h3. Summary

Comprehensive end-to-end test for SingleChannel view, covering configuration, data polling, metadata retrieval, reconfiguration, and cleanup, validating the complete lifecycle of a SingleChannel task.

h3. Objective

Verify that a SingleChannel task can be configured, polled for data, queried for metadata, reconfigured to a different channel, and cleaned up successfully, demonstrating a full end-to-end workflow.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{end-to-end}}, {{full-flow}}, {{lifecycle}}
* *Test Type*: Integration Test (End-to-End)

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-E2E
* *Description*: Complete SingleChannel lifecycle must function correctly

h3. Pre-Conditions

# Focus Server is running
# Baby Analyzer is processing data
# At least 50 sensors available

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_e2e_<timestamp>"",
  ""initial_config"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 10,
      ""max"": 10
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  },
  ""reconfig"": {
    ""channels"": {
      ""min"": 25,
      ""max"": 25
    }
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|*Phase 1: Initial Configuration*|*|
|2|Generate unique task_id|Task ID created|
|3|Send POST /configure with channel 10|Status 200, {{stream_amount=1}}, {{channel_to_stream_index={""10"": 0}}}|
|4|*Phase 2: Data Polling*|*|
|5|Poll GET /waterfall 10 times (collect data)|Status 201, 10 successful polls, data for channel 10|
|6|Verify all data contains sensor ID 10|All rows have {{sensors[0].id == 10}}|
|7|Verify timestamps are sequential|Timestamps increase across polls|
|8|*Phase 3: Metadata Retrieval*|*|
|9|Send GET /metadata/{task_id}|Status 200, metadata returned|
|10|Verify metadata consistency|Metadata reflects channel 10 configuration|
|11|*Phase 4: Reconfiguration*|*|
|12|Send POST /configure with channel 25 (same task_id)|Status 200, {{channel_to_stream_index={""25"": 0}}}|
|13|Poll GET /waterfall 5 times|Status 201, data for channel 25|
|14|Verify all data now contains sensor ID 25|All rows have {{sensors[0].id == 25}}|
|15|*Phase 5: Cleanup (optional)*|*|
|16|If DELETE /task/{task_id} supported, send DELETE|Status 200, task deleted|
|17|Verify task no longer exists|GET /waterfall returns 404|

h3. Expected Result

* *Phase 1*: Task configured successfully for channel 10
* *Phase 2*: 10 polls return valid data for channel 10
* *Phase 3*: Metadata retrieved and consistent
* *Phase 4*: Task reconfigured to channel 25, data updates correctly
* *Phase 5*: Task cleaned up (if supported)

h3. Post-Conditions

* Task either active (if cleanup not supported) or deleted
* No errors in server logs

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_complete_flow_e2e

task_id = generate_task_id(""e2e"")

# Phase 1: Initial Configuration
logger.info(""Phase 1: Configuring channel 10"")
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.stream_amount == 1
assert response.channel_to_stream_index == {""10"": 0}

# Phase 2: Data Polling
logger.info(""Phase 2: Polling data for channel 10"")
poll_count = 0
for poll_num in range(10):
    waterfall_response = focus_server_api.get_waterfall(task_id, 5)
    if waterfall_response.status_code == 201:
        poll_count += 1
        for data_block in waterfall_response.data:
            for row in data_block.data[0].rows:
                assert row.sensors[0].id == 10
    time.sleep(0.5)

assert poll_count >= 8, f""Only {poll_count}/10 polls succeeded""

# Phase 3: Metadata
logger.info(""Phase 3: Retrieving metadata"")
metadata_response = focus_server_api.get_metadata(task_id)
assert metadata_response is not None

# Phase 4: Reconfiguration
logger.info(""Phase 4: Reconfiguring to channel 25"")
config_payload[""channels""][""min""] = 25
config_payload[""channels""][""max""] = 25
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.channel_to_stream_index == {""25"": 0}

time.sleep(1.0)  # Wait for reconfiguration

for poll_num in range(5):
    waterfall_response = focus_server_api.get_waterfall(task_id, 5)
    if waterfall_response.status_code == 201:
        for data_block in waterfall_response.data:
            for row in data_block.data[0].rows:
                assert row.sensors[0].id == 25
    time.sleep(0.5)

logger.info(""End-to-end SingleChannel flow completed successfully""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_complete_flow_e2e}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,PZ-13861,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7710d721,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u2n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:05 PM
Integration - SingleChannel Stream Mapping Verification,PZ-13861,43411,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:02 PM,21/Oct/25 12:05 PM,21/Oct/25 12:03 PM,,,0,"h3. Summary

Validates the {{channel_to_stream_index}} mapping returned in the configuration response for various SingleChannel configurations, ensuring correct 1:1 mapping in all cases.

h3. Objective

Verify that for any valid SingleChannel configuration (any channel ID), the {{channel_to_stream_index}} always maps the single channel to stream index 0.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{channel-mapping}}, {{validation}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-MAPPING
* *Description*: SingleChannel must always map the configured channel to stream index 0

h3. Pre-Conditions

# Focus Server is running
# Multiple sensors available (e.g., 0-99)

h3. Test Data

{code:json}{
  ""task_id_template"": ""singlechannel_mapping_ch<channel>_<timestamp>"",
  ""config_payload_template"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": <channel>,
      ""max"": <channel>
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Test channels*: 0, 1, 7, 15, 25, 50, 99

h3. Steps

||#||Step Description||Expected Result||
|1|For each channel in [0, 1, 7, 15, 25, 50, 99]|Iterate test channels|
|2|Create unique task_id for each channel|Task IDs generated|
|3|Create {{ConfigureRequest}} with {{channels.min=channel}}, {{channels.max=channel}}|Payload created|
|4|Send POST /configure|Status 200, task configured|
|5|Verify {{stream_amount == 1}}|Single stream|
|6|Verify {{channel_to_stream_index == {str(channel): 0}}}|Channel maps to stream 0|
|7|Verify {{channel_amount == 1}}|Single channel|

h3. Expected Result

* *All channels map correctly*: Every channel (0, 1, 7, etc.) maps to stream index 0
* *stream_amount*: Always 1
* *channel_amount*: Always 1

h3. Post-Conditions

* Multiple tasks configured, each with correct mapping

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_stream_mapping_verification

test_channels = [0, 1, 7, 15, 25, 50, 99]

for channel in test_channels:
    task_id = generate_task_id(f""mapping_ch{channel}"")
    
    config_payload[""channels""][""min""] = channel
    config_payload[""channels""][""max""] = channel
    
    response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
    
    # Verify mapping
    assert response.stream_amount == 1
    assert response.channel_to_stream_index == {str(channel): 0}, \
        f""Channel {channel} did not map to stream 0""
    assert response.channel_amount == 1
    
    logger.info(f""Channel {channel} → Stream 0 (verified)"")

logger.info(""All channel-to-stream mappings verified""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_stream_mapping_verification}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13862,PZ-13860,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@853d4a3,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u2f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:02 PM
Integration - SingleChannel Metadata Consistency,PZ-13860,43410,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:01 PM,21/Oct/25 12:02 PM,21/Oct/25 12:01 PM,,,0,"h3. Summary

Validates that metadata returned for a SingleChannel task (via GET /metadata/{task_id}) is consistent with the configuration and reflects the single-channel setup.

h3. Objective

Verify that GET /metadata returns consistent and accurate metadata for a SingleChannel task, including channel count, stream count, and other configuration details.

h3. Priority

*Low*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{metadata}}, {{consistency}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-METADATA
* *Description*: Metadata endpoint must return accurate configuration details

h3. Pre-Conditions

# Focus Server is running
# SingleChannel task configured

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_metadata_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure SingleChannel task for channel 7|Status 200, task configured|
|2|Send GET /metadata/{task_id}|Status 200, metadata returned|
|3|Verify metadata contains expected fields|Required fields present|
|4|Verify metadata reflects SingleChannel configuration|Metadata shows 1 channel, 1 stream|
|5|Compare metadata with configuration|Metadata consistent with config|

h3. Expected Result

* *Metadata returned*: GET /metadata returns status 200
* *Consistency*: Metadata reflects SingleChannel configuration (1 channel, 1 stream)

h3. Post-Conditions

* Metadata endpoint functional

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_metadata_consistency

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.stream_amount == 1

# Get metadata
metadata_response = focus_server_api.get_metadata(task_id)

# Verify metadata structure
assert metadata_response is not None
logger.info(f""Metadata: {metadata_response}"")

# Additional assertions based on actual metadata structure
# (may vary depending on API implementation){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_metadata_consistency}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13861,PZ-13859,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@167c1164,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u27:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:01 PM
Integration - SingleChannel Polling Stability,PZ-13859,43409,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 12:00 PM,21/Oct/25 12:01 PM,21/Oct/25 12:00 PM,,,0,"h3. Summary

Validates that prolonged polling of a SingleChannel task remains stable, with consistent response times and no degradation over time.

h3. Objective

Verify that continuous polling of a SingleChannel task for an extended period (e.g., 100 polls) does not result in performance degradation, memory leaks, or errors.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{stability}}, {{performance}}, {{long-running}}
* *Test Type*: Integration Test (Stability)

h3. Requirements

* *Requirement ID*: FOCUS-API-STABILITY
* *Description*: Continuous polling must remain stable without performance degradation

h3. Pre-Conditions

# Focus Server is running
# SingleChannel task configured

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_polling_stability_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure SingleChannel task|Status 200, task configured|
|2|For poll_num in range(100):|100 polling iterations|
|3|Send GET /waterfall/{task_id}/10|Status 201, data returned|
|4|Record response time|Response time tracked|
|5|Verify response structure|Valid data structure|
|6|Sleep 0.5 seconds|Wait before next poll|
|7|After all polls, analyze response times|Average response time calculated|
|8|Verify no significant degradation|Response times remain consistent|

h3. Expected Result

* *All polls succeed*: 100/100 polls return status 201
* *Response times stable*: No significant increase over time
* *No errors*: No timeouts or server errors

h3. Post-Conditions

* Task still active and responsive
* Server memory usage stable

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_polling_stability

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.stream_amount == 1

response_times = []
successful_polls = 0

for poll_num in range(100):
    start_time = time.time()
    waterfall_response = focus_server_api.get_waterfall(task_id, 10)
    elapsed = time.time() - start_time
    
    if waterfall_response.status_code == 201:
        successful_polls += 1
        response_times.append(elapsed)
    
    if poll_num % 20 == 0:
        logger.info(f""Poll {poll_num}/100, response time: {elapsed:.3f}s"")
    
    time.sleep(0.5)

# Verify all polls succeeded
assert successful_polls >= 95, f""Only {successful_polls}/100 polls succeeded""

# Verify response times are stable
avg_response_time = sum(response_times) / len(response_times)
max_response_time = max(response_times)

logger.info(f""Polling stability: {successful_polls}/100 succeeded"")
logger.info(f""Avg response time: {avg_response_time:.3f}s, Max: {max_response_time:.3f}s"")

assert avg_response_time < 2.0, ""Average response time too high""
assert max_response_time < 5.0, ""Max response time too high""{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_polling_stability}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13860,PZ-13858,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1b83535e,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u1z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 12:00 PM
Integration - SingleChannel Rapid Reconfiguration,PZ-13858,43408,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:59 AM,21/Oct/25 12:00 PM,21/Oct/25 11:59 AM,,,0,"h3. Summary

Validates that a SingleChannel task can be rapidly reconfigured multiple times without errors, ensuring the server handles configuration updates correctly.

h3. Objective

Verify that Focus Server can handle multiple rapid reconfigurations of the same task_id, updating the channel selection each time without memory leaks or errors.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{reconfiguration}}, {{stress}}, {{stability}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-RECONFIGURATION
* *Description*: Tasks must support reconfiguration without resource leaks

h3. Pre-Conditions

# Focus Server is running
# At least 20 sensors available

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_rapid_reconfig_<timestamp>"",
  ""config_payload_template"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": <channel_id>,
      ""max"": <channel_id>
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Generate single task_id|Unique task_id created|
|2|For each channel in [0, 5, 10, 15, 20] (5 iterations)|Iterate channels|
|3|Update {{config_payload}} with new channel (min=max=channel)|Payload updated|
|4|Send POST /configure with same task_id|Status 200, configuration updated|
|5|Verify response {{stream_amount == 1}}|Single stream|
|6|Verify {{channel_to_stream_index == {str(channel): 0}}}|Correct channel mapped|
|7|Poll GET /waterfall|Status 201, data for new channel returned|
|8|Verify sensor ID matches new channel|Correct sensor in data|

h3. Expected Result

* *All reconfigurations succeed*: 5 rapid reconfigurations complete successfully
* *Data consistency*: Each poll returns data for the newly configured channel
* *No errors*: No memory leaks or server errors

h3. Post-Conditions

* Task configured with last channel (20)
* Server stable, no resource exhaustion

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_rapid_reconfiguration

task_id = generate_task_id(""rapid_reconfig"")
test_channels = [0, 5, 10, 15, 20]

for channel in test_channels:
    logger.info(f""Reconfiguring task to channel {channel}"")
    
    config_payload[""channels""][""min""] = channel
    config_payload[""channels""][""max""] = channel
    
    response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
    assert response.status == ""Config received successfully""
    assert response.stream_amount == 1
    assert response.channel_to_stream_index == {str(channel): 0}
    
    # Verify data
    time.sleep(0.5)  # Brief wait for reconfiguration
    waterfall_response = focus_server_api.get_waterfall(task_id, 5)
    if waterfall_response.status_code == 201:
        for data_block in waterfall_response.data:
            for row in data_block.data[0].rows:
                assert row.sensors[0].id == channel

logger.info(""Rapid reconfiguration test passed""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_rapid_reconfiguration}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13859,PZ-13857,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@61deb77c,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u1r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:59 AM
Integration - SingleChannel NFFT Validation,PZ-13857,43407,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:58 AM,21/Oct/25 11:59 AM,21/Oct/25 11:59 AM,,,0,"h3. Summary

Validates that SingleChannel view correctly accepts and applies different NFFT (FFT size) configurations.

h3. Objective

Verify that Focus Server accepts various NFFT values (e.g., 512, 1024, 2048) and configures the task successfully, affecting the frequency resolution of the returned data.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{configuration}}, {{nfft}}, {{fft}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-NFFT-CONFIG
* *Description*: NFFT configuration must be accepted and applied to signal processing

h3. Pre-Conditions

# Focus Server is running
# Baby Analyzer supports various NFFT values

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_nfft_<nfft>_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": <test_nfft>,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Test NFFT values*: 512, 1024, 2048

h3. Steps

||#||Step Description||Expected Result||
|1|For each NFFT in [512, 1024, 2048]|Iterate test NFFT values|
|2|Create {{ConfigureRequest}} with {{nfftSelection = <test_nfft>}}|Payload created|
|3|Send POST /configure|Status 200, task configured|
|4|Verify response success|{{status: ""Config received successfully""}}|
|5|Poll GET /waterfall|Status 201, data returned|
|6|Verify intensity array length varies with NFFT|Different NFFT → different intensity array size|

h3. Expected Result

* *All NFFT values accepted*: 512, 1024, 2048 configure successfully
* *Data impact*: Intensity array length reflects NFFT configuration

h3. Post-Conditions

* Tasks configured with different NFFT values

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_nfft_validation

test_nfft_values = [512, 1024, 2048]

for nfft in test_nfft_values:
    task_id = generate_task_id(f""nfft_{nfft}"")
    config_payload[""nfftSelection""] = nfft
    
    response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
    assert response.status == ""Config received successfully""
    
    # Poll data and check intensity length
    waterfall_response = focus_server_api.get_waterfall(task_id, 5)
    for data_block in waterfall_response.data:
        for row in data_block.data[0].rows:
            intensity_len = len(row.sensors[0].intensity)
            logger.info(f""NFFT {nfft} → intensity length: {intensity_len}"")
            assert intensity_len > 0{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_nfft_validation}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13858,PZ-13855,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@787d98c4,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u1j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:58 AM
Integration - SingleChannel Canvas Height Validation,PZ-13855,43405,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:55 AM,21/Oct/25 11:58 AM,21/Oct/25 11:55 AM,,,0,"h3. Summary

Validates that SingleChannel view correctly accepts and applies different canvas height configurations.

h3. Objective

Verify that Focus Server accepts various canvas height values (e.g., 500, 1000, 1500, 2000) and configures the task successfully.

h3. Priority

*Low*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{configuration}}, {{canvas-height}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-CANVAS-CONFIG
* *Description*: Canvas height configuration must be accepted and stored

h3. Pre-Conditions

# Focus Server is running

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_canvas_height_<height>_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": <test_height>
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Test heights*: 500, 1000, 1500, 2000

h3. Steps

||#||Step Description||Expected Result||
|1|For each height in [500, 1000, 1500, 2000]|Iterate test heights|
|2|Create {{ConfigureRequest}} with {{displayInfo.height = <test_height>}}|Payload created|
|3|Send POST /configure|Status 200, task configured|
|4|Verify response success|{{status: ""Config received successfully""}}|
|5|Poll GET /waterfall|Status 201, data returned|

h3. Expected Result

* *All heights accepted*: 500, 1000, 1500, 2000 all configure successfully
* *No errors*: All requests return status 200

h3. Post-Conditions

* Tasks configured with different canvas heights

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_canvas_height_validation

test_heights = [500, 1000, 1500, 2000]

for height in test_heights:
    task_id = generate_task_id(f""canvas_height_{height}"")
    config_payload[""displayInfo""][""height""] = height
    
    response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
    assert response.status == ""Config received successfully""
    
    logger.info(f""Canvas height {height} configured successfully""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_canvas_height_validation}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13857,PZ-13854,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@71429cfa,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u13:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:55 AM
Integration - SingleChannel Frequency Range Validation,PZ-13854,43404,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:54 AM,21/Oct/25 11:55 AM,21/Oct/25 11:54 AM,,,0,"h3. Summary

Validates that SingleChannel view correctly applies and respects the specified frequency range configuration.

h3. Objective

Verify that when a SingleChannel task is configured with a specific frequency range (e.g., 100-300 Hz), the returned data reflects this configuration and the server processes the request correctly.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{configuration}}, {{frequency-range}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-FREQUENCY-RANGE
* *Description*: Frequency range configuration must be applied correctly to data processing

h3. Pre-Conditions

# Focus Server is running
# Baby Analyzer supports frequency range filtering

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_freq_range_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 100,
      ""max"": 300
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{frequencyRange: {min: 100, max: 300}}}|Payload validated|
|2|Send POST /configure|Status 200, task configured|
|3|Verify response indicates frequency range accepted|Response contains frequency range or no error|
|4|Poll GET /waterfall|Status 201, data returned|
|5|Verify intensity array length reflects frequency range|{{len(intensity)}} matches expected frequency bins|

h3. Expected Result

* *Configuration*: Frequency range 100-300 Hz applied
* *Data*: Intensity arrays have correct length for specified range

h3. Post-Conditions

* Task configured with custom frequency range

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_frequency_range_validation

config_payload[""frequencyRange""][""min""] = 100
config_payload[""frequencyRange""][""max""] = 300

response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.status == ""Config received successfully""

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 201

# Verify data structure (intensity length may vary based on NFFT and freq range)
for data_block in waterfall_response.data:
    for row in data_block.data[0].rows:
        assert len(row.sensors[0].intensity) > 0
        logger.info(f""Intensity length: {len(row.sensors[0].intensity)}""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_frequency_range_validation}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13855,PZ-13853,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6423c082,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u0v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:54 AM
Integration - SingleChannel Data Consistency Check,PZ-13853,43403,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:53 AM,21/Oct/25 11:54 AM,21/Oct/25 11:53 AM,,,0,"h3. Summary

Validates data consistency and integrity for a SingleChannel view over multiple polling cycles, ensuring that all returned data blocks consistently contain only the configured channel with valid, non-corrupted data.

h3. Objective

Verify that data delivered by Focus Server for a SingleChannel task is consistent across multiple polls, with correct sensor IDs, non-empty intensity arrays, and sequential timestamps.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{data-integrity}}, {{consistency}}, {{quality}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-DATA-CONSISTENCY
* *Description*: Data returned from Focus Server must be consistent, complete, and free of corruption

h3. Pre-Conditions

# Focus Server is running
# Baby Analyzer is actively processing live data
# SingleChannel task configured (e.g., channel 7)

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_consistency_ch7_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 7,
      ""max"": 7
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Configure SingleChannel task for channel 7|Status 200, task configured|
|2|Poll GET /waterfall/{task_id}/20 (20 rows)|Status 201, data blocks returned|
|3|Verify all data blocks contain exactly 1 stream|{{len(data_block.data) == 1}}|
|4|Verify all rows in all streams have sensor ID == 7|{{row.sensors[0].id == 7}} for all rows|
|5|Verify all intensity arrays are non-empty|{{len(sensor.intensity) > 0}}|
|6|Verify timestamps are sequential and increasing|{{row[i].endTimestamp <= row[i+1].startTimestamp}}|
|7|Repeat polling 5 more times (total 6 polls)|All polls return consistent data structure|
|8|Track total rows collected|Total rows > 100 (across all polls)|
|9|Verify no duplicate timestamps|All {{startTimestamp}} values are unique|
|10|Verify no missing data (gaps in timestamps)|Timestamp deltas are consistent|

h3. Expected Result

* *Consistency*: All polls return exactly 1 stream with sensor ID 7
* *Integrity*: All intensity arrays non-empty, no null/corrupted values
* *Timestamps*: Sequential, increasing, no duplicates, no gaps
* *Total rows*: > 100 rows collected across 6 polls

h3. Post-Conditions

* Task remains active
* No errors or warnings in logs

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_data_consistency

# Configure task
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))
assert response.stream_amount == 1

all_rows = []
all_timestamps = set()

# Poll 6 times
for poll_num in range(6):
    waterfall_response = focus_server_api.get_waterfall(task_id, 20)
    assert waterfall_response.status_code == 201
    
    for data_block in waterfall_response.data:
        # Verify exactly 1 stream
        assert len(data_block.data) == 1
        
        for row in data_block.data[0].rows:
            # Verify sensor ID
            assert row.sensors[0].id == 7
            
            # Verify intensity data
            assert len(row.sensors[0].intensity) > 0
            
            # Track timestamps (check for duplicates)
            assert row.startTimestamp not in all_timestamps
            all_timestamps.add(row.startTimestamp)
            
            # Track all rows
            all_rows.append(row)
    
    time.sleep(1.0)  # Wait before next poll

# Verify timestamp ordering
for i in range(len(all_rows) - 1):
    assert all_rows[i].endTimestamp <= all_rows[i+1].startTimestamp

# Verify total rows collected
assert len(all_rows) > 100, f""Expected >100 rows, got {len(all_rows)}""
logger.info(f""Data consistency verified: {len(all_rows)} rows, all valid""){code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_data_consistency}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13854,PZ-13852,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2bf6598a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u0n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:53 AM
Integration - SingleChannel with Min > Max (Validation Error),PZ-13852,43402,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:52 AM,21/Oct/25 11:53 AM,21/Oct/25 11:52 AM,,,0,"h3. Summary

Validates that Focus Server properly rejects SingleChannel configuration requests when {{channels.min > channels.max}}, which violates the valid range definition for SingleChannel (min must equal max).

h3. Objective

Verify proper validation and error handling when attempting to configure a SingleChannel view with an invalid range where min is greater than max.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{error-handling}}, {{validation}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-RANGE
* *Description*: For SingleChannel view, channels.min must equal channels.max (1:1 mapping)

h3. Pre-Conditions

# Focus Server is running
# Valid sensor range is known (e.g., 0-99)

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_min_gt_max_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 10,
      ""max"": 5
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Note*: {{min=10 > max=5}} is invalid for SingleChannel.

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{channels.min=10}}, {{channels.max=5}}, {{view_type=SINGLECHANNEL}}|Payload created|
|2|Send POST /configure|Status 400 or 422 (Validation Error)|
|3|Verify error message|Error indicates ""min > max"" or ""invalid range for SingleChannel""|
|4|Verify task not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400 or 422 for /configure
* *Error message*: ""For SingleChannel view, channels.min must equal channels.max"" or similar
* *Behavior*: Request rejected

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_min_greater_than_max

config_payload[""channels""][""min""] = 10
config_payload[""channels""][""max""] = 5  # Invalid: min > max

with pytest.raises(Exception) as exc_info:
    focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))

error_msg = str(exc_info.value).lower()
assert ""min"" in error_msg and ""max"" in error_msg or \
       ""invalid range"" in error_msg or \
       ""singlechannel"" in error_msg

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_min_greater_than_max}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13853,PZ-13837,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7e43a520,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02u0f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:52 AM
Integration - SingleChannel with Invalid Channel (Negative),PZ-13837,43377,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:05 AM,21/Oct/25 11:52 AM,21/Oct/25 11:51 AM,,,0,"h3. Summary

Validates that Focus Server properly rejects SingleChannel configuration requests when the specified channel is a negative value.

h3. Objective

Verify proper error handling when attempting to configure a SingleChannel view with a negative channel ID.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{error-handling}}, {{validation}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-VALIDATION
* *Description*: SingleChannel view must validate channel bounds and reject negative values

h3. Pre-Conditions

# Focus Server is running
# Sensor IDs are expected to be non-negative integers

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_invalid_negative_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": -5,
      ""max"": -5
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{channels.min=-5}}, {{channels.max=-5}}|Payload created|
|2|Send POST /configure|Status 400 or 422 (Validation Error)|
|3|Verify error response message|Error indicates ""negative channel"" or ""invalid channel""|
|4|Verify task was not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400 or 422 for /configure
* *Error message*: ""Channel index cannot be negative"" or similar
* *Behavior*: Request rejected immediately

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_with_negative_channel

config_payload[""channels""][""min""] = -5
config_payload[""channels""][""max""] = -5

with pytest.raises(Exception) as exc_info:
    focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))

assert ""negative"" in str(exc_info.value).lower() or \
       ""invalid"" in str(exc_info.value).lower()

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_with_negative_channel}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13852,PZ-13836,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@280cdcc8,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tx3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:05 AM
Integration - SingleChannel with Invalid Channel (Negative),PZ-13836,43376,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:04 AM,21/Oct/25 11:05 AM,21/Oct/25 11:04 AM,,,0,"h3. Summary

Validates that Focus Server properly rejects SingleChannel configuration requests when the specified channel is a negative value.

h3. Objective

Verify proper error handling when attempting to configure a SingleChannel view with a negative channel ID.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{error-handling}}, {{validation}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-VALIDATION
* *Description*: SingleChannel view must validate channel bounds and reject negative values

h3. Pre-Conditions

# Focus Server is running
# Sensor IDs are expected to be non-negative integers

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_invalid_negative_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": -5,
      ""max"": -5
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Create {{ConfigureRequest}} with {{channels.min=-5}}, {{channels.max=-5}}|Payload created|
|2|Send POST /configure|Status 400 or 422 (Validation Error)|
|3|Verify error response message|Error indicates ""negative channel"" or ""invalid channel""|
|4|Verify task was not created|GET /waterfall returns 404|

h3. Expected Result

* *Status code*: 400 or 422 for /configure
* *Error message*: ""Channel index cannot be negative"" or similar
* *Behavior*: Request rejected immediately

h3. Post-Conditions

* No task created

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_with_negative_channel

config_payload[""channels""][""min""] = -5
config_payload[""channels""][""max""] = -5

with pytest.raises(Exception) as exc_info:
    focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))

assert ""negative"" in str(exc_info.value).lower() or \
       ""invalid"" in str(exc_info.value).lower()

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_with_negative_channel}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13837,PZ-13835,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7e8c35ec,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02twv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:04 AM
Integration - SingleChannel with Invalid Channel (Out of Range High),PZ-13835,43375,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:03 AM,21/Oct/25 11:04 AM,21/Oct/25 11:03 AM,,,0,"h3. Summary

Validates that Focus Server properly rejects SingleChannel configuration requests when the specified channel exceeds the maximum available sensor index.

h3. Objective

Verify proper error handling when attempting to configure a SingleChannel view with a channel ID that is higher than the maximum available sensor.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{error-handling}}, {{validation}}, {{negative-test}}
* *Test Type*: Integration Test (Negative)

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-VALIDATION
* *Description*: SingleChannel view must validate channel bounds and reject out-of-range values

h3. Pre-Conditions

# Focus Server is running
# GET /sensors returns available sensor list (e.g., 0-99 for 100 sensors)
# Maximum sensor ID is known

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_invalid_high_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 9999,
      ""max"": 9999
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Note*: Channel 9999 is intentionally out of range.

h3. Steps

||#||Step Description||Expected Result||
|1|Send GET /sensors to get max available sensor|Status 200, sensors list returned|
|2|Create {{ConfigureRequest}} with {{channels.min=9999}}, {{channels.max=9999}} (out of range)|Payload created|
|3|Send POST /configure|Status 400 or 422 (Bad Request / Validation Error)|
|4|Verify error response contains appropriate message|Error message indicates ""channel out of range"" or similar|
|5|Attempt GET /waterfall/{task_id}/10|Status 404 (task was never created)|

h3. Expected Result

* *Status code*: 400 or 422 for /configure
* *Error message*: ""Channel index 9999 out of range"" or similar validation error
* *Behavior*: Request rejected, no task created

h3. Post-Conditions

* No task created in server memory
* No consumer registered for this task_id

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_with_invalid_channel_high

sensors_response = focus_server_api.get_sensors()
max_sensor = sensors_response.sensors[-1]

invalid_channel = 9999  # Clearly out of range
config_payload[""channels""][""min""] = invalid_channel
config_payload[""channels""][""max""] = invalid_channel

# Expect validation error
with pytest.raises(Exception) as exc_info:
    focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))

# Verify error indicates channel out of range
assert ""out of range"" in str(exc_info.value).lower() or \
       ""invalid channel"" in str(exc_info.value).lower()

# Verify task was not created
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 404, ""Task should not exist""{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_with_invalid_channel_high}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13836,PZ-13834,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@621cb9f9,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02twn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:03 AM
 Integration - SingleChannel Edge Case - Middle Channel,PZ-13834,43374,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:02 AM,21/Oct/25 11:03 AM,21/Oct/25 11:03 AM,,,0,"h3. Summary

Validates SingleChannel view behavior when configuring a middle-range channel, ensuring correct 1:1 mapping and data delivery for a non-boundary sensor.

h3. Objective

Verify that Focus Server correctly handles SingleChannel view for any arbitrary channel in the middle of the sensor range, demonstrating that the feature works beyond just edge cases.

h3. Priority

*Medium*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{functional}}, {{channel-mapping}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-ARBITRARY
* *Description*: SingleChannel view must work for any valid sensor index

h3. Pre-Conditions

# Focus Server is running
# At least 10 sensors available in the system
# Baby Analyzer is processing data

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_middle_ch<middle>_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": <middle_sensor_id>,
      ""max"": <middle_sensor_id>
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Note*: {{<middle_sensor_id>}} is calculated as {{len(sensors) // 2}}.

h3. Steps

||#||Step Description||Expected Result||
|1|Send GET /sensors|Sensors list returned|
|2|Calculate middle sensor ID ({{middle = len(sensors) // 2}})|Middle sensor identified|
|3|Create {{ConfigureRequest}} with {{channels.min=middle}}, {{channels.max=middle}}, {{view_type=SINGLECHANNEL}}|Valid payload|
|4|Send POST /configure|Status 200, success message|
|5|Verify {{stream_amount == 1}}|One stream|
|6|Verify {{channel_to_stream_index == {str(middle): 0}}}|Correct mapping|
|7|Poll GET /waterfall|Status 201, data returned|
|8|Verify sensor ID in all rows matches middle channel|Correct sensor ID|
|9|Validate intensity data|Non-empty, valid values|

h3. Expected Result

* *stream_amount*: 1
* *channel_to_stream_index*: {{{str(middle): 0}}}
* *Data*: All rows contain middle channel sensor with valid intensity

h3. Post-Conditions

* Task active and delivering data for middle channel

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_middle_channel

sensors_response = focus_server_api.get_sensors()
middle_sensor = len(sensors_response.sensors) // 2

config_payload[""channels""][""min""] = middle_sensor
config_payload[""channels""][""max""] = middle_sensor
response = focus_server_api.config_task(task_id, ConfigTaskRequest(**config_payload))

assert response.stream_amount == 1
assert response.channel_to_stream_index == {str(middle_sensor): 0}

waterfall_response = focus_server_api.get_waterfall(task_id, 10)
for data_block in waterfall_response.data:
    for row in data_block.data[0].rows:
        assert row.sensors[0].id == middle_sensor{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_middle_channel}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13835,PZ-13833,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6a6d1f68,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02twf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:02 AM
Integration - SingleChannel Edge Case - Maximum Channel (Last Available),PZ-13833,43373,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 11:01 AM,21/Oct/25 11:02 AM,21/Oct/25 11:01 AM,,,0,"h3. Summary

Validates SingleChannel view behavior when configuring the maximum available channel (last sensor), ensuring correct 1:1 mapping and data delivery for the edge case of the last sensor.

h3. Objective

Verify that Focus Server correctly handles SingleChannel view for the maximum available channel, returning exactly one stream with proper mapping, and delivers valid sensor data for the last channel.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{edge-case}}, {{channel-mapping}}, {{api-integration}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-EDGE
* *Description*: SingleChannel view must correctly handle boundary channels (min/max available sensors)

h3. Pre-Conditions

# Focus Server is running and accessible
# GET /sensors endpoint returns available sensor list with N sensors
# Maximum sensor index (N-1) is available
# Baby Analyzer is processing live data

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_max_ch<max>_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": <max_sensor_id>,
      ""max"": <max_sensor_id>
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

*Note*: {{<max_sensor_id>}} is dynamically determined from GET /sensors response.

h3. Steps

||#||Step Description||Expected Result||
|1|Send GET /sensors to retrieve available sensors|Status 200, sensors list returned|
|2|Extract maximum sensor ID ({{max_sensor = sensors[-1]}})|Maximum sensor ID identified|
|3|Create {{ConfigureRequest}} with {{channels.min=max_sensor}}, {{channels.max=max_sensor}}, {{view_type=SINGLECHANNEL}}|Request payload validated|
|4|Send POST /configure|Status 200, {{status: ""Config received successfully""}}|
|5|Verify response {{stream_amount == 1}}|Exactly one stream configured|
|6|Verify response {{channel_to_stream_index == {str(max_sensor): 0}}}|Correct mapping for max channel|
|7|Verify response {{channel_amount == 1}}|Only one channel configured|
|8|Poll GET /waterfall/{task_id}/10|Status 201, data blocks returned|
|9|Verify each data block contains exactly 1 stream|Only one stream per block|
|10|Verify rows contain sensor ID == max_sensor|All rows have correct sensor ID|
|11|Validate intensity data is non-empty|Valid intensity values|
|12|Validate timestamps are sequential|Proper time ordering|

h3. Expected Result

* *Status code*: 200 for /configure, 201 for /waterfall
* *stream_amount*: 1
* *channel_to_stream_index*: {{{str(max_sensor): 0}}}
* *channel_amount*: 1
* *Data integrity*: All rows contain max_sensor with valid intensity data

h3. Post-Conditions

* Task active and delivering data for maximum channel
* No errors in server logs

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_maximum_channel

# Get available sensors
sensors_response = focus_server_api.get_sensors()
max_sensor = sensors_response.sensors[-1]
logger.info(f""Maximum sensor ID: {max_sensor}"")

# Configure SingleChannel for max sensor
config_payload[""channels""][""min""] = max_sensor
config_payload[""channels""][""max""] = max_sensor
config_request = ConfigTaskRequest(**config_payload)
response = focus_server_api.config_task(task_id, config_request)

# Assertions
assert response.stream_amount == 1
assert response.channel_to_stream_index == {str(max_sensor): 0}
assert response.channel_amount == 1

# Verify data
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
for data_block in waterfall_response.data:
    for row in data_block.data[0].rows:
        assert row.sensors[0].id == max_sensor
        assert len(row.sensors[0].intensity) > 0{code}

h3. Environment

* *Environment Name*: new_production

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_maximum_channel}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13834,PZ-13832,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@58fdc0b5,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tw7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 11:01 AM
Integration - SingleChannel Edge Case - Minimum Channel (Channel 0),PZ-13832,43372,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,21/Oct/25 10:59 AM,21/Oct/25 11:01 AM,21/Oct/25 11:00 AM,,,0,"h3. Summary

Validates SingleChannel view behavior when configuring the minimum available channel (channel 0), ensuring correct 1:1 mapping and data delivery for the edge case of the first sensor.

h3. Objective

Verify that Focus Server correctly handles SingleChannel view for channel 0, returning exactly one stream with proper mapping, and delivers valid sensor data for the first channel.

h3. Priority

*High*

h3. Components/Labels

* *Component*: Focus Server Backend API
* *Labels*: {{singlechannel}}, {{edge-case}}, {{channel-mapping}}, {{api-integration}}
* *Test Type*: Integration Test

h3. Requirements

* *Requirement ID*: FOCUS-API-SINGLECHANNEL-EDGE
* *Description*: SingleChannel view must correctly handle boundary channels (min/max available sensors)

h3. Pre-Conditions

# Focus Server is running and accessible at configured base URL
# Environment configured for {{new_production}}
# GET /sensors endpoint returns available sensor list
# At least sensor 0 exists in the system
# Baby Analyzer process is running and processing live fiber data
# RabbitMQ connection is active for command routing

h3. Test Data

{code:json}{
  ""task_id"": ""singlechannel_min_ch0_<timestamp>"",
  ""config_payload"": {
    ""displayTimeAxisDuration"": 10,
    ""nfftSelection"": 1024,
    ""displayInfo"": {
      ""height"": 1000
    },
    ""channels"": {
      ""min"": 0,
      ""max"": 0
    },
    ""frequencyRange"": {
      ""min"": 0,
      ""max"": 500
    },
    ""start_time"": null,
    ""end_time"": null,
    ""view_type"": 1
  }
}{code}

h3. Steps

||#||Step Description||Expected Result||
|1|Generate unique {{task_id}} for test|Unique task_id created (format: {{singlechannel_min_ch0_<timestamp>}})|
|2|Send GET /sensors to retrieve available sensors|Status 200, sensors list contains 0|
|3|Create {{ConfigureRequest}} with {{view_type=SINGLECHANNEL}}, {{channels.min=0}}, {{channels.max=0}}|Request payload validated by Pydantic model|
|4|Send POST /configure with payload|Status 200, response contains {{status: ""Config received successfully""}}|
|5|Verify response {{stream_amount}}|{{stream_amount == 1}} (exactly one stream)|
|6|Verify response {{channel_to_stream_index}}|Mapping is {{{""0"": 0}}} (channel 0 maps to stream 0)|
|7|Verify response {{channel_amount}}|{{channel_amount == 1}} (only one channel configured)|
|8|Send GET /waterfall/{task_id}/10 to poll data|Status 201, data blocks returned|
|9|Verify each data block contains exactly 1 stream|{{len(data_block.data) == 1}} for all blocks|
|10|Verify rows contain sensor ID 0|All rows have {{sensors[0].id == 0}}|
|11|Validate intensity data is non-empty|{{len(sensors[0].intensity) > 0}}|
|12|Validate timestamps are sequential and increasing|{{startTimestamp <= endTimestamp}}, timestamps increase between rows|

h3. Expected Result

* *Status code*: 200 for /configure, 201 for /waterfall
* *stream_amount*: 1
* *channel_to_stream_index*: {{{""0"": 0}}}
* *channel_amount*: 1
* *Data integrity*: All rows contain sensor 0 with valid intensity data
* *Timestamps*: Sequential and increasing

h3. Post-Conditions

* Task remains active (for live monitoring) or completes successfully (for historic)
* No error messages in Focus Server logs
* RabbitMQ command was successfully routed to Baby Analyzer
* Consumer for {{task_id}} exists in server memory

h3. Assertions (Python Code)

{code:python}# Test function: test_singlechannel_minimum_channel (from test_singlechannel_view_mapping.py)

# Assertion 1: Verify response status
assert isinstance(response, ConfigTaskResponse)
assert response.status == ""Config received successfully""

# Assertion 2: Verify stream amount
assert response.stream_amount == 1, \
    f""Expected stream_amount=1 for SingleChannel, got {response.stream_amount}""

# Assertion 3: Verify channel-to-stream mapping
assert response.channel_to_stream_index == {""0"": 0}, \
    f""Expected mapping {{'0': 0}}, got {response.channel_to_stream_index}""

# Assertion 4: Verify channel amount
assert response.channel_amount == 1, \
    f""Expected channel_amount=1, got {response.channel_amount}""

# Assertion 5: Poll and verify data structure
waterfall_response = focus_server_api.get_waterfall(task_id, 10)
assert waterfall_response.status_code == 201, ""No data received""

# Assertion 6: Verify exactly one stream in data blocks
for data_block in waterfall_response.data:
    assert len(data_block.data) == 1, \
        f""Expected 1 stream per block, got {len(data_block.data)}""

# Assertion 7: Verify sensor ID is 0
for data_block in waterfall_response.data:
    for row in data_block.data[0].rows:  # Access first (and only) stream
        assert row.sensors[0].id == 0, \
            f""Expected sensor ID 0, got {row.sensors[0].id}""
        assert len(row.sensors[0].intensity) > 0, ""Intensity data is empty""{code}

h3. Environment

* *Environment Name*: new_production
* *Focus Server*: [https://10.10.100.100/focus-server/|https://10.10.100.100/focus-server/]
* *MongoDB*: 10.10.100.108:27017
* *RabbitMQ*: 10.10.100.107:5672

h3. Automation Status

* ✅ *Automated*
* *Test Function*: {{test_singlechannel_minimum_channel}}
* *Test File*: {{tests/integration/api/test_singlechannel_view_mapping.py}}
* *Execution*: {{pytest -m ""integration and api"" tests/integration/api/test_singlechannel_view_mapping.py::TestSingleChannelViewEdgeCases::test_singlechannel_minimum_channel}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,PZ-13833,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4117a9ff,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tvz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,21/Oct/25 10:59 AM
API – SingleChannel Rejects Channel Zero,PZ-13824,43360,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:29 PM,20/Oct/25 9:30 PM,20/Oct/25 9:30 PM,,,0,"*Summary:* API – SingleChannel Rejects Channel Zero

*Objective:* Verify that channel 0 is rejected (if channels start from 1).

*Priority:* Low

*Components/Labels:* focus-server, singlechannel, validation, boundary

*Requirements:* FOCUS-CHANNEL-VALIDATION

*Pre-Conditions:*

* PC-001: Channel numbering starts from 1

*Test Data:*

{code:json}Invalid Request:
{
  ""channels"": {""min"": 0, ""max"": 0},  // Channel 0
  ""view_type"": 1
}

Expected: HTTP 400 (if channels start from 1){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request with channel=0|*|Request created|
|2|POST /configure|*|HTTP 400 or success|
|3|Verify behavior|*|Depends on system|

*Expected Result (overall):*

* If channels start from 1: rejected
* If channels start from 0: accepted
* Behavior documented

*Post-Conditions:*

* Depends on system configuration

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 0, ""max"": 0},  # Channel 0
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

try:
    response = focus_server_api.configure(config_request)
    # If accepted, channel 0 is valid
    logger.info(""Channel 0 is valid (channels start from 0)"")
    assert ""0"" in response.channel_to_stream_index
except APIError as e:
    # If rejected, channels start from 1
    logger.info(""Channel 0 rejected (channels start from 1)"")
    assert e.status_code == 400

logger.info(""✅ Channel 0 behavior documented""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_zero_channel}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@154cc634,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tuf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:29 PM
API – SingleChannel Rejects When min ≠ max,PZ-13823,43359,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:28 PM,20/Oct/25 9:29 PM,20/Oct/25 9:29 PM,,,0,"*Summary:* API – SingleChannel Rejects When min ≠ max

*Objective:* Verify that view_type=SINGLECHANNEL requires min=max in channels, and rejects min≠max.

*Priority:* Critical

*Components/Labels:* focus-server, singlechannel, validation, negative-test

*Requirements:* FOCUS-SINGLECHANNEL-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enforces min=max for SINGLECHANNEL

*Test Data:*

{code:json}Invalid Request (min ≠ max):
{
  ""channels"": {""min"": 5, ""max"": 10},  // Invalid for SINGLECHANNEL
  ""view_type"": 1
}

Expected: HTTP 400 or 422{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request|min=5, max=10, view_type=1|Request created|
|2|POST /configure|*|HTTP 400 or 422|
|3|Verify error|*|""SINGLECHANNEL requires min=max""|

*Expected Result (overall):*

* Request rejected
* Clear validation error
* No job created

*Post-Conditions:*

* No job created

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 5, ""max"": 10},  # Invalid for SINGLECHANNEL!
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

try:
    response = focus_server_api.configure(config_request)
    assert False, ""Should have rejected min≠max for SINGLECHANNEL""
except APIError as e:
    assert e.status_code in [400, 422]
    logger.info(f""✓ min≠max rejected for SINGLECHANNEL: {e}"")

logger.info(""✅ SINGLECHANNEL correctly requires min=max""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_min_not_equal_max_should_fail}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b1e4e4a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tu7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:28 PM
API – SingleChannel Rejects Invalid NFFT Value,PZ-13822,43358,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:27 PM,20/Oct/25 9:28 PM,20/Oct/25 9:28 PM,,,0,"*Summary:* API – SingleChannel Rejects Invalid NFFT Value

*Objective:* Verify SingleChannel rejects NFFT that is not power of 2.

*Priority:* Medium

*Components/Labels:* focus-server, singlechannel, validation, nfft

*Requirements:* FOCUS-NFFT-VALIDATION

*Pre-Conditions:*

* PC-001: NFFT validation enabled

*Test Data:*

{code:json}Invalid Request:
{
  ""nfftSelection"": 1000,  // Not power of 2
  ""channels"": {""min"": 10, ""max"": 10},
  ""view_type"": 1
}

Expected: HTTP 400{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request with nfft=1000|*|Request created|
|2|POST /configure|*|HTTP 400|
|3|Verify error|*|""NFFT must be power of 2""|

*Expected Result (overall):*

* Non-power-of-2 NFFT rejected
* Clear error message

*Post-Conditions:*

* No job created

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1000,  # Not power of 2!
    displayInfo={""height"": 1000},
    channels={""min"": 10, ""max"": 10},
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

try:
    response = focus_server_api.configure(config_request)
    assert False, ""Should have rejected non-power-of-2 NFFT""
except (APIError, PydanticValidationError) as e:
    logger.info(f""✓ Invalid NFFT rejected: {e}"")

logger.info(""✅ SingleChannel rejects invalid NFFT""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_invalid_nfft}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5130ec5f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ttz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:27 PM
API – SingleChannel Rejects Invalid Display Height,PZ-13821,43357,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:26 PM,20/Oct/25 9:26 PM,20/Oct/25 9:26 PM,,,0,"*Summary:* API – SingleChannel Rejects Invalid Display Height

*Objective:* Verify SingleChannel rejects invalid display height (zero or negative).

*Priority:* Medium

*Components/Labels:* focus-server, singlechannel, validation, negative-test

*Requirements:* FOCUS-SINGLECHANNEL-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enabled

*Test Data:*

{code:json}Invalid Request:
{
  ""displayInfo"": {""height"": 0},  // Invalid height
  ""channels"": {""min"": 10, ""max"": 10},
  ""view_type"": 1
}

Expected: HTTP 400{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request with height=0|*|Request created|
|2|POST /configure|*|HTTP 400|
|3|Verify error|*|""Invalid height""|

*Expected Result (overall):*

* Zero height rejected
* Clear error message

*Post-Conditions:*

* No job created

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 0},  # Invalid!
    channels={""min"": 10, ""max"": 10},
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

try:
    response = focus_server_api.configure(config_request)
    assert False, ""Should have rejected zero height""
except (APIError, PydanticValidationError) as e:
    logger.info(f""✓ Zero height rejected: {e}"")

logger.info(""✅ SingleChannel rejects invalid height""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_invalid_height}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@66218ef2,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ttr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:26 PM
API – SingleChannel Rejects Invalid Frequency Range,PZ-13820,43356,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:25 PM,20/Oct/25 9:25 PM,20/Oct/25 9:25 PM,,,0,"*Summary:* API – SingleChannel Rejects Invalid Frequency Range

*Objective:* Verify SingleChannel view rejects configurations with invalid frequency ranges (min > max).

*Priority:* Medium

*Components/Labels:* focus-server, singlechannel, validation, negative-test

*Requirements:* FOCUS-SINGLECHANNEL-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enabled

*Test Data:*

{code:json}Invalid Request:
{
  ""channels"": {""min"": 10, ""max"": 10},
  ""frequencyRange"": {""min"": 500, ""max"": 0},  // Invalid: min > max
  ""view_type"": 1
}

Expected: HTTP 400 or validation error{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create invalid request|freq min > max|Request created|
|2|POST /configure|*|HTTP 400|
|3|Verify error message|*|""Invalid frequency range""|

*Expected Result (overall):*

* Request rejected with 400
* Clear error message
* No job created

*Post-Conditions:*

* No job created

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 10, ""max"": 10},
    frequencyRange={""min"": 500, ""max"": 0},  # Invalid!
    view_type=ViewType.SINGLECHANNEL
)

try:
    response = focus_server_api.configure(config_request)
    assert False, ""Should have rejected invalid frequency range""
except APIError as e:
    assert e.status_code == 400
    assert ""frequency"" in str(e).lower() or ""invalid"" in str(e).lower()
    logger.info(f""✓ Invalid frequency rejected: {e}"")

logger.info(""✅ SingleChannel rejects invalid frequency range""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_invalid_frequency_range}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@680b91dd,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ttj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:25 PM
API – SingleChannel View with Various Frequency Ranges,PZ-13819,43355,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:24 PM,20/Oct/25 9:24 PM,20/Oct/25 9:24 PM,,,0,"*Summary:* API – SingleChannel View with Various Frequency Ranges

*Objective:* Verify SingleChannel works with different frequency configurations.

*Priority:* Low

*Components/Labels:* focus-server, singlechannel, frequency

*Requirements:* FOCUS-SINGLECHANNEL-FREQUENCY

*Pre-Conditions:*

* PC-001: Various frequency ranges supported

*Test Data:*

{code:json}Test 1: {""frequencyRange"": {""min"": 0, ""max"": 500}}
Test 2: {""frequencyRange"": {""min"": 100, ""max"": 400}}
Test 3: {""frequencyRange"": {""min"": 0, ""max"": 1000}}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure with freq 0-500|*|Success, mapping correct|
|2|Configure with freq 100-400|*|Success, mapping correct|
|3|Configure with freq 0-1000|*|Success, mapping correct|
|4|Verify all work|*|All pass|

*Expected Result (overall):*

* SingleChannel works with all frequency ranges
* Mapping always correct
* Frequency doesn't affect channel mapping

*Post-Conditions:*

* Multiple jobs created

*Assertions:*

{code:python}frequency_ranges = [
    (0, 500),
    (100, 400),
    (0, 1000)
]

channel = 10

for freq_min, freq_max in frequency_ranges:
    config_request = ConfigureRequest(
        displayTimeAxisDuration=10,
        nfftSelection=1024,
        displayInfo={""height"": 1000},
        channels={""min"": channel, ""max"": channel},
        frequencyRange={""min"": freq_min, ""max"": freq_max},
        view_type=ViewType.SINGLECHANNEL
    )
    
    response = focus_server_api.configure(config_request)
    
    assert response.stream_amount == 1
    assert str(channel) in response.channel_to_stream_index
    assert response.channel_to_stream_index[str(channel)] == 0
    
    logger.info(f""✓ Freq [{freq_min},{freq_max}]: mapping correct"")

logger.info(""✅ SingleChannel works with different frequency ranges""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_with_different_frequency_ranges}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@79bcd78a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ttb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:24 PM
API – Compare SingleChannel vs MultiChannel View Types,PZ-13818,43354,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:22 PM,20/Oct/25 9:23 PM,20/Oct/25 9:23 PM,,,0,"*Summary:* API – Compare SingleChannel vs MultiChannel View Types

*Objective:* Verify distinct behavior between view_type=SINGLECHANNEL (1) and view_type=MULTICHANNEL (0).

*Priority:* Medium

*Components/Labels:* focus-server, view-type, comparison

*Requirements:* FOCUS-VIEWTYPE-COMPARISON

*Pre-Conditions:*

* PC-001: Both view types supported

*Test Data:*

{code:json}MultiChannel Request:
{
  ""channels"": {""min"": 1, ""max"": 10},
  ""view_type"": 0
}
Expected: stream_amount > 1, multiple mappings

SingleChannel Request:
{
  ""channels"": {""min"": 5, ""max"": 5},
  ""view_type"": 1
}
Expected: stream_amount = 1, single mapping{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure MultiChannel|channels 1-10|stream_amount > 1|
|2|Count mappings|*|len(mapping) = 10|
|3|Configure SingleChannel|channel 5 only|stream_amount = 1|
|4|Count mappings|*|len(mapping) = 1|
|5|Compare results|*|Clear difference|

*Expected Result (overall):*

* MultiChannel returns multiple streams
* SingleChannel returns one stream
* Mapping counts differ
* Behavior clearly distinct

*Post-Conditions:*

* Two jobs created with different configs

*Assertions:*

{code:python}# MultiChannel request
multi_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 1, ""max"": 10},
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=0  # MULTICHANNEL
)

multi_response = focus_server_api.configure(multi_request)

assert multi_response.stream_amount > 1
assert len(multi_response.channel_to_stream_index) == 10

logger.info(f""MultiChannel: {multi_response.stream_amount} streams, {len(multi_response.channel_to_stream_index)} mappings"")

# SingleChannel request
single_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 5, ""max"": 5},
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

single_response = focus_server_api.configure(single_request)

assert single_response.stream_amount == 1
assert len(single_response.channel_to_stream_index) == 1

logger.info(f""SingleChannel: {single_response.stream_amount} stream, {len(single_response.channel_to_stream_index)} mapping"")

# Verify difference
assert multi_response.stream_amount != single_response.stream_amount
assert len(multi_response.channel_to_stream_index) != len(single_response.channel_to_stream_index)

logger.info(""✅ SingleChannel vs MultiChannel behavior distinct""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_singlechannel_vs_multichannel_comparison}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b1f9eca,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tt3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:22 PM
API – Same SingleChannel Returns Consistent Mapping Across Multiple Requests,PZ-13817,43353,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:21 PM,20/Oct/25 9:21 PM,20/Oct/25 9:21 PM,,,0,"*Summary:* API – Same SingleChannel Returns Consistent Mapping Across Multiple Requests

*Objective:* Verify that requesting the same single channel multiple times returns consistent mapping.

*Priority:* High

*Components/Labels:* focus-server, singlechannel, consistency, idempotency

*Requirements:* FOCUS-SINGLECHANNEL-IDEMPOTENCY

*Pre-Conditions:*

* PC-001: Focus Server API accessible

*Test Data:*

{code:json}Request (repeated 3 times):
{
  ""channels"": {""min"": 25, ""max"": 25},
  ""view_type"": 1
}

Expected: All 3 responses identical mapping:
{""25"": 0}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure channel 25 (1st)|*|mapping={""25"": 0}|
|2|Configure channel 25 (2nd)|*|mapping={""25"": 0}|
|3|Configure channel 25 (3rd)|*|mapping={""25"": 0}|
|4|Compare all mappings|*|All identical|
|5|Verify consistency|*|Passed|

*Expected Result (overall):*

* All 3 requests return same mapping
* Consistent behavior across calls
* No randomness or drift

*Post-Conditions:*

* 3 jobs created (may be different job_ids)

*Assertions:*

{code:python}channel = 25
mappings = []

for i in range(3):
    config_request = ConfigureRequest(
        displayTimeAxisDuration=10,
        nfftSelection=1024,
        displayInfo={""height"": 1000},
        channels={""min"": channel, ""max"": channel},
        frequencyRange={""min"": 0, ""max"": 500},
        view_type=ViewType.SINGLECHANNEL
    )
    
    response = focus_server_api.configure(config_request)
    mapping = response.channel_to_stream_index
    
    assert str(channel) in mapping
    assert mapping[str(channel)] == 0
    
    mappings.append(mapping)
    logger.info(f""Request {i+1}: mapping = {mapping}"")

# Verify all mappings are identical
for mapping in mappings:
    assert mapping == mappings[0], ""Inconsistent mappings!""

logger.info(""✅ Same channel returns consistent mapping""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_same_channel_multiple_requests_consistent_mapping}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@16e91715,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tsv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:21 PM
API – Different SingleChannels Return Different Mappings,PZ-13816,43352,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:19 PM,20/Oct/25 9:20 PM,20/Oct/25 9:20 PM,,,0,"*Summary:* API – Different SingleChannels Return Different Mappings

*Objective:* Verify that requesting different single channels returns distinct channel_to_stream_index entries.

*Priority:* High

*Components/Labels:* focus-server, singlechannel, mapping-consistency

*Requirements:* FOCUS-SINGLECHANNEL-CONSISTENCY

*Pre-Conditions:*

* PC-001: Focus Server API accessible

*Test Data:*

{code:json}Test 1: {""channels"": {""min"": 5, ""max"": 5}}
Test 2: {""channels"": {""min"": 10, ""max"": 10}}
Test 3: {""channels"": {""min"": 50, ""max"": 50}}

Expected:
- Test 1: {""5"": 0}
- Test 2: {""10"": 0}
- Test 3: {""50"": 0}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure channel 5|min=max=5|mapping={""5"": 0}|
|2|Configure channel 10|min=max=10|mapping={""10"": 0}|
|3|Configure channel 50|min=max=50|mapping={""50"": 0}|
|4|Compare mappings|*|All different channels|
|5|Verify consistency|*|All map to stream 0|

*Expected Result (overall):*

* Each channel gets correct mapping
* Channel key differs in each response
* Stream index always 0 for SingleChannel

*Post-Conditions:*

* Multiple jobs created

*Assertions:*

{code:python}test_channels = [5, 10, 50]
mappings = []

for channel in test_channels:
    config_request = ConfigureRequest(
        displayTimeAxisDuration=10,
        nfftSelection=1024,
        displayInfo={""height"": 1000},
        channels={""min"": channel, ""max"": channel},
        frequencyRange={""min"": 0, ""max"": 500},
        view_type=ViewType.SINGLECHANNEL
    )
    
    response = focus_server_api.configure(config_request)
    
    assert str(channel) in response.channel_to_stream_index
    assert response.channel_to_stream_index[str(channel)] == 0
    
    mappings.append(response.channel_to_stream_index)
    logger.info(f""Channel {channel}: mapping = {response.channel_to_stream_index}"")

# Verify all mappings are different (different channel keys)
assert len(set(str(m) for m in mappings)) == len(mappings)

logger.info(""✅ Different channels return different mappings""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_different_channels_different_mappings}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@168a6b51,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tsn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:19 PM
API – SingleChannel View for Channel 100 (Upper Boundary Test),PZ-13815,43351,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:16 PM,20/Oct/25 9:17 PM,20/Oct/25 9:17 PM,,,0,"*Summary:* API – SingleChannel View for Channel 100 (Upper Boundary Test)

*Objective:* Verify SingleChannel view works correctly for channel 100 (upper boundary or high channel number).

*Priority:* Medium

*Components/Labels:* focus-server, singlechannel, boundary-test

*Requirements:* FOCUS-SINGLECHANNEL-BOUNDARY

*Pre-Conditions:*

* PC-001: Focus Server supports channel 100
* PC-002: Max channels >= 100

*Test Data:*

{code:json}POST /configure
{
  ""channels"": {""min"": 100, ""max"": 100},
  ""view_type"": 1,
  ...
}

Expected:
{
  ""stream_amount"": 1,
  ""channel_to_stream_index"": {""100"": 0}
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request|channel 100|Request created|
|2|POST /configure|*|HTTP 200|
|3|Validate mapping|*|{""100"": 0}|
|4|Verify no overflow|*|Success|

*Expected Result (overall):*

* Channel 100 works correctly
* No overflow or boundary errors
* Correct mapping

*Post-Conditions:*

* Job created for channel 100

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 100, ""max"": 100},  # Channel 100 (upper boundary)
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

response = focus_server_api.configure(config_request)

assert response.stream_amount == 1
assert ""100"" in response.channel_to_stream_index
assert response.channel_to_stream_index[""100""] == 0

logger.info(""✅ SingleChannel works for channel 100 (upper boundary)""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_configure_singlechannel_channel_100}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4b89e60d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tsf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:16 PM
API – SingleChannel View for Channel 1 (First Channel),PZ-13814,43350,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:16 PM,20/Oct/25 9:16 PM,20/Oct/25 9:16 PM,,,0,"*Summary:* API – SingleChannel View for Channel 1 (First Channel)

*Objective:* Verify SingleChannel view works correctly for channel 1 (lower boundary test).

*Priority:* Medium

*Components/Labels:* focus-server, singlechannel, boundary-test

*Requirements:* FOCUS-SINGLECHANNEL-BOUNDARY

*Pre-Conditions:*

* PC-001: Focus Server API accessible
* PC-002: Channel 1 valid (first channel)

*Test Data:*

{code:json}POST /configure
{
  ""channels"": {""min"": 1, ""max"": 1},
  ""view_type"": 1,
  ...
}

Expected:
{
  ""stream_amount"": 1,
  ""channel_to_stream_index"": {""1"": 0},
  ""channel_amount"": 1
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create request|channel 1|Request created|
|2|POST /configure|*|HTTP 200|
|3|Validate stream_amount|*|== 1|
|4|Validate mapping|*|{""1"": 0}|
|5|Check no errors|*|Success|

*Expected Result (overall):*

* Channel 1 works as SingleChannel
* Correct mapping returned
* No boundary errors

*Post-Conditions:*

* Job created for channel 1

*Assertions:*

{code:python}config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 1, ""max"": 1},  # Channel 1 (lower boundary)
    frequencyRange={""min"": 0, ""max"": 500},
    view_type=ViewType.SINGLECHANNEL
)

response = focus_server_api.configure(config_request)

assert response.stream_amount == 1
assert len(response.channel_to_stream_index) == 1
assert ""1"" in response.channel_to_stream_index
assert response.channel_to_stream_index[""1""] == 0

logger.info(""✅ SingleChannel works for channel 1 (lower boundary)""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_configure_singlechannel_channel_1}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@699d6790,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ts7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:16 PM
API – SingleChannel View Returns Correct 1:1 Mapping,PZ-13813,43349,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:14 PM,20/Oct/25 9:15 PM,20/Oct/25 9:15 PM,,,0,"*Summary:* API – SingleChannel View Returns Correct 1:1 Mapping

*Objective:* Validate that view_type=SINGLECHANNEL returns exactly one stream with correct 1:1 channel-to-stream mapping for the requested channel.

*Priority:* High

*Components/Labels:* focus-server, api, view-type, singlechannel, mapping

*Requirements:* FOCUS-API-VIEWTYPE, FOCUS-SINGLECHANNEL

*Pre-Conditions:*

* PC-001: Focus Server API accessible
* PC-002: /configure endpoint available
* PC-003: view_type=1 (SINGLECHANNEL) supported

*Test Data:*

{code:json}POST /configure
{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": {""height"": 1000},
  ""channels"": {""min"": 7, ""max"": 7},
  ""frequencyRange"": {""min"": 0, ""max"": 500},
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 1
}

Expected Response:
{
  ""job_id"": ""string"",
  ""stream_amount"": 1,
  ""stream_port"": ""integer"",
  ""stream_url"": ""string"",
  ""channel_to_stream_index"": {""7"": 0},
  ""channel_amount"": 1,
  ""frequencies_list"": [...],
  ""frequencies_amount"": ""integer"",
  ""lines_dt"": ""number"",
  ""status"": ""string"",
  ""view_type"": 1
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create ConfigureRequest|view_type=1, channels={min:7,max:7}|Request created|
|2|POST /configure|Request payload|HTTP 200 OK|
|3|Validate response schema|*|Valid ConfigureResponse|
|4|Check stream_amount|response.stream_amount|== 1|
|5|Check channel_to_stream_index size|len(mapping)|== 1|
|6|Verify mapping entry|mapping[""7""]|== 0|
|7|Check channel_amount|response.channel_amount|== 1|
|8|Verify no extra channels|*|Only channel 7|
|9|Validate view_type|response.view_type|== 1|

*Expected Result (overall):*

* Configuration accepted (HTTP 200)
* Exactly one stream created (stream_amount=1)
* Single mapping: channel ""7"" → stream index 0
* No extraneous channels in mapping
* view_type returned as 1 (SINGLECHANNEL)

*Post-Conditions:*

* Job created with correct configuration
* Baby Analyzer initialized for single channel

*Assertions:*

{code:python}from src.models.focus_server_models import ConfigureRequest, ConfigureResponse, ViewType

# Create request
config_request = ConfigureRequest(
    displayTimeAxisDuration=10,
    nfftSelection=1024,
    displayInfo={""height"": 1000},
    channels={""min"": 7, ""max"": 7},  # Single channel
    frequencyRange={""min"": 0, ""max"": 500},
    start_time=None,
    end_time=None,
    view_type=ViewType.SINGLECHANNEL  # view_type = 1
)

# Send request
response = focus_server_api.configure(config_request)

# Validate response
assert isinstance(response, ConfigureResponse)
assert response.stream_amount == 1, f""Expected 1 stream, got {response.stream_amount}""

# Validate mapping
channel_mapping = response.channel_to_stream_index
assert len(channel_mapping) == 1, f""Expected 1 mapping, got {len(channel_mapping)}""
assert ""7"" in channel_mapping, ""Channel 7 not in mapping""
assert channel_mapping[""7""] == 0, f""Expected stream 0, got {channel_mapping['7']}""

# Validate channel amount
assert response.channel_amount == 1, f""Expected 1 channel, got {response.channel_amount}""

# Validate view type
assert response.view_type == 1, f""Expected view_type=1, got {response.view_type}""

logger.info(""✅ SingleChannel mapping correct for channel 7""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_configure_singlechannel_mapping}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@79b29521,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02trz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:14 PM
Data Quality – Verify Recordings Have Complete Metadata,PZ-13812,43348,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:10 PM,20/Oct/25 9:10 PM,20/Oct/25 9:10 PM,,,0,"*Summary:* Data Quality – Verify Recordings Have Complete Metadata

*Objective:* Confirm recordings have all required metadata fields populated (not null/empty).

*Priority:* Medium

*Components/Labels:* mongodb, data-quality, metadata

*Requirements:* FOCUS-MONGO-METADATA

*Pre-Conditions:*

* PC-001: recordings collection accessible
* PC-002: Sample size >= 10 recordings

*Test Data:*

{code:json}Required Non-Empty Fields:
[
  ""uuid"",
  ""start_time"",
  ""end_time"",
  ""path"",
  ""node""
]{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to MongoDB|*|Connected|
|2|Query 10 recent recordings|limit=10|Recordings retrieved|
|3|For each recording|*|Check all fields|
|4|Verify uuid not empty|uuid != """"|True for all|
|5|Verify times not zero|start_time > 0|True for all|
|6|Verify path not empty|path != """"|True for all|
|7|Count complete records|*|All 10 complete|

*Expected Result (overall):*

* All sampled recordings have complete metadata
* No null or empty required fields
* High data quality

*Post-Conditions:*

* None

*Assertions:*

{code:python}client = MongoClient(host=""10.10.100.108"", port=27017,
                     username=""prisma"", password=""prisma"")
db = client[""prisma""]
recordings = db[""recordings""]

# Get sample
sample = list(recordings.find().sort(""start_time"", -1).limit(10))

assert len(sample) >= 10, f""Not enough recordings: {len(sample)}""

required_fields = [""uuid"", ""start_time"", ""end_time"", ""path""]

for idx, rec in enumerate(sample, 1):
    logger.info(f""Checking recording {idx}/{len(sample)}"")
    
    for field in required_fields:
        value = rec.get(field)
        assert value is not None, f""Field '{field}' is None""
        assert value != """", f""Field '{field}' is empty""
        
        if field in [""start_time"", ""end_time""]:
            assert value > 0, f""Field '{field}' is zero""

logger.info(""✅ All recordings have complete metadata"")
client.close(){code}

*Environment:* Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_recordings_have_all_required_metadata}}  
*Test File:* {{tests/integration/infrastructure/test_mongodb_data_quality.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7d0fad94,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02trr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:10 PM
Data Quality – Validate Recordings Document Schema,PZ-13811,43347,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:09 PM,20/Oct/25 9:09 PM,20/Oct/25 9:09 PM,,,0,"*Summary:* Data Quality – Validate Recordings Document Schema

*Objective:* Verify recordings in MongoDB have all required fields with correct data types.

*Priority:* High

*Components/Labels:* mongodb, data-quality, schema, recordings

*Requirements:* FOCUS-MONGO-SCHEMA

*Pre-Conditions:*

* PC-001: recordings collection has data
* PC-002: At least one recording exists

*Test Data:*

{code:json}Required Recording Fields:
{
  ""uuid"": ""string"",
  ""start_time"": ""number (epoch)"",
  ""end_time"": ""number (epoch)"",
  ""path"": ""string"",
  ""node"": ""string"",
  ""sensor_min"": ""number"",
  ""sensor_max"": ""number""
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to MongoDB|*|Connected|
|2|Query recent recording|limit=1, sort=-start_time|Recording found|
|3|Verify uuid field|type(doc[""uuid""])|str|
|4|Verify start_time|type(doc[""start_time""])|int/float|
|5|Verify end_time|type(doc[""end_time""])|int/float|
|6|Verify path|type(doc[""path""])|str|
|7|Check start < end|start_time < end_time|True|
|8|Validate all required fields|*|All present|

*Expected Result (overall):*

* Recording has all required fields
* Field types correct
* start_time < end_time
* uuid is valid string
* path is valid file path

*Post-Conditions:*

* None

*Assertions:*

{code:python}client = MongoClient(host=""10.10.100.108"", port=27017,
                     username=""prisma"", password=""prisma"")
db = client[""prisma""]
recordings = db[""recordings""]

# Get one recording
recording = recordings.find_one(sort=[(""start_time"", -1)])

assert recording is not None, ""No recordings found""

# Validate schema
required_fields = [""uuid"", ""start_time"", ""end_time"", ""path""]

for field in required_fields:
    assert field in recording, f""Missing field: {field}""
    logger.info(f""✓ Field '{field}' present"")

# Validate types
assert isinstance(recording[""uuid""], str)
assert isinstance(recording[""start_time""], (int, float))
assert isinstance(recording[""end_time""], (int, float))
assert isinstance(recording[""path""], str)

# Validate logic
assert recording[""start_time""] < recording[""end_time""], ""Invalid time range""

logger.info(""✅ Recording schema valid"")
client.close(){code}

*Environment:* Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_recording_schema_validation}}  
*Test File:* {{tests/integration/infrastructure/test_mongodb_data_quality.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@481094ad,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02trj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:09 PM
Data Quality – Verify Critical MongoDB Indexes Exist,PZ-13810,43346,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:08 PM,20/Oct/25 9:08 PM,20/Oct/25 9:08 PM,,,0,"*Summary:* Data Quality – Verify Critical MongoDB Indexes Exist

*Objective:* Confirm performance-critical indexes exist on recordings collection.

*Priority:* High

*Components/Labels:* mongodb, indexes, performance

*Requirements:* FOCUS-MONGO-INDEXES

*Pre-Conditions:*

* PC-001: recordings collection exists
* PC-002: Index creation completed

*Test Data:*

{code:json}Required Indexes on ""recordings"":
[
  {""field"": ""start_time"", ""type"": ""ascending""},
  {""field"": ""end_time"", ""type"": ""ascending""},
  {""field"": ""uuid"", ""type"": ""ascending""},
  {""field"": ""_id"", ""type"": ""ascending""}
]{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to MongoDB|*|Connected|
|2|Get recordings collection|db[""recordings""]|Collection object|
|3|List indexes|collection.list_indexes()|Index list|
|4|Verify start_time index|""start_time_1"" in indexes|True|
|5|Verify end_time index|""end_time_1"" in indexes|True|
|6|Verify uuid index|""uuid_1"" in indexes|True|
|7|Log index details|*|All indexes logged|

*Expected Result (overall):*

* start_time index exists
* end_time index exists
* uuid index exists
* All indexes are ascending

*Post-Conditions:*

* None

*Assertions:*

{code:python}client = MongoClient(host=""10.10.100.108"", port=27017,
                     username=""prisma"", password=""prisma"")
db = client[""prisma""]
collection = db[""recordings""]

# Get indexes
indexes = list(collection.list_indexes())
index_names = [idx['name'] for idx in indexes]

logger.info(f""Found {len(indexes)} indexes:"")
for idx in indexes:
    logger.info(f""  - {idx['name']}: {idx['key']}"")

# Verify required indexes
required_indexes = [""start_time_1"", ""end_time_1"", ""uuid_1""]

for idx_name in required_indexes:
    assert idx_name in index_names, f""Missing index: {idx_name}""
    logger.info(f""✓ Index '{idx_name}' exists"")

logger.info(""✅ All critical indexes exist"")
client.close(){code}

*Environment:* Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_mongodb_indexes_exist_and_optimal}}  
*Test File:* {{tests/integration/infrastructure/test_mongodb_data_quality.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5eb9c0d2,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02trb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:08 PM
Data Quality – Verify Required MongoDB Collections Exist,PZ-13809,43345,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:03 PM,20/Oct/25 9:03 PM,20/Oct/25 9:03 PM,,,0,"*Summary:* Data Quality – Verify Required MongoDB Collections Exist

*Objective:* Confirm all required MongoDB collections exist for Focus Server operation.

*Priority:* Critical

*Components/Labels:* mongodb, data-quality, collections

*Requirements:* FOCUS-MONGO-SCHEMA

*Pre-Conditions:*

* PC-001: MongoDB accessible
* PC-002: Database ""prisma"" exists

*Test Data:*

{code:json}Required Collections:
[
  ""recordings"",
  ""node4"",
  ""tasks"",
  ""jobs""
]{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to MongoDB|database=""prisma""|Connected|
|2|List collections|db.list_collection_names()|Collection list|
|3|Verify ""recordings""|""recordings"" in collections|True|
|4|Verify ""node4""|""node4"" in collections|True|
|5|Verify ""tasks""|""tasks"" in collections|True|
|6|Verify ""jobs""|""jobs"" in collections|True|
|7|Check collection counts|db[col].count_documents({})|{quote}= 0 {quote}|

*Expected Result (overall):*

* All required collections exist
* Collections are accessible
* No permission errors

*Post-Conditions:*

* None

*Assertions:*

{code:python}client = MongoClient(host=""10.10.100.108"", port=27017,
                     username=""prisma"", password=""prisma"")
db = client[""prisma""]

collections = db.list_collection_names()

required_collections = [""recordings"", ""node4"", ""tasks"", ""jobs""]

for col_name in required_collections:
    assert col_name in collections, f""Collection '{col_name}' not found""
    logger.info(f""✓ Collection '{col_name}' exists"")
    
    # Check accessible
    count = db[col_name].count_documents({})
    logger.info(f""  Documents: {count}"")

logger.info(""✅ All required collections exist"")
client.close(){code}

*Environment:* Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_required_collections_exist}}  
*Test File:* {{tests/integration/infrastructure/test_mongodb_data_quality.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4c55d86a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tr3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:03 PM
Infrastructure – MongoDB Quick Response Time Test (Performance),PZ-13808,43344,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 9:01 PM,20/Oct/25 9:02 PM,,,,0,"*Summary:* Infrastructure – MongoDB Quick Response Time Test

*Objective:* Verify MongoDB responds to ping within acceptable time limit (<100ms).

*Priority:* Medium

*Components/Labels:* mongodb, performance, latency

*Requirements:* FOCUS-MONGO-PERFORMANCE

*Pre-Conditions:*

* PC-001: MongoDB accessible
* PC-002: Normal load conditions

*Test Data:*

{code:json}Performance Criteria:
{
  ""max_response_time_ms"": 100,
  ""acceptable_response_time_ms"": 50
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to MongoDB|*|Connected|
|2|Record start time|time.time()|Timestamp captured|
|3|Send ping command|client.admin.command('ping')|Response received|
|4|Record end time|time.time()|Timestamp captured|
|5|Calculate latency|(end - start) * 1000|Latency in ms|
|6|Verify latency|latency < 100ms|Pass if under threshold|
|7|Log result|*|Latency logged|

*Expected Result (overall):*

* Ping response time < 100ms
* Ideally < 50ms
* Consistent performance

*Post-Conditions:*

* Connection closed

*Assertions:*

{code:python}import time

client = MongoClient(host=""10.10.100.108"", port=27017)

# Measure ping time
start_time = time.time()
ping_result = client.admin.command('ping')
end_time = time.time()

latency_ms = (end_time - start_time) * 1000

assert ping_result['ok'] == 1.0
assert latency_ms < 100, f""MongoDB ping too slow: {latency_ms:.2f}ms""

logger.info(f""✅ MongoDB ping: {latency_ms:.2f}ms"")

if latency_ms < 50:
    logger.info(""⚡ Excellent latency!"")
elif latency_ms < 100:
    logger.info(""✓ Acceptable latency"")

client.close(){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_quick_mongodb_ping}}  
*Test File:* {{tests/integration/infrastructure/test_basic_connectivity.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@11b17c29,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tqv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 9:01 PM
Infrastructure – MongoDB Connection Using Focus Server Config,PZ-13807,43343,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:57 PM,20/Oct/25 8:58 PM,,,,0,"*Summary:* Infrastructure – MongoDB Connection Using Focus Server Config

*Objective:* Verify MongoDB connection works using Focus Server's ConfigManager for consistent configuration across application.

*Priority:* High

*Components/Labels:* focus-server, mongodb, config-manager

*Requirements:* FOCUS-CONFIG-MONGODB

*Pre-Conditions:*

* PC-001: ConfigManager configured
* PC-002: MongoDB config in environments.yaml

*Test Data:*

{code:yaml}new_production:
  mongodb:
    host: ""10.10.100.108""
    port: 27017
    username: ""username""
    password: ""password""
    database: ""prisma""
    auth_source: ""prisma""{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Initialize ConfigManager|env=""new_production""|Manager created|
|2|Get database config|get_database_config()|Config dict returned|
|3|Verify host|config[""host""]|""10.10.100.108""|
|4|Verify port|config[""port""]|27017|
|5|Create MongoDB client|Using config|Client created|
|6|Test connection|ping|Success|

*Expected Result (overall):*

* ConfigManager loads MongoDB config correctly
* All connection parameters accurate
* Connection successful using config

*Post-Conditions:*

* Connection closed

*Assertions:*

{code:python}config_manager = ConfigManager(""new_production"")
mongo_config = config_manager.get_database_config()

assert mongo_config[""host""] == ""10.10.100.108""
assert mongo_config[""port""] == 27017
assert mongo_config[""username""] == ""username""
assert mongo_config[""database""] == ""database""

# Test connection with config
client = MongoClient(**mongo_config)
ping_result = client.admin.command('ping')
assert ping_result['ok'] == 1.0

client.close()
logger.info(""✅ MongoDB connection via ConfigManager works""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_mongodb_connection}}  
*Test File:* {{tests/integration/infrastructure/test_basic_connectivity.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4a034c1f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tqn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:57 PM
Infrastructure – MongoDB Direct TCP Connection and Authentication,PZ-13806,43342,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:55 PM,20/Oct/25 8:56 PM,,,,0,"*Summary:* Infrastructure – MongoDB Direct TCP Connection and Authentication

*Objective:* Verify Focus Server can establish direct TCP connection to MongoDB server, authenticate successfully, and execute basic ping command.

*Priority:* Critical

*Components/Labels:* focus-server, mongodb, infrastructure, database, connectivity

*Requirements:* FOCUS-INFRA-MONGODB, FOCUS-DB-CONNECTION

*Pre-Conditions:*

* PC-001: MongoDB server running at configured host/port
* PC-002: MongoDB credentials configured (username/password)
* PC-003: Authentication database accessible
* PC-004: pymongo client library installed

*Test Data:*

{code:json}MongoDB Configuration:
{
  ""host"": ""10.10.100.108"",
  ""port"": 27017,
  ""username"": ""username"",
  ""password"": ""password"",
  ""auth_source"": ""prisma"",
  ""database"": ""prisma"",
  ""connection_timeout_ms"": 10000,
  ""server_selection_timeout_ms"": 10000
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Load MongoDB config|ConfigManager(""new_production"")|Config loaded|
|2|Create MongoClient|host, port, credentials|Client created|
|3|Establish TCP connection|*|Connection established|
|4|Authenticate|authSource=""prisma""|Authentication success|
|5|Send ping command|client.admin.command('ping')|{'ok': 1.0}|
|6|Get server info|client.server_info()|Version returned|
|7|List databases|client.list_database_names()|Database list returned|
|8|Verify prisma DB exists|""prisma"" in db_list|True|
|9|Close connection|client.close()|Clean disconnect|

*Expected Result (overall):*

* TCP connection to MongoDB successful
* Authentication passes with prisma/prisma credentials
* Ping command returns {'ok': 1.0}
* Server version retrieved (e.g., ""7.0.5"")
* Database list includes ""prisma""
* Connection closes cleanly without errors

*Post-Conditions:*

* MongoDB connection closed
* No hanging connections

*Assertions:*

{code:python}from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError

config_manager = ConfigManager(""new_production"")
mongo_config = config_manager.get_database_config()

# Create client
client = MongoClient(
    host=mongo_config[""host""],  # 10.10.100.108
    port=mongo_config[""port""],  # 27017
    username=mongo_config[""username""],  
    password=mongo_config[""password""],  
    authSource=mongo_config.get(""auth_source"", ""admin""),
    serverSelectionTimeoutMS=10000,
    connectTimeoutMS=10000,
    socketTimeoutMS=10000
)

# Test connection with ping
ping_result = client.admin.command('ping')
assert ping_result['ok'] == 1.0

# Get server info
server_info = client.server_info()
assert 'version' in server_info
logger.info(f""MongoDB Version: {server_info['version']}"")

# List databases
db_list = client.list_database_names()
assert 'prisma' in db_list
assert len(db_list) > 0

# Close
client.close()
logger.info(""✅ MongoDB connectivity test PASSED""){code}

*Environment:* Staging, Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_mongodb_direct_connection}}  
*Test File:* {{tests/integration/infrastructure/test_basic_connectivity.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@32985622,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tqf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:55 PM
Integration – Dynamic Visualization – Colormap Change Commands,PZ-13805,43341,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:51 PM,20/Oct/25 8:51 PM,,,,0,"*Summary:* Dynamic Visualization – Colormap Change Commands

*Objective:* Verify colormap selection commands work via RabbitMQ.

*Priority:* Low

*Components/Labels:* focus-server, colormap, rabbitmq

*Requirements:* FOCUS-COLORMAP

*Pre-Conditions:*

* PC-001: Supported colormaps known (e.g., ""jet"", ""viridis"")

*Test Data:*

{code:json}Colormap Command:
{
  ""command_type"": ""ColormapCommand"",
  ""colormap"": ""viridis""
}

Supported: jet, viridis, plasma, hot, cool{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Send colormap command|""viridis""|Accepted|
|2|Verify visualization|*|Uses new colormap|

*Expected Result (overall):*

* Colormap changed dynamically
* Visualization updated

*Post-Conditions:*

* New colormap active

*Assertions:*

{code:python}colormap = ""viridis""

mq_client.send_colormap_command(colormap=colormap)
logger.info(f""✅ Colormap changed to {colormap}""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_colormap_commands}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@569fa6a4,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tq7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:51 PM
Integration – Valid CAxis Range,PZ-13804,43340,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:49 PM,20/Oct/25 8:50 PM,,,,0,"*Summary:* Dynamic Visualization – Valid CAxis Adjustment

*Objective:* Confirm valid CAxis ranges are accepted.

*Priority:* Medium

*Components/Labels:* focus-server, caxis, colormap

*Requirements:* FOCUS-CAXIS

*Pre-Conditions:*

* PC-001: Baby Analyzer running

*Test Data:*

{code:json}Valid CAxis:
{
  ""caxis_min"": -80,
  ""caxis_max"": -10
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define valid range|[-80, -10]|Valid|
|2|Send command|*|Accepted|
|3|Verify applied|GET /waterfall|New range used|

*Expected Result (overall):*

* Valid CAxis accepted
* Visualization updated

*Post-Conditions:*

* Colormap uses new range

*Assertions:*

{code:python}caxis_min = -80
caxis_max = -10

assert caxis_min < caxis_max  # Valid

mq_client.send_caxis_adjustment(caxis_min, caxis_max)
logger.info(""✅ Valid CAxis applied""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_valid_caxis_range}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2ef130d8,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tpz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:49 PM
Integration – Invalid CAxis Range (General),PZ-13803,43339,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:48 PM,20/Oct/25 8:48 PM,,,,0,"*Summary:* Dynamic Visualization – Various Invalid CAxis Scenarios

*Objective:* Test multiple invalid CAxis scenarios.

*Priority:* Low

*Components/Labels:* focus-server, caxis, validation

*Requirements:* FOCUS-CAXIS-VALIDATION

*Pre-Conditions:*

* PC-001: Validation rules defined

*Test Data:*

{code:json}Scenarios:
1. {""min"": 0, ""max"": 0} - Zero range
2. {""min"": null, ""max"": -20} - Null value
3. {""min"": ""invalid"", ""max"": -20} - Wrong type{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Test zero range|min=0, max=0|Rejected|
|2|Test null value|min=null|Rejected|
|3|Test wrong type|min=""string""|Rejected|

*Expected Result (overall):*

* All invalid scenarios rejected
* Clear error messages

*Post-Conditions:*

* System state unchanged

*Assertions:*

{code:python}# Zero range
try:
    mq_client.send_caxis_adjustment(0, 0)
    assert False
except:
    logger.info(""✅ Zero range rejected"")

# Other invalid cases...{code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_invalid_caxis_range}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@221d57ee,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tpr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:48 PM
Integration – CAxis with Invalid Range (Min > Max),PZ-13802,43338,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:47 PM,20/Oct/25 8:47 PM,,,,0,"*Summary:* Dynamic Visualization – Reject Invalid CAxis Range

*Objective:* Ensure invalid CAxis ranges (min > max) are rejected.

*Priority:* Medium

*Components/Labels:* focus-server, caxis, validation, negative-test

*Requirements:* FOCUS-CAXIS-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enabled

*Test Data:*

{code:json}Invalid CAxis:
{
  ""caxis_min"": -20,
  ""caxis_max"": -60
}

Reversed range (min > max){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define invalid CAxis|min=-20, max=-60|Reversed|
|2|Validate range|min > max|Invalid|
|3|Attempt send|*|Should fail or warn|

*Expected Result (overall):*

* Validation detects reversed range
* Command rejected or warning issued

*Post-Conditions:*

* Invalid command not applied

*Assertions:*

{code:python}caxis_min = -20
caxis_max = -60

assert caxis_min > caxis_max  # Invalid!

# Should raise exception or validation error
try:
    mq_client.send_caxis_adjustment(caxis_min, caxis_max)
    assert False, ""Should have rejected invalid CAxis""
except Exception as e:
    logger.info(f""✅ Invalid CAxis correctly rejected: {e}""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_caxis_with_invalid_range}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4bead179,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tpj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:47 PM
Integration – CAxis Adjustment Command,PZ-13801,43337,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:46 PM,20/Oct/25 8:46 PM,,,,0,"*Summary:* Dynamic Visualization – Send CAxis Adjustment Command

*Objective:* Verify CAxis (color axis) adjustment commands can be sent via RabbitMQ for dynamic colormap range changes.

*Priority:* Medium

*Components/Labels:* focus-server, caxis, colormap, rabbitmq

*Requirements:* FOCUS-CAXIS-COMMAND

*Pre-Conditions:*

* PC-001: RabbitMQ accessible
* PC-002: Baby Analyzer supports CAxis commands

*Test Data:*

{code:json}CAxis Command:
{
  ""command_type"": ""CAxisAdjustmentCommand"",
  ""caxis_min"": -60,
  ""caxis_max"": -20,
  ""routing_key"": ""caxis""
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to RabbitMQ|*|Connected|
|2|Define CAxis range|min=-60, max=-20|Range defined|
|3|Create CAxis command|*|Command created|
|4|Publish to exchange|routing_key=""caxis""|Published|
|5|Verify no errors|*|Success|

*Expected Result (overall):*

* CAxis command sent successfully
* Baby Analyzer receives command
* Colormap range adjusted dynamically

*Post-Conditions:*

* Waterfall visualization uses new color range

*Assertions:*

{code:python}caxis_min = -60
caxis_max = -20

assert caxis_min < caxis_max  # Valid range

mq_client.send_caxis_adjustment(
    caxis_min=caxis_min,
    caxis_max=caxis_max,
    routing_key=""caxis""
)

logger.info(""✅ CAxis command sent successfully""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_caxis_adjustment}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4427988d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tpb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:46 PM
Integration – Safe ROI Change (Within Limits),PZ-13800,43336,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:45 PM,20/Oct/25 8:45 PM,,,,0,"*Summary:* Dynamic ROI – Validate Safe ROI Change

*Objective:* Confirm safe ROI changes pass validation.

*Priority:* High

*Components/Labels:* focus-server, roi, safety, validation

*Requirements:* FOCUS-ROI-SAFETY

*Pre-Conditions:*

* PC-001: Safety rules configured

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 100}
Safe New ROI: {""min"": 10, ""max"": 110}
Change: +10 shift, 90% overlap{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Calculate overlap|[10,100] overlap|90%|
|2|Calculate size change|100 vs 100|0%|
|3|Run validation|*|Passes|
|4|Check is_safe|*|True|

*Expected Result (overall):*

* Safe change approved
* No warnings
* Command sent successfully

*Post-Conditions:*

* ROI changed to new range

*Assertions:*

{code:python}current_min = 0
current_max = 100
new_min = 10
new_max = 110

safety_result = validate_roi_change_safety(
    current_min=current_min,
    current_max=current_max,
    new_min=new_min,
    new_max=new_max,
    max_change_percent=50.0
)

assert safety_result[""is_safe""] == True
assert safety_result[""overlap_percent""] >= 80
assert len(safety_result[""warnings""]) == 0

mq_client.send_roi_change(start=new_min, end=new_max)
logger.info(""✅ Safe ROI change approved""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_safe_roi_change}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@33a67bc5,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tp3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:45 PM
Integration – Unsafe ROI Shift (Large Position Change),PZ-13799,43335,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:43 PM,20/Oct/25 8:44 PM,,,,0,"*Summary:* Dynamic ROI – Detect Unsafe Position Shift

*Objective:* Verify detection when ROI shifts position dramatically.

*Priority:* Medium

*Components/Labels:* focus-server, roi, safety, validation

*Requirements:* FOCUS-ROI-SAFETY

*Pre-Conditions:*

* PC-001: Safety rules for position shifts

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 100}
New ROI: {""min"": 150, ""max"": 250}
Shift: +150 sensors, 0% overlap{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Calculate shift|new_min - current_min|+150|
|2|Calculate overlap|*|0%|
|3|Run validation|*|Warning issued|

*Expected Result (overall):*

* Large shift detected
* Low overlap flagged

*Post-Conditions:*

* Warning logged

*Assertions:*

{code:python}current_min = 0
current_max = 100
new_min = 150
new_max = 250

shift = new_min - current_min
assert shift == 150  # Large shift

safety_result = validate_roi_change_safety(
    current_min=current_min,
    current_max=current_max,
    new_min=new_min,
    new_max=new_max
)

assert safety_result[""overlap_percent""] == 0
logger.info(""✅ Large shift detected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_unsafe_roi_shift}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@69c0fb4f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tov:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:43 PM
Integration - Unsafe ROI Range Change (Size Change > 50%),PZ-13798,43334,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:42 PM,20/Oct/25 8:42 PM,,,,0,"*Summary:* Dynamic ROI – Detect Unsafe Size Change

*Objective:* Verify detection of unsafe size changes (e.g., doubling or halving ROI).

*Priority:* High

*Components/Labels:* focus-server, roi, safety, validation

*Requirements:* FOCUS-ROI-SAFETY

*Pre-Conditions:*

* PC-001: Safety rules configured

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 100, ""size"": 100}
Unsafe New ROI: {""min"": 0, ""max"": 250, ""size"": 250}
Size change: 150% (unsafe){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Calculate size change|(250-100)/100|150% increase|
|2|Run validation|max_change=50%|Fails|
|3|Check warnings|*|""size change too large""|

*Expected Result (overall):*

* Size change > 50% detected as unsafe
* Warning issued

*Post-Conditions:*

* Change logged

*Assertions:*

{code:python}current_size = 100
new_size = 250
size_change_percent = ((new_size - current_size) / current_size) * 100

assert size_change_percent == 150  # > 50% threshold

safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=0,
    new_max=250,
    max_change_percent=50.0
)

assert safety_result[""is_safe""] == False
logger.info(""✅ Unsafe size change detected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_unsafe_roi_range_change}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1c62d83d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ton:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:42 PM
Integration - Unsafe ROI Change (Large Jump),PZ-13797,43333,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:40 PM,20/Oct/25 8:41 PM,,,,0,"*Summary:* Dynamic ROI – Detect Unsafe ROI Change (Large Jump)

*Objective:* Verify safety validation detects unsafe ROI changes (e.g., >50% change).

*Priority:* Critical

*Components/Labels:* focus-server, roi, safety, validation

*Requirements:* FOCUS-ROI-SAFETY

*Pre-Conditions:*

* PC-001: Safety rules: max_change_percent=50%

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 100}
Unsafe New ROI: {""min"": 200, ""max"": 300}
Change: 100% shift, 0% overlap{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define large jump|[0,100] → [200,300]|No overlap|
|2|Run safety validation|*|Fails|
|3|Check is_safe|*|False|
|4|Check warnings|*|""unsafe"" or ""large change""|

*Expected Result (overall):*

* Validation detects unsafe change
* Warning issued
* Command not sent (or sent with warning logged)

*Post-Conditions:*

* Admin notified of unsafe change

*Assertions:*

{code:python}current_min = 0
current_max = 100
new_min = 200
new_max = 300

safety_result = validate_roi_change_safety(
    current_min=current_min,
    current_max=current_max,
    new_min=new_min,
    new_max=new_max,
    max_change_percent=50.0
)

assert safety_result[""is_safe""] == False
assert safety_result[""overlap_percent""] == 0  # No overlap
assert len(safety_result[""warnings""]) > 0

logger.info(""✅ Unsafe ROI change detected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_unsafe_roi_change}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@e3257cb,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tof:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:40 PM
Integration - ROI Starting at Zero,PZ-13796,43332,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:39 PM,20/Oct/25 8:39 PM,,,,0,"*Summary:* Dynamic ROI – ROI Starting at Sensor Index 0

*Objective:* Verify ROI can start at sensor 0 (boundary).

*Priority:* Medium

*Components/Labels:* focus-server, roi, boundary

*Requirements:* FOCUS-ROI-BOUNDARY

*Pre-Conditions:*

* PC-001: RabbitMQ accessible

*Test Data:*

{code:json}ROI at Lower Boundary:
{
  ""start"": 0,
  ""end"": 50
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define ROI at 0|[0, 50]|Valid|
|2|Send command|*|Accepted|
|3|Verify waterfall|*|Sensors start at 0|

*Expected Result (overall):*

* Sensor 0 included correctly
* No index errors

*Post-Conditions:*

* System monitoring from sensor 0

*Assertions:*

{code:python}start = 0
end = 50

mq_client.send_roi_change(start=start, end=end)
time.sleep(5)

waterfall = focus_api.get_waterfall(task_id)
assert waterfall.sensor_range[""min""] == 0
logger.info(""✅ ROI at sensor 0 works""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_with_zero_start}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@718e08a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02to7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:39 PM
Integration - ROI with Large Range (Edge Case),PZ-13795,43331,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:37 PM,20/Oct/25 8:38 PM,,,,0,"*Summary:* Dynamic ROI – Very Large ROI Range (e.g., all sensors)

*Objective:* Test edge case where ROI covers maximum sensor range.

*Priority:* Medium

*Components/Labels:* focus-server, roi, edge-case, performance

*Requirements:* FOCUS-ROI-EDGE

*Pre-Conditions:*

* PC-001: System max sensors known (e.g., 512 or 1024)

*Test Data:*

{code:json}Large Valid ROI:
{
  ""start"": 0,
  ""end"": 512
}

Size: 512 sensors (maximum){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define large ROI|[0, 512]|All sensors|
|2|Run validation|*|Passes|
|3|Send command|*|Accepted|
|4|Monitor resources|CPU/Memory|No exhaustion|

*Expected Result (overall):*

* Maximum ROI accepted
* System handles high data volume
* No performance degradation

*Post-Conditions:*

* System monitoring all sensors

*Assertions:*

{code:python}start = 0
end = 512  # Or max_sensors from config
size = end - start

assert size == 512

safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

assert safety_result[""is_safe""] == True
mq_client.send_roi_change(start=start, end=end)
logger.info(""✅ Large ROI handled correctly""){code}

*Environment:* Staging, Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_with_large_range}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@78315354,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tnz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:37 PM
Integration - ROI with Small Range (Edge Case),PZ-13794,43330,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:36 PM,20/Oct/25 8:37 PM,,,,0,"*Summary:* Dynamic ROI – Very Small ROI Range (e.g., 2 sensors)

*Objective:* Test edge case where ROI is very small but valid.

*Priority:* Medium

*Components/Labels:* focus-server, roi, edge-case

*Requirements:* FOCUS-ROI-EDGE

*Pre-Conditions:*

* PC-001: RabbitMQ accessible

*Test Data:*

{code:json}Small Valid ROI:
{
  ""start"": 50,
  ""end"": 52
}

Size: 2 sensors{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define small ROI|[50, 52]|2 sensors|
|2|Run validation|validate_roi_change_safety()|Passes (warning maybe)|
|3|Send command|*|Accepted|
|4|Verify system handles|*|No crash or error|

*Expected Result (overall):*

* Small ROI accepted (possibly with warning)
* System handles edge case gracefully
* Baby Analyzer reinitializes with 2 sensors

*Post-Conditions:*

* System monitoring 2 sensors only

*Assertions:*

{code:python}start = 50
end = 52
size = end - start

assert size == 2  # Very small

# May pass with warning
safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

# Should work even if small
mq_client.send_roi_change(start=start, end=end)
logger.info(""✅ Small ROI handled correctly""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_with_small_range}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@37ac0997,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tnr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:36 PM
Integration - Dynamic ROI – Reject ROI with Negative End Value,PZ-13793,43329,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:35 PM,20/Oct/25 8:36 PM,,,,0,"*Summary:* Dynamic ROI – Reject ROI with Negative End Value

*Objective:* Verify system rejects ROI with negative end sensor index.

*Priority:* High

*Components/Labels:* focus-server, roi, validation, negative-test

*Requirements:* FOCUS-ROI-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enabled

*Test Data:*

{code:json}Invalid ROI (Negative End):
{
  ""start"": 10,
  ""end"": -50
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define negative end|start=10, end=-50|ROI defined|
|2|Check end value|end < 0|True (invalid)|
|3|Run validation|validate_roi_change_safety()|Fails|

*Expected Result (overall):*

* Negative end value rejected
* Clear error message

*Post-Conditions:*

* No state change

*Assertions:*

{code:python}start = 10
end = -50

assert end < 0  # Negative end

safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

assert safety_result[""is_safe""] == False
logger.info(""✅ Negative end ROI rejected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_with_negative_end}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6b25c1f5,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tnj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:35 PM
Integration - ROI with Negative Start,PZ-13792,43328,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:33 PM,20/Oct/25 8:34 PM,,,,0,"*Summary:* Dynamic ROI – Reject ROI with Negative Start Value

*Objective:* Verify system rejects ROI with negative start sensor index.

*Priority:* High

*Components/Labels:* focus-server, roi, validation, negative-test

*Requirements:* FOCUS-ROI-VALIDATION

*Pre-Conditions:*

* PC-001: Validation enabled
* PC-002: Sensor indices must be >= 0

*Test Data:*

{code:json}Invalid ROI (Negative Start):
{
  ""start"": -10,
  ""end"": 50
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define negative start|start=-10, end=50|ROI defined|
|2|Check start value|start < 0|True (invalid)|
|3|Run validation|validate_roi_change_safety()|Fails|
|4|Check warnings|*|""negative"" or ""invalid index""|

*Expected Result (overall):*

* Validation rejects negative sensor index
* Warning message clear
* No command sent

*Post-Conditions:*

* System state unchanged

*Assertions:*

{code:python}start = -10
end = 50

assert start < 0  # Negative start

safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

assert safety_result[""is_safe""] == False
assert any(""negative"" in w.lower() or ""invalid"" in w.lower() 
           for w in safety_result[""warnings""])

logger.info(""✅ Negative start ROI rejected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_with_negative_start}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7b5659e2,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tnb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:33 PM
Integration - ROI with Reversed Range (Start > End),PZ-13791,43327,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:32 PM,20/Oct/25 8:33 PM,,,,0,"*Summary:* Dynamic ROI – Reject Reversed ROI Range

*Objective:* Ensure system rejects ROI where start > end (reversed range).

*Priority:* Critical

*Components/Labels:* focus-server, roi, validation, negative-test

*Requirements:* FOCUS-ROI-VALIDATION

*Pre-Conditions:*

* PC-001: Safety validation enabled

*Test Data:*

{code:json}Invalid ROI (Reversed):
{
  ""start"": 150,
  ""end"": 50
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define reversed ROI|start=150, end=50|ROI defined|
|2|Calculate size|end - start|size = -100 (negative!)|
|3|Run validation|validate_roi_change_safety()|Fails|
|4|Check is_safe|result[""is_safe""]|False|
|5|Check warnings|*|""reversed"" or ""invalid range""|

*Expected Result (overall):*

* Validation fails due to reversed range
* Clear warning message
* No command sent

*Post-Conditions:*

* Original ROI preserved

*Assertions:*

{code:python}start = 150
end = 50

safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

assert safety_result[""is_safe""] == False
assert end - start < 0  # Negative size
assert len(safety_result[""warnings""]) > 0

logger.info(""✅ Reversed ROI correctly rejected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_invalid_roi_reversed}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1103b40a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tn3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:32 PM
Integration - ROI with Equal Start and End (Zero Size),PZ-13790,43326,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:31 PM,20/Oct/25 8:32 PM,,,,0,"*Summary:* Dynamic ROI – Reject ROI with Start Equal to End

*Objective:* Verify that the system rejects an ROI change where start equals end (zero-size ROI) as this is invalid.

*Priority:* High

*Components/Labels:* focus-server, roi, validation, negative-test

*Requirements:* FOCUS-ROI-VALIDATION

*Pre-Conditions:*

* PC-001: RabbitMQ accessible
* PC-010: Safety validation enabled

*Test Data:*

{code:json}Invalid ROI (Zero Size):
{
  ""start"": 50,
  ""end"": 50
}

Expected Validation Result:
{
  ""is_safe"": false,
  ""warnings"": [""ROI size is zero""]
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define zero-size ROI|start=50, end=50|ROI defined|
|2|Calculate ROI size|end - start|size = 0|
|3|Run safety validation|validate_roi_change_safety()|Validation fails|
|4|Check is_safe flag|result[""is_safe""]|False|
|5|Check warnings|result[""warnings""]|Contains ""zero"" or ""invalid size""|
|6|Attempt to send command|*|Command should not be sent|

*Expected Result (overall):*

* Safety validation fails (is_safe=False)
* Warning about zero-size ROI generated
* Command not sent to RabbitMQ
* System protects against invalid ROI

*Post-Conditions:*

* Original ROI unchanged
* No command published

*Assertions:*

{code:python}# Zero-size ROI
start = 50
end = 50

# Validate safety
safety_result = validate_roi_change_safety(
    current_min=0,
    current_max=100,
    new_min=start,
    new_max=end
)

# Should fail validation
assert safety_result[""is_safe""] == False
assert len(safety_result[""warnings""]) > 0
assert any(""size"" in w.lower() or ""zero"" in w.lower() 
           for w in safety_result[""warnings""])

logger.info(""✅ Zero-size ROI correctly rejected""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_equal_start_end}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@31a179a4,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tmv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:31 PM
Integration - ROI Shift (Move Range),PZ-13789,43325,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:29 PM,20/Oct/25 8:30 PM,,,,0,"*Summary:* Dynamic ROI – Shift ROI Range Without Changing Size

*Objective:* Verify that shifting ROI (moving the window while keeping size constant) works correctly.

*Priority:* Medium

*Components/Labels:* focus-server, roi, shift, pan

*Requirements:* FOCUS-ROI-SHIFT

*Pre-Conditions:*

* PC-001: Task configured with ROI [0, 50]

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 50}
New ROI (Shifted): {""min"": 100, ""max"": 150}
Change: Moved +100 sensors right, size unchanged{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure initial ROI|[0, 50]|50 sensors (0-49)|
|2|Send shift command|[100, 150]|Command accepted|
|3|Wait|5 sec|Reinitialization|
|4|Verify new range|GET /waterfall|Sensors [100, 150]|
|5|Check size unchanged|*|Still 50 sensors|

*Expected Result (overall):*

* ROI shifted right by 100 sensors
* Size remains 50 sensors
* No overlap with previous range

*Post-Conditions:*

* System monitoring new shifted range

*Assertions:*

{code:python}initial_roi = {""min"": 0, ""max"": 50}
initial_size = initial_roi[""max""] - initial_roi[""min""]

new_roi = {""min"": 100, ""max"": 150}
mq_client.send_roi_change(start=100, end=150)
time.sleep(5)

waterfall = focus_api.get_waterfall(task_id)
new_size = waterfall.sensor_range[""max""] - waterfall.sensor_range[""min""]

assert waterfall.sensor_range[""min""] == 100
assert waterfall.sensor_range[""max""] == 150
assert new_size == initial_size  # Size unchanged

logger.info(""✅ ROI shifted successfully""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_shift}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@42643fb6,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tmn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:29 PM
Integration - ROI Shrinking (Decrease Range),PZ-13788,43324,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:28 PM,20/Oct/25 8:29 PM,,,,0,"*Summary:* Dynamic ROI – Shrink ROI Range (Reduce Sensor Coverage)

*Objective:* Validate that shrinking the ROI range (making it smaller) works correctly for focusing on specific area.

*Priority:* Medium

*Components/Labels:* focus-server, roi, shrink, focus

*Requirements:* FOCUS-ROI-SHRINK

*Pre-Conditions:*

* PC-001: Task configured with ROI [0, 200]
* PC-002: System monitoring 200 sensors

*Test Data:*

{code:json}Current ROI: {""min"": 0, ""max"": 200}
New ROI (Shrunk): {""min"": 75, ""max"": 125}
Change: -150 sensors (focus on middle 50){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure initial ROI|[0, 200]|200 sensors monitored|
|2|Send shrink command|[75, 125]|Command accepted|
|3|Wait for reinitialization|5 sec|Baby Analyzer restarts|
|4|Verify new range|GET /waterfall|Sensors [75, 125]|
|5|Check sensor count|*|50 sensors monitored|

*Expected Result (overall):*

* ROI successfully shrunk from 200 to 50 sensors
* System now monitors only middle range
* Performance improved due to fewer sensors

*Post-Conditions:*

* System continues with shrunk ROI
* Resources freed up

*Assertions:*

{code:python}# Initial ROI
initial_roi = {""min"": 0, ""max"": 200}
assert (initial_roi[""max""] - initial_roi[""min""]) == 200

# Shrink ROI
new_roi = {""min"": 75, ""max"": 125}
mq_client.send_roi_change(start=75, end=125)
time.sleep(5)

# Verify shrink
waterfall = focus_api.get_waterfall(task_id)
assert waterfall.sensor_range[""min""] == 75
assert waterfall.sensor_range[""max""] == 125
assert (waterfall.sensor_range[""max""] - waterfall.sensor_range[""min""]) == 50

logger.info(""✅ ROI shrunk successfully""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_shrinking}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@30a5c47d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tmf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:28 PM
Integration -  ROI Expansion (Increase Range),PZ-13787,43323,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:21 PM,20/Oct/25 8:21 PM,,,,0,"*Summary:* Dynamic ROI – Expand ROI Range (Increase Sensor Coverage)

*Objective:* Validate that expanding the ROI range (making it larger) works correctly and system adapts to monitor more sensors.

*Priority:* Medium

*Components/Labels:* focus-server, roi, expansion

*Requirements:* FOCUS-ROI-EXPANSION

*Pre-Conditions:*

* PC-001: Task configured with ROI [50, 100]
* PC-002: System monitoring 50 sensors

*Test Data:*

{code:json}Current ROI: {""min"": 50, ""max"": 100}
New ROI (Expanded): {""min"": 25, ""max"": 125}
Change: +50 sensors (25 below, 25 above){code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Configure initial ROI|[50, 100]|50 sensors monitored|
|2|Send expansion command|[25, 125]|Command accepted|
|3|Wait for reinitialization|5 sec|Baby Analyzer restarts|
|4|Verify new range|GET /waterfall|Sensors [25, 125]|
|5|Check sensor count|*|100 sensors monitored|

*Expected Result (overall):*

* ROI successfully expanded from 50 to 100 sensors
* System now monitors additional 50 sensors
* No data loss or corruption

*Post-Conditions:*

* System continues with expanded ROI

*Assertions:*

{code:python}# Initial ROI
initial_roi = {""min"": 50, ""max"": 100}
assert (initial_roi[""max""] - initial_roi[""min""]) == 50

# Expand ROI
new_roi = {""min"": 25, ""max"": 125}
mq_client.send_roi_change(start=25, end=125)
time.sleep(5)

# Verify expansion
waterfall = focus_api.get_waterfall(task_id)
assert waterfall.sensor_range[""min""] == 25
assert waterfall.sensor_range[""max""] == 125
assert (waterfall.sensor_range[""max""] - waterfall.sensor_range[""min""]) == 100

logger.info(""✅ ROI expanded successfully""){code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_expansion}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@237c430,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tm7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:21 PM
Integration - Multiple ROI Changes in Sequence,PZ-13786,43322,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:19 PM,20/Oct/25 8:20 PM,,,,0,"*Summary:* Dynamic ROI – Sequential ROI Changes Without System Restart

*Objective:* Verify that multiple ROI change commands can be sent in sequence without requiring system restart or causing state corruption.

*Priority:* High

*Components/Labels:* focus-server, roi, sequence, stability

*Requirements:* FOCUS-ROI-MULTIPLE, FOCUS-STABILITY

*Pre-Conditions:*

* PC-001: RabbitMQ accessible
* PC-010: Live task configured and running
* PC-011: Baby Analyzer responsive to commands

*Test Data:*

{code:json}ROI Sequence:
[
  {""start"": 0, ""end"": 100},
  {""start"": 50, ""end"": 150},
  {""start"": 100, ""end"": 200},
  {""start"": 75, ""end"": 175},
  {""start"": 25, ""end"": 125}
]

Wait Time Between Commands: 2 seconds{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Send ROI command 1|[0, 100]|Command sent|
|2|Wait|2 seconds|System stabilizes|
|3|Send ROI command 2|[50, 150]|Command sent|
|4|Wait|2 seconds|System stabilizes|
|5|Send ROI command 3|[100, 200]|Command sent|
|6|Wait|2 seconds|System stabilizes|
|7|Send ROI command 4|[75, 175]|Command sent|
|8|Wait|2 seconds|System stabilizes|
|9|Send ROI command 5|[25, 125]|Command sent|
|10|Verify all successful|*|No errors in sequence|

*Expected Result (overall):*

* All 5 ROI commands sent successfully
* No connection drops or timeouts
* System remains responsive throughout
* No memory leaks or resource exhaustion

*Post-Conditions:*

* Final ROI applied correctly ([25, 125])
* System stable and operational

*Assertions:*

{code:python}roi_sequence = [
    (0, 100),
    (50, 150),
    (100, 200),
    (75, 175),
    (25, 125)
]

for i, (start, end) in enumerate(roi_sequence, 1):
    logger.info(f""Sending ROI change {i}/5: [{start}, {end}]"")
    client.send_roi_change(start=start, end=end)
    assert True  # No exception
    time.sleep(2)  # Wait for processing

logger.info(""✅ All 5 ROI commands sent successfully"")
assert len(roi_sequence) == 5{code}

*Environment:* Staging, Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_multiple_roi_changes_sequence}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@317f44b6,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tlz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:19 PM
Integration - ROI Change with Safety Validation,PZ-13785,43321,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:17 PM,20/Oct/25 8:18 PM,,,,0,"*Summary:* Dynamic ROI Change – Validate Safety Before Sending Command

*Objective:* Ensure ROI change requests undergo safety validation to prevent unsafe or destructive changes that could impact system stability.

*Priority:* Critical

*Components/Labels:* focus-server, roi, safety-validation, rabbitmq

*Requirements:* FOCUS-ROI-SAFETY, FOCUS-VALIDATION

*Pre-Conditions:*

* PC-001: RabbitMQ accessible
* PC-010: Current ROI known (min=0, max=100)
* PC-011: Safety validation rules configured (max_change_percent=50%)

*Test Data:*

{code:json}Current ROI:
{
  ""current_min"": 0,
  ""current_max"": 100
}

New ROI (Safe):
{
  ""new_min"": 20,
  ""new_max"": 120
}

Safety Rules:
{
  ""max_change_percent"": 50.0,
  ""min_overlap_percent"": 30.0
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Define current ROI|min=0, max=100|Current state captured|
|2|Define new ROI|min=20, max=120|New state defined|
|3|Calculate change percentage|(120-20) vs (100-0)|0% change (same size)|
|4|Calculate overlap|[20,100] overlap|80% overlap|
|5|Run safety validation|validate_roi_change_safety()|Safety check passes|
|6|Check is_safe flag|result[""is_safe""]|True|
|7|Send ROI command|start=20, end=120|Command sent successfully|

*Expected Result (overall):*

* Safety validation passes (is_safe=True)
* No warnings generated
* ROI command sent to RabbitMQ
* System accepts new ROI range

*Post-Conditions:*

* Safety validation logs recorded
* Command successfully delivered

*Assertions:*

{code:python}current_min = 0
current_max = 100
new_min = 20
new_max = 120

# Validate safety
safety_result = validate_roi_change_safety(
    current_min=current_min,
    current_max=current_max,
    new_min=new_min,
    new_max=new_max,
    max_change_percent=50.0
)

assert safety_result[""is_safe""] == True
assert len(safety_result[""warnings""]) == 0
assert safety_result[""overlap_percent""] >= 30.0

# Send command
client.send_roi_change(start=new_min, end=new_max)
assert True  # No exception means success{code}

*Environment:* All

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_roi_change_with_validation}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@24c97471,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tlr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:17 PM
Integration - Send ROI Change Command via RabbitMQ,PZ-13784,43320,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 8:16 PM,20/Oct/25 8:17 PM,,,,0,"*Summary:* Dynamic ROI Change – Send RegionOfInterestCommand via RabbitMQ

*Objective:* Verify that a RegionOfInterestCommand can be successfully published to RabbitMQ Baby Analyzer exchange and accepted by the system without errors.

*Priority:* High

*Components/Labels:* focus-server, rabbitmq, roi, baby-analyzer, mq-commands

*Requirements:* FOCUS-ROI-COMMAND, FOCUS-RABBITMQ-INTEGRATION

*Pre-Conditions:*

* PC-001: RabbitMQ server accessible at configured host/port
* PC-002: RabbitMQ credentials valid (username/password)
* PC-003: Baby Analyzer exchange exists
* PC-004: ROI routing key configured

*Test Data:*

{code:json}RabbitMQ Connection:
{
  ""host"": ""10.10.100.107"",
  ""port"": 5672,
  ""username"": ""prisma"",
  ""password"": ""prisma"",
  ""vhost"": ""/""
}

ROI Command Payload:
{
  ""command_type"": ""RegionOfInterestCommand"",
  ""start"": 50,
  ""end"": 150,
  ""routing_key"": ""roi""
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Connect to RabbitMQ|host=10.10.100.107, port=5672|Connection established|
|2|Verify exchange exists|exchange=""baby_analyzer""|Exchange available|
|3|Create ROI command|start=50, end=150|Command object created|
|4|Publish command|routing_key=""roi""|Message published successfully|
|5|Verify no exceptions|*|No errors raised|
|6|Check MQ acknowledgment|*|ACK received|

*Expected Result (overall):*

* ROI change command published to RabbitMQ successfully
* No exceptions or connection errors
* Command delivered to Baby Analyzer queue
* MQ acknowledgment received

*Post-Conditions:*

* RabbitMQ connection closed cleanly
* No orphaned messages in queues

*Assertions:*

{code:python}# Connect to RabbitMQ
client = BabyAnalyzerMQClient(host=""10.10.100.107"", port=5672, 
                               username=""prisma"", password=""prisma"")
client.connect()
assert client.is_connected() == True

# Send ROI command
new_start = 50
new_end = 150
client.send_roi_change(start=new_start, end=new_end, routing_key=""roi"")

# Verify no exceptions
assert True  # Test passes if no exception raised

# Cleanup
client.disconnect(){code}

*Environment:* Staging, Production

*Automation Status:* IMPLEMENTED  
*Test Function:* {{test_send_roi_change_command}}  
*Test File:* {{tests/integration/api/test_dynamic_roi_adjustment.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3ca1a3a7,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tlj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 8:16 PM
Performance – /config/{task_id} Latency P95,PZ-13770,43299,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:58 AM,20/Oct/25 11:58 AM,,,,0,"*Summary:* Performance – /config/{task_id} Latency P95

*Objective:* Ensure configuration endpoint meets SLA (p95 < 2 seconds).

*Priority:* Medium

*Components/Labels:* focus-server, performance, latency

*Requirements:* FOCUS-PERF-CONFIG

*Pre-Conditions:*

* PC-001: Minimal load environment

*Test Data:*

* Standard live payload
* 100 sequential requests

*Steps:*

||#||Action||Data||Expected Result||
|1|Warm up|5 requests|System ready|
|2|Send 100 requests|Sequential|All succeed|
|3|Record times|Each request|Times logged|
|4|Calculate p95|All times|Compute percentile|
|5|Validate SLA|p95 value|< 2.0 seconds|
|6|Check memory|During test|No leaks|
|7|Check CPU|Average|< 80%|

*Expected Result (overall):*

* p95 < 2.0 seconds
* p99 < 5.0 seconds
* No memory leaks
* Stable performance

*Post-Conditions:*

* Cleanup test tasks

*Assertions:*

{code:python}assert percentile(times, 95) < 2.0
assert percentile(times, 99) < 5.0
assert max(times) < 10.0
assert memory_leak < 100_000_000  # 100MB{code}

*Environment:* Staging (non-loaded)

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_config_latency_p95}}
*Test File:* {{tests/integration/performance/test_latency.py}} ",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@41f5fec7,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02thj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:58 AM
Security – Malformed Input Handling,PZ-13769,43298,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:56 AM,20/Oct/25 11:57 AM,,,,0,"*Summary:* Security – Malformed Input Handling

*Objective:* Validate input hardening against malformed/injection attempts.

*Priority:* Critical

*Components/Labels:* focus-server, security, input-validation

*Requirements:* FOCUS-SEC-ROBUSTNESS

*Pre-Conditions:*

* PC-001: Security logging enabled

*Test Data:*

{noformat}1. Malformed JSON: {""invalid"": unclosed
2. SQL Injection: ?id='1' OR '1'='1
3. XSS: <script>alert('xss')</script>
4. XXE: <!DOCTYPE foo [<!ENTITY xxe SYSTEM ""file:///etc/passwd"">]>
5. Oversized: 10MB JSON
6. Null bytes: \u0000
7. Path traversal: ../../etc/passwd{noformat}

*Steps:*

||#||Action||Data||Expected Result||
|1|POST malformed JSON|Broken JSON|HTTP 400|
|2|GET with SQLi|Injection param|Sanitized|
|3|POST XSS|Script tags|Rejected|
|4|POST XXE|Entity payload|Rejected|
|5|POST oversized|10MB|HTTP 413 or handled|
|6|POST null bytes|\u0000|Sanitized|
|7|Path traversal|../..|Blocked|
|8|Check logs|Security log|All logged|
|9|Verify server|Status|Running|

*Expected Result (overall):*

* All attacks blocked
* No 500 errors
* Security logged
* Server stable

*Post-Conditions:*
None

*Assertions:*

{code:python}assert all(r.status_code != 500 for r in attacks)
assert malformed.status_code == 400
assert security_log.contains_all_attempts()
assert server.is_running(){code}

*Environment:* Staging

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_security_malformed_inputs}}
*Test File:* {{tests/integration/security/test_input_security.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@63f703db,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02thb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:56 AM
Integration – RabbitMQ Outage Handling,PZ-13768,43297,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:55 AM,20/Oct/25 11:55 AM,,,,0,"*Summary:* Integration – RabbitMQ Outage Handling

*Objective:* Validate graceful failure when RabbitMQ unavailable.

*Priority:* Critical

*Components/Labels:* focus-server, resilience, rabbitmq, outage

*Requirements:* FOCUS-RESILIENCE-RABBIT

*Pre-Conditions:*

* PC-020: RabbitMQ can be blocked
* PC-021: Live payload ready

*Test Data:*

{code:json}{
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 0,
  ...
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Verify RabbitMQ healthy|Connection test|Connected|
|2|Block RabbitMQ|Network/credentials|Blocked|
|3|POST /config/{task_id}|Live|HTTP 503 or 502|
|4|Check error|Response|""RabbitMQ unavailable""|
|5|Verify no resources|kubectl|No Job/Service|
|6|Check logs|Server|Error logged|
|7|Restore RabbitMQ|Unblock|Restored|
|8|POST /config again|Same|HTTP 200|

*Expected Result (overall):*

* Proper 5xx during outage
* No resource allocation
* Quick recovery

*Post-Conditions:*

* RabbitMQ restored

*Assertions:*

{code:python}assert outage_response.status_code in [502, 503]
assert ""rabbit"" in error.lower()
assert not kubernetes.job_created()
assert recovery_response.status_code == 200{code}

*Environment:* Staging

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_rabbitmq_outage_resilience}}
*Test File:* {{tests/integration/infrastructure/test_resilience.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@c4d15c0,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02th3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:55 AM
Integration – MongoDB Outage Handling,PZ-13767,43296,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:34 AM,20/Oct/25 11:35 AM,,,,0,"*Summary:* Integration – MongoDB Outage Handling

*Objective:* Ensure MongoDB outage fails gracefully without launching jobs.

*Priority:* Critical

*Components/Labels:* focus-server, resilience, mongodb, outage

*Requirements:* FOCUS-RESILIENCE-MONGO

*Pre-Conditions:*

* PC-010: MongoDB can be disabled (K8s access)
* PC-011: Historical payload ready

*Test Data:*

{code:json}{
  ""start_time"": 1700000000,
  ""end_time"": 1700000600,
  ...
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Verify MongoDB healthy|kubectl|Running|
|2|Scale MongoDB to 0|kubectl scale|Stopped|
|3|Verify unreachable|Connection test|Refused|
|4|POST /config/{task_id}|Historical|HTTP 503|
|5|Check error|Response|""MongoDB unavailable""|
|6|Verify no K8s job|kubectl|No new jobs|
|7|Check server|Logs|Still running|
|8|Restore MongoDB|kubectl scale|Restored|
|9|POST /config again|Same|HTTP 200|

*Expected Result (overall):*

* Clean 503 during outage
* No resource leaks
* Automatic recovery

*Post-Conditions:*

* MongoDB restored

*Assertions:*

{code:python}assert outage_response.status_code == 503
assert ""mongo"" in error.lower()
assert not kubernetes.new_jobs_created()
assert recovery_response.status_code == 200{code}

*Environment:* Staging (K8s required)

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_mongodb_outage_resilience}}
*Test File:* {{tests/integration/infrastructure/test_resilience.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6348439a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tgv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:34 AM
 API – POST /recordings_in_time_range – Returns Recording Windows,PZ-13766,43295,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:31 AM,20/Oct/25 11:32 AM,,,,0,"*Summary:* API – POST /recordings_in_time_range – Returns Recording Windows

*Objective:* Validate endpoint returns recordings that intersect time range.

*Priority:* High

*Components/Labels:* focus-server, api, history, recordings

*Requirements:* FOCUS-API-RECORDINGS

*Pre-Conditions:*

* PC-010: MongoDB accessible
* PC-011: Recordings exist

*Test Data:*

{code:json}{
  ""start_time"": 1700000000,
  ""end_time"": 1700000600
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|POST /recordings_in_time_range|Time window|HTTP 200|
|2|Validate format|Response|List of [start, end]|
|3|Check overlap|Each recording|Overlaps request|
|4|Validate timestamps|Values|Valid epochs|
|5|Check ordering|List|Sorted by start|
|6|Test empty range|Future time|Empty list|

*Expected Result (overall):*

* Correct recordings returned
* Proper overlap
* Sorted results

*Post-Conditions:*
None

*Assertions:*

{code:python}assert response.status_code == 200
assert isinstance(response.json(), list)
assert all(r[0] < req_end and r[1] > req_start for r in recordings){code}

*Environment:* Staging/Production

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_post_recordings_in_time_range}}
*Test File:* {{tests/integration/api/test_history_endpoints.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1273d26a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tgn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:31 AM
API – GET /live_metadata – Returns 404 When Unavailable,PZ-13765,43294,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:27 AM,20/Oct/25 11:28 AM,,,,0,"*Summary:* API – GET /live_metadata – Returns 404 When Unavailable

*Objective:* Validate proper 404 when metadata not available.

*Priority:* Medium

*Components/Labels:* focus-server, api, metadata, negative-test

*Requirements:* FOCUS-API-METADATA

*Pre-Conditions:*

* PC-001: fiber_metadata NOT configured

*Test Data:*
N/A (GET request)

*Steps:*

||#||Action||Data||Expected Result||
|1|Ensure metadata=None|Server config|Not available|
|2|GET /live_metadata|None|HTTP 404|
|3|Validate error|Response|""Not available"" message|
|4|Check no crash|Logs|No exceptions|

*Expected Result (overall):*

* Clean 404
* Clear message

*Post-Conditions:*
None

*Assertions:*

{code:python}assert response.status_code == 404
assert ""not available"" in response.json()['error'].lower(){code}

*Environment:* Dev without metadata

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_get_live_metadata_missing}}
*Test File:* {{tests/integration/api/test_info_endpoints.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@ffcdc48,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tgf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:27 AM
 API – GET /live_metadata – Returns Metadata When Available,PZ-13764,43293,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:24 AM,20/Oct/25 11:25 AM,,,,0,"*Summary:* API – GET /live_metadata – Returns Metadata When Available

*Objective:* Validate live metadata endpoint returns complete fiber metadata.

*Priority:* Medium

*Components/Labels:* focus-server, api, metadata, live

*Requirements:* FOCUS-API-METADATA

*Pre-Conditions:*

* PC-001: fiber_metadata configured

*Test Data:*
N/A (GET request)

*Steps:*

||#||Action||Data||Expected Result||
|1|GET /live_metadata|None|HTTP 200|
|2|Validate schema|Response|Contains dx, prr, fiber_*|
|3|Check dx|Value|{quote}0 {quote}|
|4|Check prr|Value|{quote}0 {quote}|
|5|Validate all fields|Response|No nulls|

*Expected Result (overall):*

* Complete metadata returned
* All fields populated

*Post-Conditions:*
None

*Assertions:*

{code:python}assert response.status_code == 200
assert response.json()['dx'] > 0
assert response.json()['prr'] > 0{code}

*Environment:* Dev/Staging with metadata

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_get_live_metadata_present}}
*Test File:* {{tests/integration/api/test_info_endpoints.py}} ",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@f84f621,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tg7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:24 AM
API – GET /channels – Returns System Channel Bounds,PZ-13762,43291,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:21 AM,20/Oct/25 11:21 AM,,,,0,"*Summary:* API – GET /channels – Returns System Channel Bounds

*Objective:* Validate channel range endpoint returns authoritative bounds.

*Priority:* High

*Components/Labels:* focus-server, api, channels, info

*Requirements:* FOCUS-API-CHANNELS

*Pre-Conditions:*

* PC-001: API reachable
* PC-002: FocusManager.channels configured

*Test Data:*
N/A (GET request)

*Steps:*

||#||Action||Data||Expected Result||
|1|GET /channels|None|HTTP 200|
|2|Validate schema|Response|Contains lowest_channel, highest_channel|
|3|Check lowest_channel|Value|== 1|
|4|Check highest_channel|Value|{quote}0, matches config {quote}|
|5|Call 5 times|None|Consistent results|
|6|Check response time|Latency|< 100ms|

*Expected Result (overall):*

* Returns configured bounds
* Consistent across calls
* Fast response

*Post-Conditions:*
None

*Assertions:*

{code:python}assert response.status_code == 200
assert response.json()['lowest_channel'] == 1
assert response.json()['highest_channel'] > 0{code}

*Environment:* Any

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_get_channels}}
*Test File:* {{tests/integration/api/test_info_endpoints.py}} ",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@e7bd8ab,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tfr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:21 AM
API – POST /config/{task_id} – Invalid Frequency Range Rejection,PZ-13761,43289,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:17 AM,20/Oct/25 11:18 AM,,,,0,"*Summary:* API – POST /config/{task_id} – Invalid Frequency Range Rejection

*Objective:* Validate rejection of malformed frequency ranges.

*Priority:* High

*Components/Labels:* focus-server, api, validation, negative-test

*Requirements:* FOCUS-API-VALIDATION

*Pre-Conditions:*

* PC-001: API reachable

*Test Data:*

{code:json}Test A - min > max:
{
  ""frequencyRange"": { ""min"": 500, ""max"": 0 },
  ...
}

Test B - negative:
{
  ""frequencyRange"": { ""min"": -100, ""max"": 500 },
  ...
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|POST /config/{task_id}|min > max|HTTP 400|
|2|Validate error|Response|""Invalid frequency range""|
|3|POST /config/{task_id}|Negative|HTTP 400|
|4|Test valid range|min=0, max=1000|HTTP 200|

*Expected Result (overall):*

* Invalid rejected
* Valid accepted

*Post-Conditions:*
None

*Assertions:*

{code:python}assert invalid_response.status_code == 400
assert ""frequency"" in error_message.lower(){code}

*Environment:* Any

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_config_invalid_frequency}}
*Test File:* {{tests/integration/api/test_config_validation.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@50f427ce,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tfj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:17 AM
API – POST /config/{task_id} – Invalid Channel Range Rejection,PZ-13760,43288,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:15 AM,20/Oct/25 11:17 AM,,,,0,"*Summary:* API – POST /config/{task_id} – Invalid Channel Range Rejection

*Objective:* Confirm server blocks illegal channel windows (min > max, negative, out of bounds).

*Priority:* High

*Components/Labels:* focus-server, api, validation, negative-test

*Requirements:* FOCUS-API-VALIDATION

*Pre-Conditions:*

* PC-001: API reachable
* PC-002: System max channels known

*Test Data:*

{code:json}Test A - min > max:
{
  ""channels"": { ""min"": 300, ""max"": 100 },
  ...
}

Test B - negative:
{
  ""channels"": { ""min"": -5, ""max"": 10 },
  ...
}

Test C - out of bounds:
{
  ""channels"": { ""min"": 1, ""max"": 99999 },
  ...
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|POST /config/{task_id}|min > max|HTTP 400|
|2|Check error|Response|""Invalid channel range""|
|3|POST /config/{task_id}|Negative|HTTP 400|
|4|POST /config/{task_id}|Out of bounds|HTTP 400|
|5|Test boundary|min=1, max=max_channels|HTTP 200|

*Expected Result (overall):*

* Invalid ranges rejected
* Boundary values work
* Clear errors

*Post-Conditions:*
None

*Assertions:*

{code:python}assert invalid_response.status_code == 400
assert ""channel"" in error_message.lower()
assert boundary_response.status_code == 200{code}

*Environment:* Any

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_config_invalid_channels}}
*Test File:* {{tests/integration/api/test_config_validation.py}}

----",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@15e11e62,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tfb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:15 AM
API – POST /config/{task_id} – Invalid Time Range Rejection,PZ-13759,43287,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,20/Oct/25 11:13 AM,20/Oct/25 11:14 AM,,,,0,"*Summary:* API – POST /config/{task_id} – Invalid Time Range Rejection

*Objective:* Ensure invalid time ranges (start >= end) are rejected with proper 400 error.

*Priority:* High

*Components/Labels:* focus-server, api, validation, negative-test

*Requirements:* FOCUS-API-VALIDATION

*Pre-Conditions:*

* PC-001: API reachable

*Test Data:*

{code:json}Test Case A - start > end:
{
  ""start_time"": 1700000600,
  ""end_time"": 1700000000,
  ""view_type"": 0,
  ...
}

Test Case B - start == end:
{
  ""start_time"": 1700000000,
  ""end_time"": 1700000000,
  ""view_type"": 0,
  ...
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Generate task_id|Unique ID|Valid task_id|
|2|POST /config/{task_id}|start > end|HTTP 400|
|3|Validate error message|Response|Contains ""Invalid time range""|
|4|POST /config/{task_id}|start == end|HTTP 400|
|5|Check no task created|System|Task doesn't exist|
|6|Verify logs|Server logs|Validation error logged|

*Expected Result (overall):*

* Invalid time ranges rejected with 400
* Clear error messages
* No task creation

*Post-Conditions:*
None

*Assertions:*

{code:python}assert response.status_code == 400
assert ""time range"" in response.json()['error'].lower()
assert not task_exists(task_id){code}

*Environment:* Any

*Automation Status:* TO BE AUTOMATED
*Test Function:* {{test_config_invalid_time_range}}
*Test File:* {{tests/integration/api/test_config_validation.py}} (NEW FILE)",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6af2a9c1,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02tf3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,20/Oct/25 11:13 AM
Data Lifecycle – Historical vs Live Recordings Classification,PZ-13705,42987,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 8:34 PM,19/Oct/25 5:33 PM,,,,0,"Requirements

[PZ-13598] (Data Quality – Mongo collections and schema)

FOCUS-DATA-LIFECYCLE (Recording lifecycle management)

FOCUS-CLEANUP-SERVICE (Data cleanup and retention)

Objective

Validate that MongoDB correctly distinguishes between Historical (completed), Live (in-progress), and Deleted (cleanup) recordings. Verify that the recording lifecycle is properly managed and that cleanup services (Sweeper/Data Manager) are functioning correctly.

Business Impact:

Historical recordings must be properly indexed for history playback via POST /recordings_in_time_range

Live recordings must be distinguished from stale/crashed recordings

Deleted recordings must be properly marked for cleanup

Missing end_time on deleted recordings indicates improper cleanup logic

Pre-Conditions

PC-010: MongoDB is reachable and accessible

PC-013: Recording collection exists and contains data

PC-021: Recording collection is dynamically named (GUID-based)

PC-022: base_paths collection contains GUID for recording collection

PC-023: System has active or historical recordings

Architectural Context

MongoDB Role in Focus/Panda System

MongoDB serves as a metadata index for recordings:

Purpose: Store metadata for recordings (NOT raw data)

Used by: Focus Server & Baby Analyzer

Function: Find raw files (PRP2/SEGY) in S3/local storage for specific time ranges

API: POST /recordings_in_time_range requires start_time and end_time

Recording Types

Type

Description

Schema

Status

Historical 

Completed recordings 

Has start_time AND end_time, deleted=False 

Normal ✅ 

Live 

Currently recording 

Has start_time, NO end_time (yet), deleted=False 

Normal ✅ 

Deleted 

Soft-deleted recordings 

Has deleted=True 

Cleanup ⚠️ 

Stale 

Crashed/failed recordings 

Old (>24h) with NO end_time, deleted=False 

Bug ❌ 

Test Data

Database name: prisma (staging)

Collections:

base_paths - contains GUID for recording collection

{GUID} - dynamic recording collection (e.g., 77e49b5d-e06a-4aae-a33e-17117418151c)

{GUID}-unrecognized_recordings - failed recognition recordings

Expected Recording Fields:

uuid (string, unique) - Recording identifier

start_time (datetime) - Recording start timestamp

end_time (datetime or null) - Recording end timestamp

deleted (boolean) - Soft delete flag

Thresholds:

Stale threshold: 24 hours (recordings older than 24h without end_time)

Historical majority: >50% of recordings should be completed

Test Steps

#

Action

Data

Expected Result

SETUP 

1 

Connect to MongoDB 

Database name from config 

Connection successful 

2 

Access base_paths collection 

Collection name: ""base_paths"" 

Collection accessible 

3 

Get dynamic recording collection GUID 

Query base_paths for GUID 

GUID retrieved 

4 

Access recording collection 

Collection name: {GUID} 

Collection accessible 

5 

Count total recordings 

count_documents({}) 

Total count > 0 

CLASSIFICATION 

6 

Count Historical recordings 

Query: {start_time: exists, end_time: {$ne: null}, deleted: false} 

Count returned 

7 

Count Live recordings 

Query: {start_time: exists, end_time: null, deleted: false} 

Count returned 

8 

Count Deleted recordings 

Query: {deleted: true} 

Count returned 

9 

Count Invalid recordings 

Query: {start_time: {$exists: false}} 

Should be 0 

10 

Calculate percentages 

(count / total) * 100 

Percentages calculated 

11 

Log classification summary 

All categories 

Summary displayed 

VALIDATION 

12 

Sample Historical recordings 

Limit: 5, sort by start_time 

Retrieve 5 records 

13 

Verify Historical have complete metadata 

Check: uuid, start_time, end_time, deleted 

All fields present 

14 

Calculate recording durations 

(end_time - start_time) in hours 

Duration calculated 

15 

Log Historical samples 

UUID, Start, End, Duration 

Details displayed 

LIVE ANALYSIS 

16 

Check Live recordings age 

For each Live, calculate age from start_time 

Age in hours 

17 

Identify stale Live recordings 

Filter: age > 24 hours 

Stale recordings listed 

18 

If stale found, log details 

UUID, age, start_time 

Warning displayed 

19 

If no stale, confirm all Live are recent 

Age < 24 hours 

Success message 

CLEANUP SERVICE 

20 

Count Deleted with end_time 

Query: {deleted: true, end_time: {$ne: null}} 

Count returned 

21 

Count Deleted without end_time 

Query: {deleted: true, end_time: null} 

Count returned 

22 

Calculate Deleted percentages 

(count / total_deleted) * 100 

Percentages calculated 

23 

Sample Deleted recordings 

Limit: 3 

Retrieve 3 deleted records 

24 

For each Deleted, calculate age 

(now - start_time) in days 

Age calculated 

25 

Log Deleted samples 

UUID, Started, Age, Has end_time 

Details displayed 

26 

If no Deleted, note cleanup status 

Check if cleanup service active 

Note displayed 

ASSERTIONS 

27 

Assert: No invalid recordings 

invalid_count == 0 

PASS if 0, FAIL otherwise 

28 

Assert: Classification integrity 

historical + live + deleted == total 

PASS if match 

29 

Assert: Historical majority 

(historical / total) > 0.50 

PASS if >50% 

30 

Log final test result 

Overall status 

Test result displayed 

Expected Result (overall)

Classification Results

Historical (completed): ~99% of total recordings

Live (in-progress): <1% of total recordings (recent, <24h old)

Deleted (cleanup): <1% of total recordings

Invalid (no start_time): 0 recordings ✅

Stale (crashed/failed): 0 recordings ✅

Data Integrity

✅ All recordings have start_time

✅ Classification totals match overall count

✅ Historical recordings are the majority (>50%)

✅ All Historical recordings have complete metadata (uuid, start_time, end_time)

✅ Live recordings are recent (<24 hours old)

⚠️ Deleted recordings may be missing end_time (deleted while running)

Cleanup Service Behavior

Cleanup service (Sweeper/Data Manager) sets deleted=True

Some deleted recordings may lack end_time (indicates deletion during recording)

Pattern analysis of deleted recordings shows cleanup policy (age, retention)

Post-Conditions

None (read-only test)

Note: If test identifies issues (stale recordings, improper cleanup), these should be reported but do not affect test cleanup.

Attachments

Recording classification report (JSON/CSV)

Sample Historical recordings (JSON)

Sample Deleted recordings (JSON)

Stale recordings list (if any found)

Test execution log with full details

Authentication

MongoDB credentials from environments.yaml

Read-only access required (no write operations)

Assertions

Critical Assertions (Test FAILS if violated)

No Invalid Recordings:

assert invalid_count == 0, 

f""Found {invalid_count} recordings without start_time!""

Classification Integrity:

assert (historical_count + live_count + deleted_count) == total_count, 

f""Classification mismatch: {classified} vs {total}""

Historical Majority:

assert (historical_count / total_count) > 0.50, 

f""Historical recordings only {percentage:.1f}% (expected >50%)""

Warning Assertions (Test WARNS but doesn't FAIL)

Stale Recordings Detection:

if stale_count > 0:
logger.warning(f""Found {stale_count} stale recordings (>24h without end_time)"")

Deleted Missing end_time:

if deleted_without_endtime > 0:
logger.warning(f""{deleted_without_endtime} deleted recordings missing end_time"")

Automation Status

✅ Automated with Pytest

Test Function: test_historical_vs_live_recordings  
Test File: tests/integration/infrastructure/test_mongodb_data_quality.py  
Test Class: TestMongoDBDataQuality

Execution Command

# Run this test alone
pytest tests/integration/infrastructure/test_mongodb_data_quality.py::TestMongoDBDataQuality::test_historical_vs_live_recordings -v

# Run with detailed output
pytest tests/integration/infrastructure/test_mongodb_data_quality.py::TestMongoDBDataQuality::test_historical_vs_live_recordings -v -s

# Run all data lifecycle tests
pytest -m data_lifecycle -v

# Run all MongoDB data quality tests
pytest tests/integration/infrastructure/test_mongodb_data_quality.py -v

Related Issues

[PZ-13598] (Parent - MongoDB Data Quality)

T-DATA-001 (Soft Delete Implementation - Related test)

BUG-CLEANUP-001 (Deleted recordings missing end_time - Discovered by this test)

Sample Historical Recordings

UUID

Duration

Status

38e432b0-7c87-468c-9b85-fd48462d8901 

1.97 hours 

✅ Valid 

2b58e51e-50fd-4ad8-a623-46f5d48c9e8b 

0.01 hours 

✅ Valid 

55c582e8-1de4-4a03-b7e2-bd803e03264f 

0.02 hours 

✅ Valid 

Deleted Recordings Analysis

With end_time: 0 (0%)

Without end_time: 24 (100%)

Age: 84 days (all deleted on 2025-07-23)

Pattern: Bulk cleanup operation or retention policy

Performance Metrics

Test Duration: ~10 seconds

Database Queries: 15 queries

Data Scanned: 3,439 recordings

Memory Usage: Low (streaming queries)

Known Limitations

24-hour heuristic for stale detection:

Recordings older than 24h without end_time are flagged as stale

May have false positives for extremely long recordings

Recommendation: Add explicit status field in future

Dynamic collection naming:

Test depends on base_paths collection to find recording collection GUID

If base_paths is missing or corrupted, test will fail

Cleanup service identification:

Test can detect cleanup behavior but cannot identify which service performed cleanup

Action required: Manual confirmation with development team

Success Criteria

Test PASSES if:

✅ All recordings have start_time (no invalid records)

✅ Classification totals match overall count

✅ Historical recordings are >50% of total

✅ All Historical recordings have complete metadata

✅ Live recordings are recent (<24h old)

✅ No stale recordings detected

Test WARNS if:

⚠️ Deleted recordings missing end_time

⚠️ Live recordings count is unusually high (>5%)

⚠️ Recognition rate is low (<80%)

Test FAILS if:

❌ Any recordings missing start_time

❌ Classification doesn't match total count

❌ Historical recordings <50% (indicates cleanup issues)

❌ Stale recordings detected (>24h without end_time, not deleted)",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3964fce0,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02t3r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 8:34 PM
MongoDB Recovery – Recordings Indexed After Outage,PZ-13687,42961,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 2:04 PM,19/Oct/25 5:33 PM,,,,0,"Test Summary:
Validates that recordings added to storage during a MongoDB outage are automatically indexed once MongoDB recovers.

Summary: Focus Server – MongoDB Recovery and Indexing

Objective: Confirm that new recordings are indexed post-outage without manual intervention.

Priority: Critical

Components/Labels: focus-server, mongodb, resilience, recovery, indexing, data-sync

Requirements: PZ-13604, FOCUS-RESILIENCE-RECOVERY

Pre-Conditions:

MongoDB is healthy.

Kubernetes access available.

Focus Server recovery mechanism active.

Recording storage available.

Test Data:
Namespace: default
Deployment: mongodb
New recording file created during outage.

Steps:
a. Action: Verify MongoDB health
Expected: Connection successful

b. Action: Count existing recordings
Expected: Returns count N

c. Action: Simulate outage
Data: Scale replicas=0
Expected: Mongo unreachable

d. Action: Add new recording file
Expected: File created successfully

e. Action: Recover MongoDB
Data: Scale replicas=1
Expected: Mongo accessible

f. Action: Wait for recovery indexing
Expected: New recording indexed automatically

g. Action: Verify new recording appears
Expected: Found in node4

Expected Result (overall):
MongoDB recovery triggers automatic indexing of new recordings. No data loss occurs.

Post-Conditions:

MongoDB restored

Test recording cleaned up

Assertions:

Outage simulated successfully

Recovery completed

New recording indexed automatically

Count increases by +1

Environment:
Staging only (requires Kubernetes access)

Automation Status:
Automated integration test with Pytest
Test Function: test_mongodb_recovery_indexes_pending_recordings
Test File: tests/integration/infrastructure/test_mongodb_outage_resilience.py",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@61dac687,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02szz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 2:04 PM
Data Quality – MongoDB Indexes Validation,PZ-13686,42960,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 2:03 PM,19/Oct/25 5:33 PM,,,,0,"Test Summary:
Ensures critical indexes exist on node4 for efficient queries. Missing indexes degrade performance during time-based lookups.

Summary: Focus Server – MongoDB Indexes Validation

Objective: Verify the existence of required indexes on node4.

Priority: High

Components/Labels: focus-server, mongodb, data-quality, performance, indexes

Requirements: PZ-13598, FOCUS-PERF-QUERY

Pre-Conditions:

MongoDB reachable.

node4 collection exists.

Database admin access available.

Test Data:
Expected indexes:

start_time (ascending)

end_time (ascending)

uuid (unique, ascending)

deleted (ascending)

Steps:
a. Action: List indexes on node4
Expected: Index list returned

b. Action: Verify index
Data: start_time asc
Expected: Found

c. Action: Verify index
Data: end_time asc
Expected: Found

d. Action: Verify unique index
Data: uuid asc
Expected: Found and marked unique

e. Action: Verify index
Data: deleted asc
Expected: Found

Expected Result (overall):
All expected indexes exist and are correctly configured.

Post-Conditions:
None

Assertions:

Indexes found on all required fields

uuid is unique

No missing indexes

Environment:
Any (Dev / Staging / Production)

Automation Status:
Automated integration test with Pytest
Test Function: test_mongodb_indexes_exist_and_optimal
Test File: tests/integration/infrastructure/test_mongodb_data_quality.py",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@e1dc0ab,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02szr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 2:03 PM
Data Quality – Recordings Metadata Completeness,PZ-13685,42959,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 2:02 PM,19/Oct/25 5:33 PM,,,,0,"Test Summary:
Verifies that all recordings in node4 have complete metadata. Missing metadata indicates data corruption and impacts history playback.

Summary: Focus Server – Recordings Metadata Completeness

Objective: Validate metadata completeness for all recordings in node4.

Priority: Critical

Components/Labels: focus-server, mongodb, data-quality, data-integrity, recordings

Requirements: PZ-13598

Pre-Conditions:

MongoDB is reachable.

node4 collection exists.

Real recording data is available.

Test Data:

Collection: node4

Fields: uuid, start_time, end_time, deleted

Steps:
a. Action: Access node4 collection
Expected: Collection accessible

b. Action: Count total documents
Expected: Count returned

c. Action: Count missing values per field
Expected: Zero missing for all critical fields

d. Action: Identify orphaned records
Expected: No documents missing ≥2 critical fields

e. Action: Generate completeness report
Expected: Report summary logged (CSV/JSON)

Expected Result (overall):
All recordings have full metadata; no missing or null fields; no orphaned records.

Post-Conditions:
None (read-only)

Assertions:

No missing metadata fields

No nulls

No orphaned records

Environment:
Staging / Production

Automation Status:
Automated integration test with Pytest
Test Function: test_recordings_have_all_required_metadata
Test File: tests/integration/infrastructure/test_mongodb_data_quality.py",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@233dcf34,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02szj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 2:02 PM
Data Quality – node4 Schema Validation,PZ-13684,42958,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 2:02 PM,19/Oct/25 5:33 PM,,,,0,"Test Summary:
Ensures that documents in the node4 collection contain all required fields with correct data types to prevent playback or parsing errors.

Summary: Focus Server – node4 Schema Validation

Objective: Validate field existence and data types for node4 documents.

Priority: High

Components/Labels: focus-server, mongodb, data-quality, schema, node4

Requirements: PZ-13598

Pre-Conditions:

MongoDB is reachable.

node4 collection exists and contains data.

At least one document is available.

Test Data:

Collection: node4

Sample: up to 100 documents

Required fields: uuid, start_time, end_time, deleted

Steps:
a. Action: Access node4 collection
Data: Collection name = node4
Expected: Collection accessible

b. Action: Count total documents
Data: N/A
Expected: Count returned

c. Action: Sample up to 100 documents
Data: min(100, total)
Expected: Documents retrieved

d. Action: Validate field presence and type
Data: uuid, start_time, end_time, deleted
Expected: Fields exist and match expected type

e. Action: Check for null values
Data: Critical fields only
Expected: No null values present

Expected Result (overall):
All sampled documents have required fields, correct types, and no nulls.

Post-Conditions:
None (read-only)

Assertions:

All fields exist

Data types match expected types

No null values in critical fields

Environment:
Any (Dev / Staging / Production)

Automation Status:
Automated integration test with Pytest
Test Function: test_node4_schema_validation
Test File: tests/integration/infrastructure/test_mongodb_data_quality.py",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3e5a3be0,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02szb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 2:02 PM
Data Quality – MongoDB Collections Exist,PZ-13683,42957,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,15/Oct/25 2:01 PM,19/Oct/25 5:33 PM,,,,0,"Test Summary:
Validates that all required MongoDB collections exist. Missing collections cause Focus Server to fail during metadata queries or history playback.

Summary: Focus Server – Data Quality – MongoDB Collections Exist

Objective: Verify that base_paths, node2, and node4 collections exist in the database.

Priority: High

Components/Labels: focus-server, mongodb, data-quality, infrastructure, database

Requirements: PZ-13598

Pre-Conditions:

MongoDB is reachable and accessible.

Database name is configured in environments.yaml.

Valid MongoDB credentials are available.

Test Data:

Database: prisma (staging) or focus_db (local).

Required collections: base_paths, node2, node4.

Steps:
a. Action: Connect to MongoDB
Data: Database name from config
Expected: Connection successful

b. Action: List all collections
Data: N/A
Expected: List of collections returned

c. Action: Verify collection existence
Data: base_paths, node2, node4
Expected: Each required collection is present

Expected Result (overall):
All required MongoDB collections exist. No missing or misnamed collections.

Post-Conditions:

None (read-only validation)

Assertions:

All required collections exist

No missing critical collections

Environment:
Any environment (Dev / Staging / Production)

Automation Status:
Automated integration test with Pytest
Test Function: test_required_collections_exist
Test File: tests/integration/infrastructure/test_mongodb_data_quality.py",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7f7d5148,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02sz3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,15/Oct/25 2:01 PM
Integration – Orchestrator error triggers rollback,PZ-13604,42581,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:41 PM,19/Oct/25 5:33 PM,,,,0,"- Summary: Focus Server – History configure – Mongo unavailable → 503, no orchestration
- Objective: Ensure dependency outage fails fast and clean without launching processing.
- Priority: High
- Components/Labels: focus-server, integration, history, mongo, resilience
- Requirements: FOCUS-API-CONFIGURE
- Pre-Conditions:
  - PC-040 Disable Mongo or block network to it
- Test Data:
  - Valid history payload
- Steps:
  1) Action: POST /configure | Data: history payload | Expected: 503 (or documented), clear message; no job_id
  2) Action: K8s/Rabbit | Data: events/queues | Expected: no new resources
  3) Action: Logs | Data: server logs | Expected: dependency error; no crash
- Expected Result (overall):
  - Clean failure; no side effects.
- Post-Conditions:
  - None
- Attachments:
  - Error/logs; dependency outage proof",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@62284553,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02si7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,"08/Oct/25 11:23 AM;712020:f3895741-cf6d-444c-92af-d001de870705;need to fix the test, the test should check that in case that MongDB is off for while and then return to work , the recordings that didnt processed in this time will be indexing after that  ",To Do,05/Oct/25 3:41 PM
Integration – Mongo outage on History configure,PZ-13603,42580,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:41 PM,19/Oct/25 5:33 PM,,,,0,"Summary: Focus Server – History configure – Mongo unavailable → 503, no orchestration
- Objective: Ensure dependency outage fails fast and clean without launching processing.
- Priority: High
- Components/Labels: focus-server, integration, history, mongo, resilience
- Requirements: FOCUS-API-CONFIGURE
- Pre-Conditions:
  - PC-040 Disable Mongo or block network to it
- Test Data:
  - Valid history payload
- Steps:
  1) Action: POST /configure | Data: history payload | Expected: 503 (or documented), clear message; no job_id
  2) Action: K8s/Rabbit | Data: events/queues | Expected: no new resources
  3) Action: Logs | Data: server logs | Expected: dependency error; no crash
- Expected Result (overall):
  - Clean failure; no side effects.
- Post-Conditions:
  - None
- Attachments:
  - Error/logs; dependency outage proof",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4c8f400a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02shz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,05/Oct/25 3:41 PM
Integration – RabbitMQ outage on Live configure,PZ-13602,42579,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:40 PM,19/Oct/25 5:33 PM,,,,0,"Summary: Focus Server – Live configure – RabbitMQ unavailable → fail fast, no K8s job
- Objective: Validate graceful upstream failure (AMQP) without launching downstream resources.
- Priority: High
- Components/Labels: focus-server, integration, live, rabbit, resilience
- Requirements: FOCUS-API-CONFIGURE
- Pre-Conditions:
  - PC-030 RabbitMQ blocked/blackholed for test or credentials invalid
- Test Data:
  - Valid live payload
- Steps:
  1) Action: POST /configure | Data: live payload | Expected: 503/502 (documented), friendly error; no job_id
  2) Action: K8s | Data: events | Expected: no Job/Service created
  3) Action: Logs | Data: focus-server logs | Expected: upstream-unavailable entry; no crash/stacktrace
- Expected Result (overall):
  - Proper 5xx mapping with clear message; no resource leaks.
- Post-Conditions:
  - None
- Attachments:
  - Error body; Rabbit health proof; logs",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5b7ecc61,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02shr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,05/Oct/25 3:40 PM
Integration – History with empty window returns 400 and no side effects,PZ-13601,42578,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:39 PM,19/Oct/25 5:33 PM,,,,0," Summary: Focus Server – History configure – No recordings → 400, no Baby/K8s, no storage reads
- Objective: Ensure “no data” short-circuit with no orchestration nor storage scanning.
- Priority: High
- Components/Labels: focus-server, integration, history, mongo, storage
- Requirements: FOCUS-API-CONFIGURE, FOCUS-API-REC-RANGE
- Pre-Conditions:
  - PC-010 MONGO_URI reachable; selected window has no recordings
- Test Data:
  - start_time/end_time where rec_mapper has zero hits
- Steps:
  1) Action: POST /configure | Data: empty-window payload | Expected: 400; message “No recording found…”
  2) Action: K8s | Data: events | Expected: no Job/Service created
  3) Action: Storage | Data: access logs/metrics | Expected: no storage read attempts
  4) Action: Mongo | Data: index | Expected: no new docs
- Expected Result (overall):
  - Clean 400; no orchestration/storage activity.
- Post-Conditions:
  - None
- Attachments:
  - Mongo proof of zero hits; logs",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5f1b3ad1,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02shj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,05/Oct/25 3:39 PM
Integration – Invalid configure does not launch orchestration,PZ-13600,42577,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:38 PM,19/Oct/25 5:33 PM,,,,0,"Summary: Focus Server – Invalid configure – No orchestration, no side effects

- Objective: Prove invalid input is rejected with 4xx and does not start Baby/K8s/Rabbit nor mutate indexes.
- Priority: Critical
- Components/Labels: focus-server, integration, validation, k8s, rabbit, mongo
- Requirements: FOCUS-API-VALIDATION
- Pre-Conditions:
  - PC-001 API reachable; focus-server logs accessible
  - PC-020 Observability available (Rabbit mgmt/K8s API/read-only Mongo)
- Test Data:
  - Payload: channels {min:300,max:100} or frequencyRange {min>max}
- Steps:
  1) Action: POST /configure | Data: invalid payload | Expected: 400; structured error; no job_id
  2) Action: Check orchestrator activity | Data: K8s API/events | Expected: no Job/Service/YAML applied
  3) Action: Check Rabbit | Data: queues list | Expected: no new grpc-job-* queue created
  4) Action: Check Mongo | Data: relevant index collections | Expected: no new documents for this request
  5) Action: Check logs | Data: server logs | Expected: validation error; no stacktrace/5xx
- Expected Result (overall):
  - 4xx rejection with zero cross-component side effects.
- Post-Conditions:
  - None
- Attachments:
  - Error body, K8s/Rabbit snapshots, log excerpt (redacted)",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6589700c,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02shb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,05/Oct/25 3:38 PM
Data Quality – Postgres connectivity and catalogs,PZ-13599,42576,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:19 PM,19/Oct/25 5:33 PM,,,,0,"Summary: Data Quality – Postgres – Connectivity and required system objects present

Objective: Validate DB connectivity and presence of core catalogs referenced by monitoring.

Components/Labels: data-quality, postgres, observability

Requirements: N/A (infra)

Pre-Conditions: POSTGRES_DSN reachable.

Test Data: N/A.

Steps:
  1) Action: Connect and run SELECT 1 | Data: N/A | Expected: returns 1
  2) Action: Check system relations | Data: pg_stat_activity, pg_database, pg_namespace | Expected: all present

Expected Result (overall): DB connectivity verified; required catalogs exist.

Post-Conditions: None

Attachments: ERD snippet or catalog list.",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2700a679,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02sh3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,05/Oct/25 3:19 PM
Data Quality – Mongo collections and schema,PZ-13598,42575,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,05/Oct/25 3:18 PM,19/Oct/25 5:33 PM,,,,0,"- Summary: Data Quality – Mongo – Collections exist and node4 schema sane
- Objective: Validate presence of critical collections and minimal schema integrity; detect illegal inserts.
- Priority: High
- Components/Labels: data-quality, mongo, history, rec-mapper
- Requirements: FOCUS-API-REC-RANGE
- Pre-Conditions:
  - MONGO_URI and DB reachable; seeded data.
- Test Data:
  - Collections: base_paths, node2, node4; node4 expected keys: uuid, start_time, end_time, deleted.
- Steps:
  1) Action: Connect to Mongo | Data: MONGO_URI | Expected: Connection successful
  2) Action: List collections | Data: N/A | Expected: base_paths, node2, node4 present
  3) Action: Sample node4 doc | Data: N/A | Expected: required keys and reasonable types
  4) Action: Try invalid insert | Data: wrong types | Expected: Insert rejected (if validation enforced) or flagged for schema enforcement
- Expected Result (overall):
  - Collections present; schema sanity; illegal insert detected or blocked.
- Post-Conditions:
  - Remove any accidental inserts.
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6cbf6aa3,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02sgv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,"08/Oct/25 11:14 AM;712020:f3895741-cf6d-444c-92af-d001de870705;Add tests that check the indexes of the mongoDB collocations on all the recording that exist in the DB 

check for missing recording metadata in the MongoDB ",To Do,05/Oct/25 3:18 PM
Security – Robustness to malformed inputs,PZ-13572,42328,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 4:24 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Stress-tests input validation and basic hardening: missing/extra fields, extreme values, type violations, and simple injection attempts. Expects precise {{4xx}} responses, no server crashes, no sensitive data leakage, and correct CORS headers where applicable.

* Summary: Focus Server – Security – Malformed inputs do not crash server
* Objective: Validate input hardening: malformed/oversized/CORS/SQLi-like params.
* Priority: High
* Components/Labels: focus-server, security, input-validation, cors
* Requirements: FOCUS-SEC-ROBUSTNESS
* Pre-Conditions:
** Logging enabled for auditing; server reachable.
* Test Data:
** Malformed JSON; OPTIONS request; oversized payload (e.g., 10MB); query params with injection strings.
* Steps:
*# Action: POST /configure with malformed JSON | Data: corrupt body | Expected: 4xx; not 5xx
*# Action: OPTIONS any endpoint | Data: N/A | Expected: CORS headers present
*# Action: GET with SQLi-like params | Data: e.g., ?search='1'%20OR%20'1'='1 | Expected: No 500; sanitized
*# Action: POST oversized payload | Data: ~10MB | Expected: rejected/handled gracefully; no crash
* Expected Result (overall):
** No 5xx due to malformed inputs; stable behavior; CORS ok.
* Post-Conditions:
** None.

*Test Data:*
Various malformed and malicious payloads:

* {{{""invalid_json"": unclosed}}
* {{?search='1' OR '1'='1}}
* {{?id=<script>alert('xss')</script>}}
* Extremely large JSON payload (10MB+)

*Authentication:*
N/A

*Assertions:*

* No 500 Internal Server Error for malformed inputs
* CORS headers present on OPTIONS requests
* SQLi attempts do not cause crashes
* XSS attempts are sanitized
* Large payloads rejected gracefully
* Server logs security events appropriately

*Environment:*
Any (preferably Staging)

*Automation Status:*
Automated security smoke test
Test Function: {{test_security_resilience}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@53818011,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02san:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 4:24 PM
Performance – /configure latency p95,PZ-13571,42327,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 4:22 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Measures {{/configure}} response times under varying loads/concurrency for both Live and Historical flows. Tracks p50/p95/p99 against targets, watches error rates/timeouts, and flags regressions or scalability limits under burst conditions.

 Summary: Focus Server – /configure latency – p95 < 2.0s under minimal load

* Objective: Ensure low-latency control plane for configure requests.
* Priority: Low
* Components/Labels: focus-server, performance, latency, configure
* Requirements: FOCUS-PERF-CONFIGURE
* Pre-Conditions:
** Minimal concurrent load on environment.
* Test Data:
** Live payload; 5 sequential requests.
* Steps:
*# Action: Send 5 POST /configure | Data: live payload | Expected: All 200
*# Action: Compute p95 | Data: collected latencies | Expected: p95 < 2.0s
* Expected Result (overall):
** SLA met for latency.
* Post-Conditions:
** None.

*Test Data:*
Standard live configure payload

*Authentication:*
N/A

*Assertions:*

* Response time < 2.0 seconds (95th percentile)
* No significant variance between runs
* Server remains stable during repeated requests
* No memory leaks or resource exhaustion

*Environment:*
Dev/Staging (non-loaded)

*Automation Status:*
Automated performance smoke test
Test Function: {{test_configure_latency}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3550a94e,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02saf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,08/Oct/25 11:12 AM;712020:f3895741-cf6d-444c-92af-d001de870705;need to add memory load tests ,To Do,29/Sep/25 4:22 PM
E2E – Configure → Metadata → gRPC (mock),PZ-13570,42326,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 4:20 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Runs a full integration path (with mock or real analyzer): {{/configure}} → {{GET /metadata/{job_id}}} → connect to the *gRPC* stream and receive data frames. Verifies *time-to-first-message* SLO, frame shape/content sanity, and stable streaming without disconnect loops.

* Summary: Focus Server – E2E (mock) – Stream contract and metadata roundtrip
* Objective: Validate config→metadata roundtrip and stream contract (shape/fields).
* Priority: High
* Components/Labels: focus-server, e2e, grpc, mock
* Requirements: FOCUS-E2E-GRPC
* Pre-Conditions:
** Mock gRPC server/client available and reachable.
* Test Data:
** Live configure payload; expected stream fields {timestamps, data shapes, mapping}.
* Steps:
*# Action: POST /configure | Data: live payload | Expected: 200; job_id, stream_url, stream_port
*# Action: GET /metadata/{job_id} | Data: job_id | Expected: 200; config matches
*# Action: Connect to stream | Data: stream_url:stream_port | Expected: connection established; frames contain expected fields
* Expected Result (overall):
** Roundtrip valid; stream contract holds.
* Post-Conditions:
** Job cleaned up.

*Test Data:*
Use standard configure payload from Test Case #1

*Authentication:*
N/A

*Assertions:*

* Configuration successful (200)
* Metadata retrieval successful
* gRPC connection established
* DataStream messages received
* Data fields present: timestamps, spectrograms, shapes
* No data corruption or connection errors

*Environment:*
Integration test environment (Dev/Staging)

*Automation Status:*
Automated integration test with Pytest
Test Function: {{test_e2e_grpc_mock_flow}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4f7648f2,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02sa7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 4:20 PM
Orchestrator – Available ID & YAML flow,PZ-13569,42325,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 4:15 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Validates the orchestrator’s end-to-end control flow: fetch free {{job_id}}, render YAML templates with correct substitutions, apply them in the right order, and remove them on teardown. Confirms method sequencing, no leaked resources, and robust handling of template errors.

* Summary: Orchestrator – start_research_job – Generates/applies/deletes YAML and returns URL
* Objective: Validate method order and returned coordinates.
* Priority: High
* Components/Labels: focus-server, service, orchestrator, k8s
* Requirements: FOCUS-ORCH-ID-YAML
* Pre-Conditions:
** get_ongoing_jobs_in_namespace mocked; job template available.
* Test Data:
** Command parameters; template values ($JOB_ID, $PORT, $NODE_PORT).
* Steps:
*# Action: get_available_job_id | Data: mocked namespace state | Expected: unused ID
*# Action: start_research_job | Data: command, job_id | Expected: generate_yml → apply_yaml → delete_yml; returns ""<base_url>:<port>""
* Expected Result (overall):
** Proper orchestration and cleanup; usable coordinates.
* Post-Conditions:
** No leftover YAML files.

*Test Data:*
Mock Kubernetes jobs and YAML templates

*Authentication:*
N/A (internal service test)

*Assertions:*

* Available job_id excludes occupied slots
* YAML generation includes correct substitutions
* apply_yaml and delete_yml are called
* Method execution order is correct
* No errors during YAML operations

*Environment:*
Unit/Integration test with K8S mocks

*Automation Status:*
Automated with Pytest (mocked)
Test Function: {{test_orchestrator_jobid_yaml_flow}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4a79c5c7,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s9z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 4:15 PM
GRPCLauncher Service - Start/Stop Behavior (Kubernetes),PZ-13568,42324,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 4:14 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Ensures the launcher correctly *starts* a per-job gRPC server (applies the right K8s YAML, gets a service/port) and *stops* it cleanly (deletes YAML/objects). Confirms correct call order, error handling, and that returned stream endpoints are reachable when started.

* Summary: GRPCLauncher – K8s – Starts job and stops with YAML cleanup
* Objective: Validate that launcher wiring calls orchestrator correctly and updates cfg.
* Priority: High
* Components/Labels: focus-server, service, launcher, grpc, k8s
* Requirements: FOCUS-SVC-LAUNCHER-K8S
* Pre-Conditions:
** Orchestrator mocked to return ""<base_url>:<port>"".
* Test Data:
** Minimal cfg object with job_id; command string.
* Steps:
*# Action: start(cfg, cmd) | Data: cfg, cmd | Expected: orchestrator called; cfg.stream_url starts with [http://;|http://;] cfg.stream_port > 0
*# Action: stop(job_id) | Data: job_id | Expected: orchestrator delete called once
* Expected Result (overall):
** Correct call order and populated cfg fields.
* Post-Conditions:
** None.

*Test Data:*
Mock orchestrator returns: ""[http://node-1:50001|http://node-1:50001]""

*Authentication:*
N/A (internal service test)

*Assertions:*

* start() calls orchestrator correctly
* Configuration is updated with stream details
* stop() triggers YAML cleanup
* No exceptions or errors during execution

*Environment:*
Unit/Integration test with K8S mocks

*Automation Status:*
Automated with Pytest (mocked)
Test Function: {{test_k8s_launcher_start_stop}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4839a0ae,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s9r:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,08/Oct/25 11:09 AM;712020:f3895741-cf6d-444c-92af-d001de870705;need to add cleanup test,To Do,29/Sep/25 4:14 PM
FocusManager Service - Job ID Allocation (Kubernetes Mode),PZ-13566,42322,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:50 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
In K8s mode, validates that {{job_id}} selection *excludes* occupied slots based on the current cluster state ({{ongoing jobs}}). Ensures the chosen {{job_id}} is unique, follows naming conventions, and remains consistent under concurrency.

 Summary: FocusManager – K8s mode – Excludes occupied slots

* Objective: Ensure selection excludes slots from ongoing “cleanup-job-*” names.
* Priority: Medium
* Components/Labels: focus-server, service, focus-manager, k8s
* Requirements: FOCUS-SVC-FOCUSMANAGER
* Pre-Conditions:
** k8s_mode=True; ongoing jobs mocked to include slots 2 and 3.
* Test Data:
** Mock list: [""cleanup-job-2-5"", ""cleanup-job-3-7""]
* Steps:
*# Action: get_job_id() | Data: running_index set | Expected: job_id not using slots 2 or 3; format ""<gpu>-<idx>""
* Expected Result (overall):
** Available slot selected; format preserved.
* Post-Conditions:
** None.

*Test Data:*
Mock Kubernetes namespace with specific occupied jobs

*Authentication:*
N/A (internal service test)

*Assertions:*

* Occupied slots are excluded
* Returned job_id is available
* Format is correct
* Collision avoidance works

*Environment:*
Unit test environment (K8S mocked)

*Automation Status:*
Automated with Pytest (mocked)
Test Function: {{test_focus_manager_job_id_k8s}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@6791a5df,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s9b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:50 PM
Focus Manager Service - Job ID and Port Allocation (Local Mode),PZ-13565,42321,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:49 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Verifies local-mode {{job_id}} *format* and deterministic *port mapping*. Ensures no port collisions, consistent mapping rules, and a stable {{job_id}} pattern (e.g., reflecting slot/index). Confirms cleanup leaves no orphaned allocations.

* Summary: FocusManager – Local mode – Deterministic job_id and port mapping
* Objective: Validate local format and port mapping formula.
* Priority: Medium
* Components/Labels: focus-server, service, focus-manager, local
* Requirements: FOCUS-SVC-FOCUSMANAGER
* Pre-Conditions:
** k8s_mode=False.
* Test Data:
** N/A.
* Steps:
*# Action: get_job_id() | Data: N/A | Expected: ""<slot>-<index>"" format (local policy)
*# Action: get_port(job_id) | Data: job_id | Expected: BasePort + slot
* Expected Result (overall):
** Deterministic mapping verified.
* Post-Conditions:
** None.

*Test Data:*
Mock configuration with BasePort = 50000

*Authentication:*
N/A (internal service test)

*Assertions:*

* job_id follows expected format
* Port calculation is correct
* No port collisions for different jobs
* GPU slot mapping is accurate

*Environment:*
Unit test environment (local)

*Automation Status:*
Automated with Pytest (mocked)
Test Function: {{test_focus_manager_job_id_and_port_local}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1a32c214,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s93:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,08/Oct/25 11:08 AM;712020:f3895741-cf6d-444c-92af-d001de870705;Need to check if the port is closed after we close the investigation/ when we close the app/ crash/ disconnect the lan or wifi ect.,To Do,29/Sep/25 3:49 PM
API – POST /recordings_in_time_range,PZ-13564,42320,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:45 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Checks that the endpoint returns *precise* recording windows (epoch tuples) that intersect a requested time range. Ensures ordering, non-overlap (if guaranteed), valid timestamp types, and alignment with stored recordings.

Summary: Focus Server – Recordings in time range – Returns intersecting windows

* Objective: Validate returned windows overlap request and policy on ongoing recordings.
* Priority: High
* Components/Labels: focus-server, api, history, mongo
* Requirements: FOCUS-API-REC-RANGE
* Pre-Conditions:
** rec_mapper seeded for requested range.
* Test Data:
** start_ts, end_ts (epoch) with known recordings; include an ongoing run if applicable.
* Steps:
*# Action: POST /recordings_in_time_range | Data: window | Expected: 200; list of [start_ts, end_ts]
*# Action: Validate ongoing policy | Data: response | Expected: ongoing end per implementation (e.g., equals request end)
* Expected Result (overall):
** Correct, overlapping windows returned in consistent format.
* Post-Conditions:
** None.

*Test Data:*

json

{noformat}{
  ""start_time"": 1700000000,
  ""end_time"": 1700000600
}{noformat}

*Authentication:*
N/A

*Assertions:*

* Status Code: 200
* Response is a list of tuples/arrays
* Timestamps are valid epoch values
* Recordings match requested time range

*Environment:*
Dev/Staging with MongoDB seed data

*Automation Status:*
Automated with Pytest
Test Function: {{test_recordings_in_time_range}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5ea2a007,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s8v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:45 PM
API - GET /metadata/{job_id} - Valid and Invalid Job ID,PZ-13563,42319,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:44 PM,20/Oct/25 11:11 AM,,,,0,"*Summary:* API – GET /metadata/{task_id} – Valid Task Metadata Retrieval

*Objective:* Add test for valid metadata retrieval (currently only invalid case exists).

*Priority:* High

*Components/Labels:* focus-server, api, metadata, tasks

*Requirements:* FOCUS-API-METADATA

*Pre-Conditions:*

* PC-001: Valid task created via /config/{task_id}
* PC-002: Task exists in system

*Test Data:*

{noformat}Valid task_id: From previous /config call
Invalid task_ids: ""non_existent"", ""12345"", """", ""../../etc/passwd""{noformat}

*Steps:*

||#||Action||Data||Expected Result||
|1|Create task via POST /config|Valid payload|Get task_id|
|2|GET /metadata/{task_id}|Valid task_id|HTTP 200|
|3|Validate metadata returned|Response body|Contains configuration|
|4|Verify config matches|Original vs returned|Exact match|
|5|GET /metadata/{invalid}|""non_existent""|HTTP 404|
|6|Validate error message|404 response|""Task not found"" or similar|
|7|Test path traversal|""../../etc/passwd""|HTTP 400 or 404|
|8|Test empty ID|""""|HTTP 400|

*Expected Result (overall):*

* Valid tasks return complete metadata
* Invalid tasks return proper 404
* No security vulnerabilities
* Clear error messages

*Post-Conditions:*

* Test task cleaned up

*Assertions:*

{code:python}# Valid case
assert valid_response.status_code == 200
assert valid_response.json()['task_id'] == task_id
assert valid_response.json()['config'] == original_config

# Invalid case  
assert invalid_response.status_code == 404
assert ""not found"" in invalid_response.json()['error'].lower()

# Security
assert path_traversal_response.status_code in [400, 404]{code}

*Environment:* Any

*Automation Status:* PARTIALLY IMPLEMENTED (add valid case)
*Test Function:* {{test_metadata_for_invalid_task_id}} (exists), {{test_metadata_for_valid_task_id}} (TO ADD)
*Test File:* {{tests/integration/api/test_live_monitoring_flow.py}}

*Notes:*

* Currently only invalid case tested
* Add valid case test",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@32747c5d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s8n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:44 PM
API – GET /live_metadata missing,PZ-13562,42318,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:33 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
If live fiber metadata is not available, the server returns a *clean* {{404}} with a clear message. Ensures no ambiguous {{200}} with empty payload, and no server errors.

* Summary: Focus Server – Live metadata missing – Returns 404 with clear error
* Objective: Validate clean error when metadata unavailable.
* Priority: Medium
* Components/Labels: focus-server, api, metadata, live
* Requirements: FOCUS-API-METADATA
* Pre-Conditions:
** fiber_metadata=None
* Test Data:
** N/A.
* Steps:
*# Action: GET /live_metadata | Data: N/A | Expected: 404 with meaningful message; no crash
* Expected Result (overall):
** Clear absence signal without server error.
* Post-Conditions:
** None.

*Test Data:*
N/A (GET request)

*Authentication:*
N/A

*Assertions:*

* Status Code: 404
* Error response contains meaningful message
* No server crash or unexpected behavior

*Environment:*
Dev/Staging without fiber_metadata

*Automation Status:*
Automated with Pytest
Test Function: {{test_live_metadata_missing}}

*Linked Issues:*
FOCUS-API-METADATA",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@34146438,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s8f:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:33 PM
API – GET /live_metadata present,PZ-13561,42317,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:31 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
When live fiber metadata exists, the endpoint returns a *complete* and well-typed structure. Confirms mandatory fields are present and values are in valid ranges (no nulls for required attributes).

* Summary: Focus Server – Live metadata present – Returns full metadata
* Objective: Validate metadata completeness and typing.
* Priority: High
* Components/Labels: focus-server, api, metadata, live
* Requirements: FOCUS-API-METADATA
* Pre-Conditions:
** fiber_metadata populated (dx, prr, fiber_* fields).
* Test Data:
** N/A.
* Steps:
*# Action: GET /live_metadata | Data: N/A | Expected: 200; required fields present and well-typed
* Expected Result (overall):
** Complete, valid metadata.
* Post-Conditions:
** None.

*Test Data:*
N/A (GET request)

*Authentication:*
N/A

*Assertions:*

* Status Code: 200
* All required metadata fields present
* Field values are valid and within expected ranges
* JSON response is well-formed

*Environment:*
Dev/Staging with fiber_metadata configured

*Automation Status:*
Automated with Pytest
Test Function: {{test_live_metadata_present}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1bf4584e,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s87:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:31 PM
API – GET /channels,PZ-13560,42316,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:29 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Ensures the endpoint returns the *authoritative* channel bounds (e.g., {{lowest_channel}}, {{highest_channel}}) for the deployment. Validates response schema, value ranges, and stability across calls.

* Summary: Focus Server – Channels endpoint – Returns authoritative channel bounds
* Objective: Validate channel range response is correct and stable.
* Priority: High
* Components/Labels: focus-server, api, channels
* Requirements: FOCUS-API-CHANNELS
* Pre-Conditions:
** FocusManager.channels configured.
* Test Data:
** N/A.
* Steps:
*# Action: GET /channels | Data: N/A | Expected: 200; lowest_channel=1; highest_channel=configured value
* Expected Result (overall):
** Range reflects server configuration.
* Post-Conditions:
** None.

*Test Data:*
N/A (GET request, no payload)

*Authentication:*
N/A

*Assertions:*

* Status Code: 200
* lowest_channel = 1
* highest_channel matches system configuration
* Response follows expected JSON schema

*Environment:*
Any (Dev/Staging/Prod)

*Automation Status:*
Automated with Pytest
Test Function: {{test_channels_endpoint}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1a6cf1cd,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s7z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:29 PM
API - Overlap/NFFT Escalation Edge Case,PZ-13558,42314,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:23 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Verifies the internal rule that when {{window_overlap}} is below a threshold, the server escalates {{internal_nfft}} up to a documented maximum and applies padding policy if needed. Ensures these derived values are reflected in metadata and remain within safe limits.

* Summary: Focus Server – Overlap escalation – Small window triggers internal_nfft increase
* Objective: Validate algorithm escalates internal_nfft under low overlap and sets padding flags if capped.
* Priority: Low
* Components/Labels: focus-server, api, algorithm, overlap, nfft
* Requirements: FOCUS-ALG-OVERLAP
* Pre-Conditions:
** Known PRR and display height to force overlap < 0.5.
* Test Data:
** Values tuned to produce window_overlap < 0.5 (document exact chosen parameters).
* Steps:
*# Action: POST /configure | Data: tuned payload | Expected: 200
*# Action: Validate escalation | Data: logs/flags/metadata | Expected: internal_nfft increased up to cap; if cap reached and view != SINGLECHANNEL → padding flags set
* Expected Result (overall):
** Correct escalation and padding policy execution.
* Post-Conditions:
** None.

*Test Data:*
Specific values calculated based on PRR and display height to force small overlap
(Values depend on system configuration)

*Authentication:*
N/A

*Assertions:*

* Status Code: 200
* internal_nfft escalation logic is applied correctly
* MAX_INTERNAL_NFFT cap is respected
* Padding flags are set when appropriate

*Environment:*
Dev/Staging

*Automation Status:*
Automated with Pytest
Test Function: {{test_configure_overlap_escalation}}

*Linked Issues:*
FOCUS-ALG-OVERLAP",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@478eaec,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s7j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:23 PM
API – Waterfall view handling,PZ-13557,42313,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:21 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Checks {{view_type=WATERFALL}} specifics: response includes Waterfall-specific parameters (e.g., display height and any view-specific derived fields) with internally consistent values. Confirms the configuration produces a stream suitable for Waterfall rendering (no schema drift vs. other views).

* Summary: Focus Server – Waterfall view – Response contains consistent waterfall parameters
* Objective: Validate response consistency and view_type for Waterfall.
* Priority: Medium
* Components/Labels: focus-server, api, view-type, waterfall
* Requirements: FOCUS-API-VIEWTYPE
* Pre-Conditions:
** Server reachable.
* Test Data:
** view_type=2; nfftSelection=1; displayInfo.height=800; channels 1..10; frequencyRange 0..1000.
* Steps:
*# Action: POST /configure | Data: waterfall payload | Expected: 200
*# Action: Validate response | Data: response | Expected: view_type=2; waterfall-specific parameters present/consistent (per spec)
* Expected Result (overall):
** Waterfall config returned; fields are coherent for rendering.
* Post-Conditions:
** None.

*Test Data:*

json

{noformat}{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 800 },
  ""channels"": { ""min"": 1, ""max"": 10 },
  ""frequencyRange"": { ""min"": 0, ""max"": 1000 },
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 2
}{noformat}

*Authentication:*
N/A

*Assertions:*

* Status Code: 200
* Response structure matches waterfall view requirements
* Waterfall-specific parameters are present and valid

*Environment:*
Any (Dev/Staging)

*Automation Status:*
Automated with Pytest
Test Function: {{test_configure_waterfall}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5f8e76aa,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s7b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:21 PM
API – SingleChannel view mapping,PZ-13556,42312,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:17 PM,19/Oct/25 5:33 PM,,,,0,"h2. Test Summary

Validates view_type=SINGLECHANNEL behavior: the server must return exactly one stream (stream_amount=1) and a single, correct channel mapping. Ensures the requested channel (min=max) maps 1:1 to the produced stream, with no extra channels or stray mappings.

h2. Summary

Focus Server – SingleChannel view – Exactly one stream and 1:1 mapping

h2. Objective

Prove SingleChannel returns exactly one stream and correct mapping.

h2. Components/Labels

focus-server, api, view-type, singlechannel

h2. Requirements

FOCUS-API-VIEWTYPE

h2. Pre-Conditions

* Server reachable

h2. Test Data

channels {min: X, max: X}; view_type=1; otherwise valid.

h2. Steps

||#||Action||Data||Expected||
|1|POST /configure|single channel payload|200|
|2|Validate stream_amount|response|stream_amount = 1|
|3|Validate mapping|response|exactly one entry; 1:1 mapping for requested channel|

h2. Expected Result (overall)

Correct single-stream behavior and mapping.

h2. Post-Conditions

None.

*Test Data:*

json

{noformat}{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 7, ""max"": 7 },
  ""frequencyRange"": { ""min"": 0, ""max"": 500 },
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 1
}{noformat}

h2. Authentication

N/A

h2. Assertions

* Status Code: 200
* stream_amount = 1
* channel_to_stream_index contains single entry
* Mapping is correct for the requested channel

h2. Environment

Any (Dev/Staging)

h2. Automation Status

Automated with Pytest

*Test Function:* {{test_configure_singlechannel_mapping}}  
*Test File:* {{tests/integration/api/test_singlechannel_view_mapping.py}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@60c6c21f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s73:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,08/Oct/25 10:53 AM;712020:f3895741-cf6d-444c-92af-d001de870705;check in the BE if its the same channel process ,To Do,29/Sep/25 3:17 PM
API – Invalid frequency range (negative),PZ-13555,42311,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:14 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Checks that malformed frequency ranges (negative values, {{min > max}}, unsupported units) are rejected with a {{400}} and an actionable error. Confirms the system does not continue into orchestration on bad input.

* Summary: Focus Server – Configure – Invalid frequency range rejected
* Objective: Ensure illegal frequency range is rejected with clear validation error.
* Priority: Medium
* Components/Labels: focus-server, api, validation
* Requirements: FOCUS-API-VALIDATION
* Pre-Conditions:
** Server reachable.
* Test Data:
** frequencyRange {min: 500, max: 0} plus otherwise valid payload.
* Steps:
*# Action: POST /configure | Data: invalid frequency range | Expected: 400 with validation details
* Expected Result (overall):
** Clear client error; no orchestration starts.
* Post-Conditions:
** None.

*Test Data:*

json

{noformat}{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 1, ""max"": 3 },
  ""frequencyRange"": { ""min"": 500, ""max"": 0 },
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 0
}{noformat}

*Authentication:*
N/A

*Assertions:*

* Status Code: 400
* Error message clearly indicates invalid frequency range
* Response contains validation error details

*Environment:*
Any (Dev/Staging/Prod)

*Automation Status:*
Automated with Pytest
Test Function: {{test_configure_invalid_frequency_range}}

*Linked Issues:*
FOCUS-API-VALIDATION",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@22255b81,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s6v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,,To Do,29/Sep/25 3:14 PM
API – Invalid channels (negative),PZ-13554,42310,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:12 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Confirms the server blocks illegal channel windows (e.g., {{channels.min > channels.max}} or out-of-bounds). Verifies a {{400}} with clear error details and no partial creation of jobs or streams.

* Summary: Focus Server – Configure – channels.min > channels.max rejected
* Objective: Ensure channel window is validated.
* Priority: Medium
* Components/Labels: focus-server, api, validation
* Requirements: FOCUS-API-VALIDATION
* Test Type: Manual
* Environments: Any
* Pre-Conditions: None
* Test Data:

{code:json}{ ""channels"": { ""min"": 300, ""max"": 100 } }{code}

* Steps:
*# Action: POST /configure | Data: payload with channels 300..100 | Expected: 400; message “Invalid channel range”
* Expected Result (overall):
** Client error; no orchestration.
* Post-Conditions: None
* Attachments: Request and error JSON

*Test Data:*

json

{noformat}{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 300, ""max"": 100 },
  ""frequencyRange"": { ""min"": 0, ""max"": 500 },
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 0
}{noformat}

*Authentication:*
N/A

*Assertions:*

* Status Code: 400
* Error message clearly indicates invalid channel range
* Response contains validation error details

*Environment:*
Any (Dev/Staging/Prod)

*Automation Status:*
Automated with Pytest
Test Function: {{test_configure_invalid_channels_range}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4e4830c2,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s6n:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,12/Oct/25 8:56 AM;712020:f3895741-cf6d-444c-92af-d001de870705;🎉 Looks good!,To Do,29/Sep/25 3:12 PM
API – Invalid time range (negative),PZ-13552,42308,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 3:02 PM,19/Oct/25 5:33 PM,,,,0,"*Test Summary:*
Ensures invalid time ranges (e.g., {{start_time >= end_time}}) are rejected. Expects a precise {{400 Bad Request}} with a helpful validation message and *no* resource allocation (no gRPC server, no job created).

* Summary: Focus Server – Configure – Invalid time range is rejected without side effects
* Objective: Ensure time validation blocks zero-length or inverted windows.
* Priority: Medium
* Components/Labels: focus-server, api, validation, history
* Requirements: FOCUS-API-VALIDATION
* Test Type: Manual
* Environments: Any
* Pre-Conditions: None
* Test Data:
** Payload A: start_time == end_time
** Payload B: start_time > end_time
* Steps:
*# Action: POST /configure | Data: Payload A | Expected: 400; error message “Invalid time range” (or documented wording); no job_id
*# Action: POST /configure | Data: Payload B | Expected: 400; same constraints
*# Action: GET /metadata/{any} | Data: N/A | Expected: 404 for non-created IDs
*# Check if the error is relevant to the scenario
* Expected Result (overall):
** No job creation; clear 4xx errors; logs show validation failure not server errors.
* Post-Conditions: None

*Test Data:*

json

{noformat}{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 1, ""max"": 3 },
  ""frequencyRange"": { ""min"": 0, ""max"": 500 },
  ""start_time"": 1700000000,
  ""end_time"": 1700000000,
  ""view_type"": 0
}{noformat}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4ec559c1,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s67:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,12/Oct/25 8:56 AM;712020:f3895741-cf6d-444c-92af-d001de870705;🎉 Looks good!,To Do,29/Sep/25 3:02 PM
API – Historical configure (happy path),PZ-13548,42298,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 1:52 PM,20/Oct/25 11:06 AM,,,,0,"*Summary:* API – POST /config/{task_id} – Historical Mode Configuration (Happy Path)

*Objective:* Validate historical configuration using /config/{task_id} endpoint with time range successfully creates a task for historical playback.

*Priority:* Critical

*Components/Labels:* focus-server, api, history, config, playback

*Requirements:* FOCUS-API-CONFIG, FOCUS-API-HISTORY

*Pre-Conditions:*

* PC-010: MongoDB reachable with historical recordings
* PC-011: Time range with available recordings
* PC-012: Valid task_id format available

*Test Data:*

{code:json}Endpoint: POST /config/{task_id}
task_id format: ""historic_playback_YYYYMMDD_HHMMSS_XXX""

Payload:
{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 1, ""max"": 3 },
  ""frequencyRange"": { ""min"": 0, ""max"": 500 },
  ""start_time"": 1700000000,
  ""end_time"": 1700000600,
  ""view_type"": 0
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Query MongoDB for recordings|Time range: 1700000000-1700000600|Recordings exist|
|2|Generate unique task_id|Format: historic_{timestamp}_{random}|Valid task_id|
|3|Validate task_id format|Check pattern|Format valid|
|4|Create ConfigTaskRequest|Historical payload|Request created|
|5|POST /config/{task_id}|task_id + payload|HTTP 200 OK|
|6|Validate response status|response.status|""Config received successfully""|
|7|Verify task created|System state|Task exists|

*Expected Result (overall):*

* Historical configuration accepted
* Task created for time range
* System ready for historical playback
* Recordings available for range

*Post-Conditions:*

* Task cleanup if needed
* No MongoDB modifications

*Assertions:*

{code:python}assert response.status_code == 200
assert isinstance(response, ConfigTaskResponse)
assert response.status == ""Config received successfully""
assert mongodb_recordings_exist(start_time, end_time)
assert validate_task_id_format(task_id) == True{code}

*Environment:* Staging/Production (requires data)

*Automation Status:* IMPLEMENTED
*Test Function:* {{test_configure_historic_task_success}}
*Test File:* {{tests/integration/api/test_historic_playback_flow.py}}

*Notes:*

* API uses /config/{task_id} not /configure
* Update Xray to reflect current endpoint",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@4e6b1a9f,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s5b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PZ-13756,,,,,,,,,,,,,,12/Oct/25 8:56 AM;712020:f3895741-cf6d-444c-92af-d001de870705;🎉 Looks good!,To Do,29/Sep/25 1:52 PM
API – POST /config/{task_id} – Live Mode Configuration (Happy Path),PZ-13547,42295,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,29/Sep/25 12:59 PM,20/Oct/25 11:01 AM,,,,0,"*Objective:* Validate that a valid live configuration using the /config/{task_id} endpoint successfully creates a task and returns proper confirmation.

*Priority:* Critical

*Components/Labels:* focus-server, api, live, config, task-management

*Requirements:* FOCUS-API-CONFIG

*Pre-Conditions:*

* PC-001: API base URL reachable ([https://10.10.100.100/focus-server/)|https://10.10.100.100/focus-server/)]
* PC-002: System operational and ready
* PC-003: Valid task_id format available

*Test Data:*

{code:json}Endpoint: POST /config/{task_id}
task_id format: ""live_monitoring_YYYYMMDD_HHMMSS_XXX""

Payload:
{
  ""displayTimeAxisDuration"": 10,
  ""nfftSelection"": 1024,
  ""displayInfo"": { ""height"": 1000 },
  ""channels"": { ""min"": 1, ""max"": 3 },
  ""frequencyRange"": { ""min"": 0, ""max"": 500 },
  ""start_time"": null,
  ""end_time"": null,
  ""view_type"": 0
}{code}

*Steps:*

||#||Action||Data||Expected Result||
|1|Generate unique task_id|Format: live_{timestamp}_{random}|Valid task_id created|
|2|Validate task_id format|Check regex pattern|Format valid|
|3|Create ConfigTaskRequest|Live payload|Request object created|
|4|POST /config/{task_id}|task_id + payload|HTTP 200 OK|
|5|Validate response status|response.status|""Config received successfully""|
|6|Validate task_id in response|response.task_id|Matches input or null|
|7|Check server logs|Application logs|No errors logged|

*Expected Result (overall):*

* Configuration accepted successfully
* Task created with valid ID
* System ready for live monitoring
* No errors in processing

*Post-Conditions:*

* Task remains in system
* Resources allocated for task
* Cleanup via DELETE if needed

*Assertions:*

{code:python}assert response.status_code == 200
assert isinstance(response, ConfigTaskResponse)
assert response.status == ""Config received successfully""
assert response.task_id == task_id or response.task_id is None
assert validate_task_id_format(task_id) == True{code}

*Environment:* Dev/Staging/Production

*Automation Status:* IMPLEMENTED
*Test Function:* {{test_configure_live_task_success}}
*Test File:* {{tests/integration/api/test_live_monitoring_flow.py}}

*Notes:* 

* API changed from {{/configure}} to {{/config/{task_id}}}",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@73e910c6,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02s4v:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,PZ-13756,,,,,,,,,,,,,,12/Oct/25 8:55 AM;712020:f3895741-cf6d-444c-92af-d001de870705;🎉 Looks good!,To Do,29/Sep/25 12:59 PM
Spike profile recovers quickly after sharp surges,PZ-13433,41702,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:59 PM,16/Sep/25 7:59 PM,,,,0,"Description
Generates sharp load spikes to test burst handling and recovery time. Confirms that after peak load, the system returns to baseline without prolonged error bursts.

Preconditions
* Same as above.

Inputs
* Example spike: base 20 → peak 400 users; rise 60s, hold 120s, fall 60s; spawn_rate=80/s.

Steps
1. Execute spike pattern with the specified timings.
2. During rise and hold, monitor for error bursts and elevated latency.
3. After fall, verify metrics return to baseline (throughput, latency, error rate).
4. Capture the time to recovery and confirm it meets internal criteria.

Expected Results
* System returns to baseline promptly after the spike.
* Sustained failure rate remains ≤ 5%.
* No persistent instability (e.g., rate limiting thrash, connection exhaustion).

Automation location
.../load_tests/locust_focus_server.py::SpikeProfile
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@42676ee5,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rcf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:59 PM
Steady profile sustains target throughput within SLOs,PZ-13432,41701,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:58 PM,16/Sep/25 7:58 PM,,,,0,"Description
Applies a steady sustained load for a fixed duration. Ensures the system maintains throughput, keeps error rate low, and hits latency SLOs.

Preconditions
* Same as above (reachability, data, auth, Locust installed).

Inputs
* Example parameters: users=120, spawn_rate=10/s, duration=20m.

Steps
1. Run the steady profile with chosen parameters.
2. Track p50/p95/p99 latency, throughput, and error rate throughout the run.
3. Note any timeouts/retries and ensure they remain within acceptable thresholds.

Expected Results
* p95 latency within SLO.
* Failure rate ≤ 5%.
* No degradation or memory/connection leak symptoms.

Automation location
.../load_tests/locust_focus_server.py::SteadyProfile
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@38edf068,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rc7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:58 PM
" Ramp profile maintains stability during ramp-up, steady, and ramp-down",PZ-13431,41700,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:58 PM,16/Sep/25 7:58 PM,,,,0,"Description
Executes a ramp load pattern to validate Focus Server resilience and performance. Verifies error rate, latency percentiles, and overall stability across ramp phases.

Preconditions
* Target environment reachable; representative data and auth configured.
* Locust installed and the load script configured for target base URL and headers.

Inputs
* Example parameters: users=200, spawn_rate=10/s, duration=300s.

Steps
1. Start Locust in headless mode with the above parameters.
2. Continuously monitor RPS, p50/p95/p99 latency, and failure rate.
3. Observe behavior across ramp-up, steady, and ramp-down phases.
4. Validate that metadata polling endpoints (e.g., /live_metadata) continue succeeding.
5. Export/collect Locust statistics (console or HTML report, if enabled).

Expected Results
* Failure rate ≤ 5%.
* No persistent 5xx spikes.
* Latency percentiles within the defined SLOs across phases.

Automation location
panda-backend-api-tests/focus_server_api_load_tests/load_tests/locust_focus_server.py::RampProfile
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3c9c08be,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rbz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:58 PM
Configure rejects bodies missing required fields (422),PZ-13430,41699,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:57 PM,16/Sep/25 7:57 PM,,,,0,"Description
Confirms 422 is returned when required fields are omitted for the selected mode, with field-specific error details.

Preconditions
* Validation enabled.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  { ""mode"": ""spectrum"" }

Steps
1. Submit minimal invalid body.
2. Assert HTTP 422.
3. Assert error payload precisely lists missing fields (e.g., channels, plus nfft or freq as applicable).

Expected Results
* 422; clear list of missing mandatory fields.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_missing_required_fields_422

",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3c929db7,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rbr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:57 PM
Configure enforces Waterfall NFFT = 1 (422 if violated),PZ-13429,41698,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:56 PM,16/Sep/25 7:56 PM,,,,0,"Description
Enforces Waterfall rule that nfft must equal 1. Any other value is rejected with 422 and a specific error.

Preconditions
* Waterfall rules active.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  { ""mode"": ""waterfall"", ""channels"": [1], ""nfft"": 4 }

Steps
1. Submit the payload above.
2. Assert HTTP 422.
3. Assert error message references nfft and the allowed value.

Expected Results
* 422; explicit message that nfft must be 1 for Waterfall.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_waterfall_nfft_must_be_1

",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@5edd73ad,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rbj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:56 PM
Configure rejects Waterfall requests containing forbidden fields (422),PZ-13428,41697,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:56 PM,16/Sep/25 7:56 PM,,,,0,"Description
Confirms that Waterfall mode must not include frequency range and similar forbidden fields. The server returns 422 with a precise error.

Preconditions
* Validation enabled; Waterfall rules defined.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  { ""mode"": ""waterfall"", ""channels"": [1], ""nfft"": 1, ""freq"": { ""min_hz"": 100, ""max_hz"": 2000 } }

Steps
1. Submit the payload above.
2. Assert HTTP 422.
3. Assert error explicitly states that freq (or frequency range) is not allowed in Waterfall mode.

Expected Results
* 422; clear rule violation message.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_waterfall_with_forbidden_fields_422

",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@65032df6,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rbb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:56 PM
Configure rejects out-of-range channel indices (422),PZ-13427,41696,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:55 PM,16/Sep/25 7:55 PM,,,,0,"Description
Validates server-side field validation by sending channels outside legal bounds and asserting a 422 Unprocessable Entity with detailed, field-level error information.

Preconditions
* Validation enabled with defined channel bounds.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  { ""mode"": ""spectrum"", ""channels"": [-1, 999999], ""nfft"": 256 }

Steps
1. Submit invalid payload.
2. Assert HTTP 422.
3. Assert error details reference channels and clearly describe the violation (e.g., “must be within [1..N]”).

Expected Results
* 422 with structured error payload identifying channels as invalid.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_channels_out_of_range_422

",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3d650800,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02rb3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:55 PM
Recordings timeline endpoint serves HTML content,PZ-13425,41694,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:48 PM,16/Sep/25 7:48 PM,,,,0,"Description
Ensures GET /get_recordings_timeline serves an HTML page for timeline visualization.

Preconditions
* Timeline endpoint enabled.

Inputs & Endpoint
* Endpoint: GET ${API_BASE}/get_recordings_timeline

Steps
1. Send GET request.
2. Assert HTTP 200.
3. Assert response header Content-Type is text/html (or starts with it).
4. (Optional) Visual sanity if opened in a browser (no JS console errors).

Expected Results
* 200 OK.
* HTML content is returned.

Automation location
.../focus_api_tests/focus_api_tests/test_recordings.py::test_get_recordings_timeline_html
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@535fc450,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ran:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:48 PM
Recordings API returns items for a valid time window,PZ-13424,41693,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:47 PM,16/Sep/25 7:47 PM,,,,0,"Description
Checks that GET /recordings with start/end returns 200 and a correct list of recordings in that window, with required metadata fields.

Preconditions
* Recordings exist in the supplied range.

Inputs & Endpoint
* Endpoint: GET ${API_BASE}/recordings?start=${T0}&end=${T1}

Steps
1. Send request with valid start and end.
2. Assert HTTP 200.
3. Assert body is a JSON array; each item contains at least:
   - id (string, non-empty),
   - start_ts (ISO-8601),
   - end_ts (ISO-8601),
   - channels (array of numbers, length ≥ 1).
4. Verify all items’ intervals intersect with [start, end].

Expected Results
* 200 OK with a correctly filtered list.
* All mandatory fields present; timestamps valid.

Automation location
.../focus_api_tests/focus_api_tests/test_recordings.py::test_recordings_in_time_range
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@1c2f7e27,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02raf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:47 PM
Configure: accept non-Waterfall configuration with frequency range and NFFT,PZ-13423,41692,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:46 PM,16/Sep/25 7:46 PM,,,,0,"Description
Validates POST /configure accepts a non-Waterfall mode (e.g., spectrum) with explicit frequency bounds and NFFT, applying them without errors.

Preconditions
* Configuration endpoint enabled.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  {
    ""mode"": ""spectrum"",
    ""channels"": [1],
    ""freq"": { ""min_hz"": 100, ""max_hz"": 2000 },
    ""nfft"": 1024
  }

Steps
1. Submit payload to /configure.
2. Assert HTTP 200.
3. Assert returned body echoes/applies mode, channels, freq.min_hz, freq.max_hz, and nfft.
4. (Optional) Validate bounds: min_hz < max_hz, both positive and within supported range.

Expected Results
* 200 OK.
* Config stored and applied; no 4xx.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_non_waterfall_with_freq_and_nfft

",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@54ebe3d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02ra7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:46 PM
Configure: accept minimal valid Waterfall configuration,PZ-13422,41691,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:45 PM,16/Sep/25 7:45 PM,,,,0,"Description
Ensures POST /configure accepts the minimal valid Waterfall configuration and persists/applies it. Confirms server enforces Waterfall constraints (e.g., nfft = 1).

Preconditions
* Configuration endpoint is enabled.
* No conflicting locks or maintenance windows.

Inputs & Endpoint
* Endpoint: POST ${API_BASE}/configure
* Payload (example):
  {
    ""mode"": ""waterfall"",
    ""channels"": [1, 2],
    ""nfft"": 1
  }

Steps
1. Submit the above JSON to /configure.
2. Assert HTTP 200.
3. Verify response indicates success (e.g., status: ""ok"" or echoed configuration).
4. (Optional) If a “get current config” endpoint exists, fetch and assert config persisted.

Expected Results
* 200 OK.
* Server accepts and applies configuration; Waterfall rule (nfft=1) is respected.

Automation location
.../focus_api_tests/focus_api_tests/test_configure.py::test_configure_waterfall_minimal
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3bf51651,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02r9z:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:45 PM
Live metadata endpoint exposes pipeline health and version (Smoke),PZ-13420,41689,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:35 PM,16/Sep/25 7:40 PM,,,,0,"Description
Confirms GET /live_metadata is available and returns a status snapshot with key health indicators: pipeline status, uptime, version, and counts. Validates types and minimal freshness/formatting.

Preconditions

* Server reachable; auth/SSL configured as needed.

Inputs & Endpoint

* Endpoint: GET ${API_BASE}/live_metadata

Steps

# Send GET /live_metadata.
# Assert HTTP 200.
# Assert response is a JSON object containing at least:
#* pipeline_status (string; one of: ""starting""|""running""|""degraded""|""stopped"" if defined),
#* uptime_sec (number, ≥ 0),
#* version (string, non-empty),
#* last_update_ts (ISO-8601 string),
#* channels_count (number, ≥ 0).
# Verify last_update_ts parses as a valid timestamp and is not wildly stale per environment expectation.

Expected Results

* 200 OK.
* All fields present and correctly typed.
* Values are sensible for the running environment.

Automation location
.../focus_api_tests/focus_api_tests/test_live_metadata.py::test_live_metadata_smoke",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3eb56d04,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02r9j:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:35 PM
Channels endpoint returns enabled channels with valid schema (Smoke),PZ-13419,41688,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,16/Sep/25 7:33 PM,16/Sep/25 7:33 PM,,,,0,"Description:
Verifies that GET /channels is reachable and returns HTTP 200 with a well-formed JSON array of channel objects. Ensures mandatory fields exist and have correct types and acceptable value ranges. Serves as a smoke test for Focus Server availability and basic contract adherence.

Preconditions:

Focus Server is reachable from the test runner.

API_BASE and (if applicable) Authorization header are configured.

SSL verification setting is aligned with the environment (valid certs or explicitly disabled).

Inputs & Endpoint:

Endpoint: GET ${API_BASE}/channels

Steps

# Send GET /channels.
# Assert HTTP 200.
# Assert response body is a JSON array (length ≥ 0).
# For each item, assert presence and type of:

* channel_id (number, > 0)
* name (string, non-empty)
* enabled (boolean)
* sample_rate_hz (number, > 0)

# (Optional) If any business rules exist (e.g., sample_rate_hz within a supported set), validate them.

Expected Results:

200 OK.

Body is an array of objects with all mandatory fields present and correctly typed.

No unexpected fields that break contract (or they’re handled via tolerant schema, if intended).

Automation location:
panda-backend-api-tests/focus_server_api_load_tests/focus_api_tests/focus_api_tests/test_channels.py::test_channels_smoke",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@25e42a7d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02r9b:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,To Do,16/Sep/25 7:33 PM
APi - 4xx errors do not log stack traces,PZ-13299,41333,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:59 PM,11/Sep/25 1:59 PM,,,,0,"@backend @api @logging
Feature: 4xx errors do not log stack traces
  As an operator
  I want validation errors not to spam error logs
  So that logs remain actionable

  @medium @stability
  Scenario: 422 responses do not include stack traces in server logs
    Given I trigger a 422 from /configure by omitting required fields
    When I fetch recent server logs
    Then no Python Traceback should be present for this request
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@7f261783,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qhz:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:59 PM
API - OpenAPI contract alignment,PZ-13298,41332,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:59 PM,11/Sep/25 1:59 PM,,,,0,"@backend @api @openapi
Feature: OpenAPI contract alignment
  As a QA and SDK generator user
  I want /openapi.json to reflect the real conditional requirements
  So that generated clients and tests are correct

  @medium @contract
  Scenario: /openapi.json documents conditional requirements for view_type=""0""
    When I GET ""/openapi.json""
    Then ConfigureRequest should mark displayTimeAxisDuration and frequencyRange as required for view_type=""0""
    And descriptions should not state ""not applicable"" if the server requires them
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2eaf14d,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qhr:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:59 PM
API - Error body uniformity,PZ-13297,41331,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:58 PM,11/Sep/25 1:58 PM,,,,0,"@backend @api @errors
Feature: Error body uniformity
  As a client
  I want error responses to share a uniform structure
  So that clients can parse them consistently

  @medium @404
  Scenario: /metadata/{bad_id} returns consistent 404 error body
    When I GET ""/metadata/DOES_NOT_EXIST_123""
    Then the response status code should be 404
    And the JSON body should match the agreed error shape (e.g., {""error"":{""code"":""NOT_FOUND"",""message"":""..."",""hint"":""...""}})

  @medium @422
  Scenario: /configure validation errors use uniform JSON shape
    Given an invalid /configure body (missing required fields)
    When I POST it to ""/configure""
    Then the response status code should be 422
    And the JSON body should match the agreed error shape for validation errors
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@3e1d1a1a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qhj:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:58 PM
API - Waterfall behavior with optional fields omitted,PZ-13296,41330,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:54 PM,11/Sep/25 1:54 PM,,,,0,"@backend @api @waterfall
Feature: Waterfall behavior with optional fields omitted
  As a QA
  I want Waterfall (view_type=""2"") to work without irrelevant fields
  So that optional fields do not crash the server

  Background:
    Given the API base URL is ""http://localhost:8500""

  @medium @waterfall
  Scenario: Waterfall without displayTimeAxisDuration / frequencyRange
    Given a JSON body:
      """"""
      {
        ""displayInfo"": {""height"": 600},
        ""channels"": {""min"": <min>, ""max"": <max>},
        ""view_type"": ""2""
      }
      """"""
    When I POST it to ""/configure""
    Then the response should be either 200 or a clear 422 per contract
    And server logs should NOT contain a stack trace

  @medium @waterfall
  Scenario: Overlap/NFFT logic does not assume missing fields
    Given a JSON body without nfftSelection and displayTimeAxisDuration for view_type=""2""
    When I POST it to ""/configure""
    Then the server should not return 500
    And the behavior should follow documented defaults or validation
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@10a8c3fe,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qhb:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:54 PM
API - Time validation uses provided epoch without hidden offsets,PZ-13295,41329,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:53 PM,11/Sep/25 1:53 PM,,,,0,"@backend @api @time
Feature: Time validation uses provided epoch without hidden offsets
  As a user of historical playback
  I want my provided start_time/end_time used as-is
  So that valid ranges are not rejected

  Background:
    Given I have a known recording window where data exists

  @high @time
  Scenario: No hidden +3h offset during recording validation
    Given a valid /configure request body with start_time and end_time matching an existing window
    When I POST it to ""/configure""
    Then the response should not be a 400 ""No recording found"" due to hidden offsets
    And server logs should not show an added time offset applied to validation
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@196d46cc,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qh3:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:53 PM
API - Stream endpoint reachability,PZ-13294,41328,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:51 PM,11/Sep/25 1:51 PM,,,,0,"@backend @api @streaming @reachability
Feature: Stream endpoint reachability
  As a client
  I want the returned stream_url:stream_port to be reachable
  So that the UI does not appear frozen

  Background:
    Given I have a 200 response from /configure and saved stream_url and stream_port

  @high @network
  Scenario: Node endpoint is reachable within SLA
    When I attempt a TCP connection to {stream_url}:{stream_port} every 10 seconds for up to 60 seconds
    Then the TCP connection should succeed within 60 seconds
    And if it does not, the /configure response should not have marked the endpoint as ready
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@29c79676,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qgv:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:51 PM
API - metadata readiness and race handling,PZ-13293,41327,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:50 PM,11/Sep/25 1:50 PM,,,,0,"@backend @api @metadata @race
Feature: /metadata readiness and race handling
  As a client
  I want /metadata/{job_id} to not 404 immediately after /configure
  So that I can implement predictable polling

  Background:
    Given I have posted a valid /configure request and saved job_id as ctx.job_id

  @high @readiness
  Scenario: Immediate GET /metadata/{job_id} should not 404
    When I GET ""/metadata/{ctx.job_id}""
    Then the response status code should be either 200 or a documented pending/status code (e.g., 202)
    And if not 200, the response body should contain a machine-readable status field (e.g., ""status"":""pending"")

  @high @readiness
  Scenario: Poll /metadata/{job_id} until ready within a reasonable window
    When I poll ""/metadata/{ctx.job_id}"" every 2 seconds for up to 60 seconds
    Then I should eventually receive a 200 with valid metadata
    And the metadata should include fields required by the client to connect
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2079b11a,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qgn:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:50 PM
API - Response invariants and consistency,PZ-13292,41326,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:48 PM,11/Sep/25 1:48 PM,,,,0,"@backend @api @response @invariants
Feature: Response invariants and consistency
  As a QA
  I want post-response invariants to hold
  So that clients can rely on stable structure and counts

  Background:
    Given I have successfully created a job via /configure as in the happy path
    And I have saved the JSON response as ctx.configure

  @medium @invariants
  Scenario: Frequencies count, lines_dt positivity, mapping sizes
    Then ctx.configure.frequencies_amount should equal len(ctx.configure.frequencies_list)
    And ctx.configure.lines_dt should be greater than 0
    And ctx.configure.channel_amount should equal the number of keys in ctx.configure.channel_to_stream_index
    And ctx.configure.stream_amount should equal the number of unique values in ctx.configure.channel_to_stream_index
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2595da65,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qgf:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:48 PM
"API - configure validation for MULTICHANNEL (view_type=""0"")",PZ-13291,41325,Test,TO DO,PZ,Prisma,software,Lior Leiba,712020:4978392c-8cde-4a84-92de-946bc188a541,,,Medium,,,,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,11/Sep/25 1:44 PM,11/Sep/25 1:44 PM,,,,0,"@backend @api @configure
Feature: /configure validation for MULTICHANNEL (view_type=""0"")
  As a QA on the Focus Server backend
  I want /configure to validate inputs properly
  So that clients receive 4xx for bad input and never 500

  Background:
    Given the API base URL is ""http://localhost:8500""
    And the server is reachable at ""/channels""
    And I fetch live metadata from ""/live_metadata""
    And I compute a valid channel range using the API response (min >= 100, max = min + 10)
    And I compute nyquist = floor(prr/2) from live metadata

  @high @contract @422
  Scenario: Missing fields must return 422 when view_type=""0""
    Given a JSON body:
      """"""
      {
        ""displayInfo"": {""height"": 600},
        ""channels"": {""min"": <min>, ""max"": <max>},
        ""view_type"": ""0"",
        ""nfftSelection"": 1
      }
      """"""
    When I POST it to ""/configure""
    Then the response status code should be 422
    And the response body should describe the missing fields
    And server logs should NOT contain a stack trace

  @high @contract @422
  Scenario: frequencyRange = null should return 422
    Given a JSON body:
      """"""
      {
        ""displayInfo"": {""height"": 600},
        ""channels"": {""min"": <min>, ""max"": <max>},
        ""view_type"": ""0"",
        ""nfftSelection"": 1,
        ""displayTimeAxisDuration"": 10,
        ""frequencyRange"": null
      }
      """"""
    When I POST it to ""/configure""
    Then the response status code should be 422
    And server logs should NOT contain a stack trace

  @medium @contract @types @happy_path
  Scenario: Valid request returns 200 and correct JSON types
    Given a JSON body:
      """"""
      {
        ""displayInfo"": {""height"": 600},
        ""channels"": {""min"": <min>, ""max"": <max>},
        ""view_type"": ""0"",
        ""nfftSelection"": 1,
        ""displayTimeAxisDuration"": 10,
        ""frequencyRange"": {""min"": 5, ""max"": <nyquist>},
        ""start_time"": <start_epoch>,
        ""end_time"": <end_epoch>
      }
      """"""
    When I POST it to ""/configure""
    Then the response status code should be 200
    And the field ""stream_port"" should be an integer
    And the field ""frequencies_list"" should be an array (it may be empty)
    And the field ""view_type"" should be serialized consistently (no Python enum repr)
    And the response JSON should match the OpenAPI schema types

  Examples:
    | start_epoch                  | end_epoch                  |
    | <known_start_epoch> | <known_end_epoch> |
",,Roy Avrahami,712020:f3895741-cf6d-444c-92af-d001de870705,,,,,,,,,,,,,,,,com.atlassian.servicedesk.plugins.approvals.internal.customfield.ApprovalsCFValue@2290e9ab,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0|i02qg7:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,d46356e5-3aca-4ffb-88e2-b7ebe90ad465,Edge - Linux Panda,,,,,,,,,,,,,,,,To Do,11/Sep/25 1:44 PM
