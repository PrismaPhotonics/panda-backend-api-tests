# ××“×¨×™×š ××¤×•×¨×˜ ×œ×˜×¡×˜×™× - ×”×›× ×” ××•×©×œ××ª ×œ×¤×’×™×©×”
## ×‘×“×™×§×•×ª Infrastructure - × ×™×ª×•×— ××§×™×£ ×•××“×¨×™×š ×™×™×©×•×

---

## ğŸ“‹ ×ª×•×›×Ÿ ×¢× ×™×™× ×™×

1. [××‘×•× - ×¡×§×™×¨×” ×›×œ×œ×™×ª](#××‘×•×)
2. [×˜×¡×˜ 1: SSH Access to Production Servers](#test-1-ssh)
3. [×˜×¡×˜ 2: Kubernetes Cluster Connection](#test-2-kubernetes)
4. [×˜×¡×˜ 3: MongoDB Direct Connection](#test-3-mongodb-connection)
5. [×˜×¡×˜ 4: MongoDB Quick Response Time](#test-4-mongodb-performance)
6. [×˜×¡×˜ 5: MongoDB via ConfigManager](#test-5-mongodb-config)
7. [×˜×¡×˜ 6: MongoDB Direct TCP Authentication](#test-6-mongodb-tcp)
8. [×¡×™×›×•× ×•××˜×¨×™×¦×ª ×”×©×•×•××”](#summary)
9. [×©××œ×•×ª ×¦×¤×•×™×•×ª ×•×ª×©×•×‘×•×ª](#qa)

---

<a name="××‘×•×"></a>
## ğŸ¯ ××‘×•× - ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××” ×”×§×‘×•×¦×” ×”×–××ª ×©×œ ×˜×¡×˜×™×?
**×‘×“×™×§×•×ª Infrastructure (×ª×©×ª×™×ª)** - ×˜×¡×˜×™× ×©×‘×•×“×§×™× ××ª ×©×›×‘×ª ×”×ª×©×ª×™×ª ×©×¢×œ×™×” ×¨×¥ Focus Server.

### ××“×•×¢ ×—×©×•×‘×™× ×˜×¡×˜×™× ××œ×”?
Focus Server **×œ× ×¢×•×‘×“ ×œ×‘×“**. ×”×•× ×ª×œ×•×™ ×‘:
- **MongoDB** - ×œ××—×¡×•×Ÿ recordings, metadata, tasks
- **Kubernetes** - ×œ× ×™×”×•×œ pods ×•-orchestration
- **SSH** - ×œ×’×™×©×” ×œ-servers ×œ×¦×•×¨×›×™ troubleshooting
- **Network connectivity** - ×§×™×©×•×¨×™×•×ª ×¨×©×ª

**×× ×”×ª×©×ª×™×ª × ×›×©×œ×ª - ×’× ××¤×œ×™×§×¦×™×” ××•×©×œ××ª ×œ× ×ª×¢×‘×•×“!**

### ××˜×¨×ª ×”×‘×“×™×§×•×ª:
1. âœ… **Availability** - ×ª×©×ª×™×ª ×–××™× ×” ×•× ×’×™×©×”
2. âœ… **Health Check** - ×›×œ ×”×§×•××¤×•× × ×˜×•×ª ×‘×¨×™××•×ª
3. âœ… **Performance** - ×ª×©×ª×™×ª ×¢×•× ×” ××”×¨ ××¡×¤×™×§
4. âœ… **Configuration** - ×§×•× ×¤×™×’×•×¨×¦×™×•×ª × ×˜×¢× ×•×ª × ×›×•×Ÿ
5. âœ… **Diagnostic Readiness** - ××¤×©×¨ ×œ×¢×©×•×ª troubleshooting

---

<a name="test-1-ssh"></a>
## ğŸ” ×˜×¡×˜ 1: SSH Access to Production Servers

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13900 |
| **Priority** | High |
| **Component** | SSH Infrastructure |
| **Status** | Automated âœ… |
| **Test File** | `tests/integration/infrastructure/test_external_connectivity.py` |
| **Test Function** | `test_ssh_connection` |
| **Lines** | 304-364 |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ××××ª ×©×™×© **×’×™×©×ª SSH ×ª×§×™× ×”** ×œ×©×¨×ª×™ production ×“×¨×š **jump host** ×œ×¦×•×¨×›×™:
- Troubleshooting (×¤×ª×¨×•×Ÿ ×‘×¢×™×•×ª)
- Maintenance (×ª×—×–×•×§×”)
- Log access (×’×™×©×” ×œ×œ×•×’×™×)
- Manual intervention (×”×ª×¢×¨×‘×•×ª ×™×“× ×™×ª)
- Running k9s / kubectl (× ×™×”×•×œ Kubernetes)

**×œ××” ×–×” ×§×¨×™×˜×™?**
×›××©×¨ ××ª×¨×—×©×ª ×ª×§×œ×” ×‘-production, ×”×“×¨×š ×”×™×—×™×“×” ×œ××‘×—×Ÿ ×•×œ×ª×§×Ÿ ×”×™× ×“×¨×š SSH!
- ×‘×œ×™ SSH â†’ ×œ× ×™×›×•×œ ×œ×¨××•×ª ×œ×•×’×™×
- ×‘×œ×™ SSH â†’ ×œ× ×™×›×•×œ ×œ×”×¨×™×¥ kubectl
- ×‘×œ×™ SSH â†’ ×œ× ×™×›×•×œ ×œ×¢×©×•×ª debugging

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×? (×©×œ×‘ ××—×¨ ×©×œ×‘)

#### **Pre-Conditions (×ª× ××™× ××•×§×“××™×):**
1. Jump host ×¨×¥ ×‘-`10.10.100.3`
2. Target host ×¨×¥ ×‘-`10.10.100.113`
3. SSH keys ××• credentials ×–××™× ×™×
4. Network routing ×××¤×©×¨ SSH connections
5. Firewall rules ×××¤×©×¨×™× SSH traffic

#### **Test Flow (×–×¨×™××ª ×”×˜×¡×˜):**

```
Test Client (××—×©×‘ ×”×‘×“×™×§×”)
    â†“
    SSH â†’ Jump Host (10.10.100.3, user: root)
    â†“
    Execute commands: hostname, whoami, uptime
    â†“
    SSH â†’ Target Host (10.10.100.113, user: prisma)
    â†“
    Execute commands: kubectl version, k9s version
    â†“
    âœ… SUCCESS - all connections work
```

#### **×¦×¢×“×™ ×”×˜×¡×˜ (15 Steps):**

| # | Action | Expected Result |
|---|--------|----------------|
| 1 | Import paramiko | Library imported successfully |
| 2 | Create SSH client | `ssh = paramiko.SSHClient()` |
| 3 | Set host key policy | `AutoAddPolicy()` set |
| 4 | Connect to jump host | Connection to 10.10.100.3 established |
| 5 | Execute `hostname` | Hostname returned successfully |
| 6 | Read stdout | Output captured |
| 7 | Check stderr | No errors |
| 8 | Execute `whoami` | Returns `root` |
| 9 | Execute `uptime` | System uptime returned |
| 10 | Close jump connection | Connection closed cleanly |
| 11 | Connect to target host | Connection to 10.10.100.113 established |
| 12 | Execute `kubectl version` | kubectl installed and working |
| 13 | Execute `k9s version` | k9s installed |
| 14 | Optional: List pods | `kubectl get pods -n panda` works |
| 15 | Close target connection | Connection closed |

#### **Expected Results (×ª×•×¦××•×ª ××¦×•×¤×•×ª):**
- âœ… Jump host (10.10.100.3) × ×’×™×© ×“×¨×š SSH
- âœ… Target host (10.10.100.113) × ×’×™×© ×“×¨×š SSH
- âœ… Commands ××ª×‘×¦×¢×•×ª ×‘×”×¦×œ×—×” ×‘×©× ×™ ×”-hosts
- âœ… `kubectl` ×–××™×Ÿ ×‘-target host
- âœ… `k9s` ×–××™×Ÿ ×‘-target host
- âœ… ××™×Ÿ authentication failures
- âœ… ××™×Ÿ network timeouts
- âœ… Connection latency < 2 seconds

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

#### **Architecture Approach:**

**×’×™×©×” 1: Paramiko (××•××œ×¥)**
```
Pros:
âœ… Pure Python SSH client
âœ… ×§×œ ×œ×©×™××•×©
âœ… ×ª×•××š ×‘×›×œ features ×©×œ SSH
âœ… Exception handling ×˜×•×‘

Cons:
âš ï¸ ×¦×¨×™×š credentials ××• SSH keys
âš ï¸ ×¦×¨×™×š ×œ×˜×¤×œ ×‘×”×–×× ×•×ª certificates
```

**×’×™×©×” 2: Subprocess + ssh command**
```
Pros:
âœ… ××©×ª××© ×‘-SSH native ×©×œ OS
âœ… ×¤×©×•×˜ ×××•×“

Cons:
âš ï¸ ×œ× cross-platform
âš ï¸ ×§×©×” ×™×•×ª×¨ ×œ×˜×¤×œ ×‘×©×’×™××•×ª
âš ï¸ ×ª×œ×•×™ ×‘-SSH configuration ××§×•××™×ª
```

**×”××œ×¦×”: Paramiko** - ×™×•×ª×¨ reliable ×•-cross-platform

---

#### **Implementation Pattern:**

```python
# File: tests/integration/infrastructure/test_external_connectivity.py

import paramiko
import pytest
from config.config_manager import ConfigManager

class TestExternalServicesConnectivity:
    """
    Test suite for external infrastructure connectivity validation.
    Validates SSH access to production servers.
    """
    
    @pytest.mark.ssh
    @pytest.mark.infrastructure
    @pytest.mark.critical
    def test_ssh_connection(self, config_manager):
        """
        Test SSH connectivity to jump host and target host.
        
        Purpose:
        - Validate SSH access for troubleshooting
        - Ensure kubectl and k9s are available
        - Verify network connectivity
        
        Steps:
        1. Connect to jump host
        2. Execute basic commands
        3. Connect to target host  
        4. Verify kubectl/k9s availability
        
        Expected:
        - All connections successful
        - Commands execute without errors
        - Latency < 2 seconds
        """
        
        # 1. Get SSH configuration
        ssh_config = config_manager.get_ssh_config()
        jump_host = ssh_config['jump_host']
        target_host = ssh_config['target_host']
        
        # 2. Test Jump Host Connection
        jump_client = self._create_ssh_client()
        try:
            # Connect with timeout
            jump_client.connect(
                hostname=jump_host['ip'],  # 10.10.100.3
                username=jump_host['username'],  # root
                password=jump_host['password'],
                timeout=5
            )
            
            # Execute test commands
            commands = ['hostname', 'whoami', 'uptime']
            for cmd in commands:
                stdin, stdout, stderr = jump_client.exec_command(cmd)
                output = stdout.read().decode().strip()
                error = stderr.read().decode().strip()
                
                assert output, f"Command '{cmd}' returned empty output"
                assert not error, f"Command '{cmd}' returned error: {error}"
                
        finally:
            jump_client.close()
        
        # 3. Test Target Host Connection
        target_client = self._create_ssh_client()
        try:
            target_client.connect(
                hostname=target_host['ip'],  # 10.10.100.113
                username=target_host['username'],  # prisma
                password=target_host['password'],
                timeout=5
            )
            
            # Verify kubectl availability
            stdin, stdout, stderr = target_client.exec_command('kubectl version --client')
            kubectl_output = stdout.read().decode()
            assert 'Client Version' in kubectl_output, "kubectl not available"
            
            # Verify k9s availability
            stdin, stdout, stderr = target_client.exec_command('k9s version')
            k9s_output = stdout.read().decode()
            assert 'Version' in k9s_output, "k9s not available"
            
        finally:
            target_client.close()
    
    def _create_ssh_client(self):
        """Helper method to create SSH client with proper settings."""
        client = paramiko.SSHClient()
        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        return client
```

#### **Dependencies:**
```python
# requirements.txt
paramiko>=3.0.0
pytest>=7.0.0
```

#### **Configuration (environments.yaml):**
```yaml
new_production:
  ssh:
    jump_host:
      ip: "10.10.100.3"
      username: "root"
      password: "***"  # From secrets
    target_host:
      ip: "10.10.100.113"
      username: "prisma"
      password: "***"  # From secrets
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª ×œ×˜×¡×˜ ×–×” + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ×œ××” ×¦×¨×™×š Jump Host? ×œ××” ×œ× ×™×©×¨ ×œ-Target Host?**
**×ª×©×•×‘×”:**
```
Security best practice!
- Production servers ×œ× ×—×©×•×¤×™× ×™×©×™×¨×•×ª ×œ××™× ×˜×¨× ×˜
- Jump host ×”×•× bastion/gateway ×××•×‘×˜×—
- ×××¤×©×¨ audit trail (×œ×•×’ ×©×œ ××™ × ×›× ×¡)
- ××§×œ ×¢×œ × ×™×”×•×œ firewall rules
```

#### **×©××œ×” 2: ××” ×§×•×¨×” ×× SSH × ×›×©×œ?**
**×ª×©×•×‘×”:**
```
×ª×¨×—×™×©×™× ××¤×©×¨×™×™×:
1. Network issue â†’ ×œ× ××’×™×¢×™× ×œ-jump host
2. Authentication failure â†’ credentials ×©×’×•×™×™×
3. Firewall blocking â†’ port 22 ×—×¡×•×
4. Target host down â†’ server ×œ× ×¨×¥

Action:
- ×‘×“×•×§ connectivity (ping)
- ×‘×“×•×§ credentials
- ×‘×“×•×§ firewall rules
- ×‘×“×•×§ logs ×‘-jump host
```

#### **×©××œ×” 3: ××™×š ××˜×¤×œ×™× ×‘-SSH keys ×‘××§×•× passwords?**
**×ª×©×•×‘×”:**
```python
# Instead of password, use key_filename:
client.connect(
    hostname=host_ip,
    username=username,
    key_filename='/path/to/private_key',
    timeout=5
)

# Or use SSH agent:
import paramiko.agent
agent = paramiko.agent.Agent()
agent_keys = agent.get_keys()
```

#### **×©××œ×” 4: ××” ×”-timeout ××•×¤×˜×™××œ×™?**
**×ª×©×•×‘×”:**
```
Recommended: 5-10 seconds
- ×™×•×ª×¨ ××“×™ ×§×¦×¨ â†’ false negatives (network hiccup)
- ×™×•×ª×¨ ××“×™ ××¨×•×š â†’ ×˜×¡×˜ ××™×˜×™

×‘××•×˜×•××¦×™×”: 5 seconds
×‘×× ×™×˜×•×¨×™× ×’: 10 seconds
```

#### **×©××œ×” 5: ××™×š ×‘×•×“×§×™× ×©×”-SSH session ×œ× × ×©××¨ ×ª×§×•×¢?**
**×ª×©×•×‘×”:**
```python
# Always use try-finally or context manager:
try:
    client.connect(...)
    # Do work
finally:
    client.close()  # Always close!

# Or better - context manager:
with paramiko.SSHClient() as client:
    client.connect(...)
    # Work here
# Auto-closes
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Metric | Threshold | ×¡×™×‘×” |
|--------|-----------|------|
| Connection latency | < 2s | ×—×™×•× ×™ ×œ-troubleshooting ××”×™×¨ |
| Authentication | 100% success | ××™ ××¤×©×¨ ×œ××‘×“ ×’×™×©×” |
| Command execution | 100% success | ×¦×¨×™×š ×œ×‘×¦×¢ ×¤×§×•×“×•×ª |
| kubectl availability | 100% | ×—×™×•× ×™ ×œ× ×™×”×•×œ K8s |

---

<a name="test-2-kubernetes"></a>
## â˜¸ï¸ ×˜×¡×˜ 2: Kubernetes Cluster Connection and Pod Health Check

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13899 |
| **Priority** | High |
| **Component** | Kubernetes Infrastructure |
| **Status** | Automated âœ… |
| **Test File** | `tests/integration/infrastructure/test_external_connectivity.py` |
| **Test Function** | `test_kubernetes_connection` |
| **Lines** | 172-219 |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ××××ª:
1. **×§×™×©×•×¨×™×•×ª ×œ-Kubernetes cluster** API server
2. **×’×™×œ×•×™ Focus Server pods** ×•×”×¡×˜×˜×•×¡ ×©×œ×”×
3. **×‘×¨×™××•×ª (health) ×©×œ pods** - Running ×•-Ready
4. **×–××™× ×•×ª services** ×•× ×›×•× ×•×ª ×”-configuration

**×œ××” ×–×” ×§×¨×™×˜×™?**
Focus Server ×¨×¥ ×‘-Kubernetes pods. ××:
- K8s cluster ×œ× × ×’×™×© â†’ ×œ× ×™×•×“×¢ ××¦×‘ ×”××¢×¨×›×ª
- Pods ×œ× ×¨×¦×™× â†’ ×”××¢×¨×›×ª ×œ× ×¢×•×‘×“×ª
- Pods restart ×”×¨×‘×” â†’ ×™×© ×‘×¢×™×” ×™×¦×™×‘×•×ª
- Services ×œ× ×§×™×™××™× â†’ ×œ× ×™×›×•×œ ×œ×”×’×™×¢ ×œ-API

**Operational Visibility** - ×—×™×™×‘×™× ×œ×“×¢×ª ××” ×§×•×¨×” ×‘-production!

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Pre-Conditions:**
1. Kubernetes cluster ×¨×¥
2. API Server × ×’×™×© ×‘-`https://10.10.100.102:6443`
3. kubeconfig ××• service account credentials ×–××™× ×™×
4. Namespace `panda` ×§×™×™×
5. Focus Server deployed ×‘-namespace
6. Network ×××¤×©×¨ ×—×™×‘×•×¨ ×œ-K8s API

#### **Test Data:**

**Cluster Details:**
```yaml
API Server: https://10.10.100.102:6443
Namespace: panda
Expected Deployment: panda-panda-focus-server
Expected Service: panda-panda-focus-server.panda:5000
```

**Expected Pods:**
```
- Pattern: panda-panda-focus-server-*
- Min count: 1
- Status: Running
- Ready: True
```

**Expected Services:**
```
Name: panda-panda-focus-server
Type: ClusterIP
ClusterIP: 10.43.103.101
Port: 5000
```

#### **×¦×¢×“×™ ×”×˜×¡×˜ (16 Steps):**

| # | Action | Expected Result |
|---|--------|----------------|
| 1 | Import kubernetes library | `from kubernetes import client, config` |
| 2 | Load kubeconfig | Config loaded successfully |
| 3 | Create API client | `v1 = client.CoreV1Api()` |
| 4 | Get cluster version | Version info (e.g., v1.25.x) |
| 5 | List namespaces | Namespace list returned |
| 6 | Verify `panda` namespace | `'panda' in namespaces` = True |
| 7 | List pods in `panda` | Pod list returned |
| 8 | Filter Focus Server pods | At least 1 pod found |
| 9 | Check pod status | Status = `Running` |
| 10 | Check pod readiness | `ready` condition = True |
| 11 | Check restart count | Restart count < 5 |
| 12 | List services | Service list returned |
| 13 | Verify service exists | `panda-panda-focus-server` found |
| 14 | Verify ClusterIP | ClusterIP = `10.43.103.101` |
| 15 | Verify port | Port = `5000` |
| 16 | Get pod logs (optional) | Last 10 lines accessible |

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

#### **Architecture Approach:**

**×’×™×©×” 1: kubernetes-python library (××•××œ×¥)**
```
Pros:
âœ… Official K8s Python client
âœ… ×ª×•××š ×‘×›×œ K8s APIs
âœ… Type-safe ×•×˜×•×‘ documented
âœ… ××•×˜×•××˜×™ load kubeconfig

Cons:
âš ï¸ ×¦×¨×™×š kubeconfig ××•×›×Ÿ
âš ï¸ ×§×¦×ª verbose
```

**×’×™×©×” 2: kubectl subprocess**
```
Pros:
âœ… ×¤×©×•×˜ ×××•×“
âœ… ××©×ª××© ×‘-kubectl native

Cons:
âš ï¸ ×¦×¨×™×š kubectl ××•×ª×§×Ÿ
âš ï¸ ×§×©×” ×œ×¤×¨×¡×¨ output
âš ï¸ ×œ× type-safe
```

**×’×™×©×” 3: REST API ×™×©×™×¨**
```
Pros:
âœ… ××œ× ×©×œ×™×˜×”

Cons:
âš ï¸ ×¦×¨×™×š ×œ×˜×¤×œ ×‘-authentication
âš ï¸ ×”×¨×‘×” boilerplate
```

**×”××œ×¦×”: kubernetes-python** - ×”×›×™ robust ×•-production-ready

---

#### **Implementation Pattern:**

```python
# File: tests/integration/infrastructure/test_external_connectivity.py

from kubernetes import client, config
from kubernetes.client.rest import ApiException
import pytest

class TestExternalServicesConnectivity:
    """
    Test suite for Kubernetes cluster connectivity and pod health validation.
    """
    
    @pytest.mark.kubernetes
    @pytest.mark.infrastructure
    @pytest.mark.high_priority
    def test_kubernetes_connection(self, config_manager):
        """
        Test Kubernetes cluster connection and Focus Server pod health.
        
        Purpose:
        - Validate K8s cluster accessibility
        - Verify Focus Server pods are running
        - Check pod health and readiness
        - Verify services are configured correctly
        
        Steps:
        1. Load kubeconfig and create API client
        2. Verify cluster connection (get version)
        3. Verify namespace exists
        4. List and validate Focus Server pods
        5. Check pod status and health
        6. Verify service configuration
        
        Expected:
        - Cluster accessible
        - At least 1 Focus Server pod running
        - All pods Ready
        - Service configured correctly
        """
        
        # 1. Load Kubernetes configuration
        try:
            # Try in-cluster config first (if running in K8s)
            config.load_incluster_config()
        except:
            # Fall back to kubeconfig
            config.load_kube_config()
        
        # 2. Create API client
        v1 = client.CoreV1Api()
        
        # 3. Test cluster connection - get version
        try:
            version_info = v1.get_api_resources()
            logger.info(f"âœ… K8s cluster accessible")
        except ApiException as e:
            pytest.fail(f"Cannot connect to K8s cluster: {e}")
        
        # 4. Verify namespace 'panda' exists
        namespaces = v1.list_namespace()
        namespace_names = [ns.metadata.name for ns in namespaces.items]
        assert 'panda' in namespace_names, "Namespace 'panda' not found"
        
        # 5. List pods in 'panda' namespace
        pods = v1.list_namespaced_pod(namespace='panda')
        
        # 6. Filter Focus Server pods
        focus_server_pods = [
            pod for pod in pods.items
            if 'panda-panda-focus-server' in pod.metadata.name
        ]
        
        assert len(focus_server_pods) > 0, "No Focus Server pods found"
        logger.info(f"Found {len(focus_server_pods)} Focus Server pods")
        
        # 7. Check each pod's health
        for pod in focus_server_pods:
            pod_name = pod.metadata.name
            
            # Check status
            phase = pod.status.phase
            assert phase == 'Running', f"Pod {pod_name} not Running (status: {phase})"
            
            # Check readiness
            conditions = pod.status.conditions
            ready_condition = next(
                (c for c in conditions if c.type == 'Ready'),
                None
            )
            assert ready_condition is not None, f"Pod {pod_name} has no Ready condition"
            assert ready_condition.status == 'True', f"Pod {pod_name} not Ready"
            
            # Check restart count
            restart_count = sum(
                container.restart_count
                for container in pod.status.container_statuses
            )
            assert restart_count < 5, f"Pod {pod_name} has high restart count: {restart_count}"
            
            logger.info(f"âœ… Pod {pod_name}: Running, Ready, Restarts: {restart_count}")
        
        # 8. Verify service exists
        services = v1.list_namespaced_service(namespace='panda')
        service_names = [svc.metadata.name for svc in services.items]
        
        assert 'panda-panda-focus-server' in service_names, "Focus Server service not found"
        
        # 9. Get service details
        service = v1.read_namespaced_service(
            name='panda-panda-focus-server',
            namespace='panda'
        )
        
        # Verify ClusterIP
        cluster_ip = service.spec.cluster_ip
        assert cluster_ip == '10.43.103.101', f"Unexpected ClusterIP: {cluster_ip}"
        
        # Verify port
        ports = service.spec.ports
        assert len(ports) > 0, "Service has no ports"
        assert ports[0].port == 5000, f"Unexpected port: {ports[0].port}"
        
        logger.info(f"âœ… Service configured correctly: {cluster_ip}:5000")
```

#### **Dependencies:**
```python
# requirements.txt
kubernetes>=28.0.0
pytest>=7.0.0
```

#### **Kubeconfig Setup:**
```bash
# Option 1: Use existing kubeconfig
export KUBECONFIG=~/.kube/config

# Option 2: Get kubeconfig from cluster
scp prisma@10.10.100.113:~/.kube/config ~/.kube/config

# Option 3: Service account (for in-cluster)
# Automatically loaded when running inside K8s
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ××” ×–×” Pod? ××” ×–×” Deployment?**
**×ª×©×•×‘×”:**
```
Pod:
- ×™×—×™×“×ª ×¨×™×¦×” ×‘×¡×™×¡×™×ª ×‘-Kubernetes
- ××›×•×œ×” ××—×ª ××• ×™×•×ª×¨ (containers)
- ×›×ª×•×‘×ª IP ××©×œ×”
- lifecycle: pending â†’ running â†’ succeeded/failed

Deployment:
- ×× ×”×œ replicas ×©×œ pods
- ××’×“×™×¨ ×›××” pods ×¦×¨×™×›×™× ×œ×¨×•×¥
- ××‘×˜×™×— ×©×”× ×¨×¦×™× (self-healing)
- ×××¤×©×¨ rolling updates

×“×•×’××”:
Deployment: panda-panda-focus-server (××’×“×™×¨ 2 replicas)
    â†“
Pod 1: panda-panda-focus-server-abc123
Pod 2: panda-panda-focus-server-def456
```

#### **×©××œ×” 2: ××” ×–×” Ready condition? ×œ××” ×–×” ×—×©×•×‘?**
**×ª×©×•×‘×”:**
```
Ready Condition:
- ×¡×˜×˜×•×¡ ×©××¨××” ×©-pod ××•×›×Ÿ ×œ×§×‘×œ traffic
- × ×§×‘×¢ ×¢"×™ readiness probes

Pod ×™×›×•×œ ×œ×”×™×•×ª Running ××‘×œ ×œ× Ready!
- Running = containers ×¨×¦×™×
- Ready = ××¤×œ×™×§×¦×™×” ××•×›× ×” ×œ×¢×‘×•×“

×“×•×’××”:
Pod Status: Running
Ready Condition: False
  â†“
×”××©××¢×•×ª: Pod ×¨×¥ ××‘×œ ××¤×œ×™×§×¦×™×” ×¢×“×™×™×Ÿ loading
Service ×œ× ×™×©×œ×— traffic ×œ-pod ×”×–×”!
```

#### **×©××œ×” 3: ××” Restart Count ×¡×‘×™×¨?**
**×ª×©×•×‘×”:**
```
Restart Count Thresholds:
0-2: âœ… ××¦×•×™×Ÿ (restarts ×¡×¤×•×¨×“×™×™×)
3-5: âš ï¸ ×¡×‘×™×¨ (××•×œ×™ deployment issues)
6-10: âŒ ×‘×¢×™×™×ª×™ (instability)
>10: âŒ ×§×¨×™×˜×™ (crash loop)

×¡×™×‘×•×ª ×œ-restarts:
- OOMKilled (out of memory)
- Application crash
- Failed liveness probe
- Node issues

Action:
kubectl logs <pod> --previous  # ×œ×¨××•×ª ××” ×§×¨×”
```

#### **×©××œ×” 4: ××” ×”×”×‘×“×œ ×‘×™×Ÿ ClusterIP ×œ×‘×™×Ÿ LoadBalancer?**
**×ª×©×•×‘×”:**
```
Service Types:

ClusterIP (internal):
- ×›×ª×•×‘×ª IP ×¤× ×™××™×ª ×‘×ª×•×š cluster
- × ×’×™×© ×¨×§ ××ª×•×š K8s
- ×©×™××•×©: ×ª×§×©×•×¨×ª ×¤× ×™××™×ª ×‘×™×Ÿ services
- ×“×•×’××”: 10.43.103.101:5000

LoadBalancer (external):
- ×—×•×©×£ service ×œ×¢×•×œ× ×”×—×™×¦×•×Ÿ
- ××§×‘×œ IP ×—×™×¦×•× ×™
- ×©×™××•×©: ×’×™×©×” ××‘×—×•×¥
- ×“×•×’××”: 10.10.100.100:443

×‘××§×¨×” ×©×œ× ×•:
- ClusterIP: ×ª×§×©×•×¨×ª ×¤× ×™××™×ª
- LoadBalancer: ×’×™×©×” external ×œ-Focus Server
```

#### **×©××œ×” 5: ××™×š ×‘×•×“×§×™× pod logs?**
**×ª×©×•×‘×”:**
```python
# Get last 10 lines of logs:
logs = v1.read_namespaced_pod_log(
    name=pod_name,
    namespace='panda',
    tail_lines=10
)
print(logs)

# Get logs from previous crashed container:
logs = v1.read_namespaced_pod_log(
    name=pod_name,
    namespace='panda',
    previous=True
)

# Stream logs in real-time:
from kubernetes.watch import Watch
w = Watch()
for line in w.stream(v1.read_namespaced_pod_log, 
                      name=pod_name, 
                      namespace='panda'):
    print(line)
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Metric | Threshold | ×¡×™×‘×” |
|--------|-----------|------|
| Cluster accessible | 100% | ×—×•×‘×” ×œ× ×™×”×•×œ |
| Pods running | â‰¥1 | ×—×•×‘×” ×œ×©×¨×•×ª |
| Pods ready | 100% | ×—×•×‘×” ×œ-traffic |
| Restart count | <5 | ×¡×™××Ÿ ×™×¦×™×‘×•×ª |
| Service exists | 100% | ×—×•×‘×” ×œ-routing |

---

<a name="test-3-mongodb-connection"></a>
## ğŸƒ ×˜×¡×˜ 3: MongoDB Direct Connection and Health Check

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13898 |
| **Priority** | High |
| **Component** | MongoDB Infrastructure |
| **Status** | Automated âœ… |
| **Test File** | `tests/integration/infrastructure/test_external_connectivity.py` |
| **Test Function** | `test_mongodb_connection` |
| **Lines** | 68-125 |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ××××ª **×§×™×©×•×¨×™×•×ª ×™×©×™×¨×” ×œ-MongoDB** ×•**×‘×¨×™××•×ª ××¡×“ ×”× ×ª×•× ×™×**:
1. TCP connection ×œ-MongoDB server
2. Authentication (××™××•×ª)
3. Ping command (×‘×“×™×§×ª ×ª×§×©×•×¨×ª)
4. Database existence (×§×™×•× ××¡×“ × ×ª×•× ×™×)
5. Collections existence (×§×™×•× ×˜×‘×œ××•×ª)
6. Basic queries (×©××™×œ×ª×•×ª ×‘×¡×™×¡×™×•×ª)

**×œ××” ×–×” ×§×¨×™×˜×™?**
MongoDB ×”×•× ×”**××§×•×¨ ×”×××ª** (source of truth) ×‘-Focus Server:
- ××—×¡×•×Ÿ recordings metadata
- ××—×¡×•×Ÿ tasks ×•-configurations
- Query capabilities ×œhistoric playback

**Isolation Test** - ×‘×•×“×§×™× MongoDB ×œ×‘×“, ××‘×•×“×“ ×-Focus Server.
×× MongoDB × ×›×©×œ â†’ Focus Server ×œ× ×™×¢×‘×•×“!

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Pre-Conditions:**
1. MongoDB deployed ×•×¨×¥
2. MongoDB LoadBalancer service ×—×©×•×£ ×‘-`10.10.100.108:27017`
3. Credentials ×–××™× ×™×: `username=prisma, password=prisma`
4. Database `prisma` ×§×™×™×
5. Network routing ×××¤×©×¨ ×—×™×‘×•×¨ ×œ-MongoDB

#### **Test Data:**

**Connection String:**
```
mongodb://prisma:prisma@10.10.100.108:27017/?authSource=prisma
```

**Expected Database:** `prisma`

**Expected Collections:**
- `recordings` - ×¨×©×™××ª recordings ×–××™× ×™×
- `tasks` - ××™×“×¢ ×¢×œ tasks ×¨×™×¦×”
- `metadata` - metadata ×©×œ recordings
- `node4` - × ×ª×•× ×™× ×”×™×¡×˜×•×¨×™×™×

#### **×¦×¢×“×™ ×”×˜×¡×˜ (14 Steps):**

| # | Action | Expected Result |
|---|--------|----------------|
| 1 | Import pymongo | Library imported |
| 2 | Create connection string | Connection string built |
| 3 | TCP connection | Socket connection established |
| 4 | Create MongoClient | Client object created |
| 5 | Authenticate | Authentication successful |
| 6 | Ping command | `{'ok': 1}` returned |
| 7 | List databases | Database list returned |
| 8 | Verify `prisma` DB | `'prisma' in databases` |
| 9 | Connect to DB | DB object created |
| 10 | List collections | Collection list returned |
| 11 | Verify collections | All 4 collections exist |
| 12 | Simple query | Query executes |
| 13 | Measure latency | Latency < 100ms |
| 14 | Close connection | Connection closed cleanly |

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

#### **Architecture Approach:**

**×’×™×©×” 1: pymongo (××•××œ×¥)**
```
Pros:
âœ… Official MongoDB Python driver
âœ… ××œ× features
âœ… Connection pooling
âœ… Retry logic

Cons:
âš ï¸ ×¦×¨×™×š credentials × ×›×•× ×™×
```

**×’×™×©×” 2: Motor (async)**
```
Pros:
âœ… Async/await support
âœ… ×˜×•×‘ ×œ-high concurrency

Cons:
âš ï¸ ××•×¨×›×‘ ×™×•×ª×¨
âš ï¸ ×œ× × ×—×•×¥ ×œ×˜×¡×˜×™×
```

**×”××œ×¦×”: pymongo** - ×¤×©×•×˜, robust, synchronous

---

#### **Implementation Pattern:**

```python
# File: tests/integration/infrastructure/test_external_connectivity.py

from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError
import pytest
import time

class TestExternalServicesConnectivity:
    """
    Test suite for MongoDB connectivity and health validation.
    """
    
    @pytest.mark.mongodb
    @pytest.mark.infrastructure
    @pytest.mark.high_priority
    def test_mongodb_connection(self, config_manager):
        """
        Test MongoDB direct connection and health check.
        
        Purpose:
        - Validate TCP connection to MongoDB
        - Verify authentication works
        - Check database and collections exist
        - Validate query capability
        - Measure connection latency
        
        This is an isolation test - MongoDB is tested independently
        from Focus Server to diagnose infrastructure issues.
        
        Steps:
        1. Create MongoDB client
        2. Test connection with ping
        3. Verify database exists
        4. Verify collections exist
        5. Test query execution
        6. Measure latency
        
        Expected:
        - Connection successful
        - Authentication passes
        - Database 'prisma' exists
        - All required collections exist
        - Latency < 100ms
        """
        
        # 1. Get MongoDB configuration
        mongo_config = config_manager.get_database_config()
        
        # 2. Build connection string
        connection_string = (
            f"mongodb://{mongo_config['username']}:{mongo_config['password']}"
            f"@{mongo_config['host']}:{mongo_config['port']}"
            f"/?authSource={mongo_config['auth_source']}"
        )
        
        # 3. Create MongoClient with timeouts
        client = MongoClient(
            connection_string,
            serverSelectionTimeoutMS=10000,  # 10 seconds
            connectTimeoutMS=10000,
            socketTimeoutMS=10000
        )
        
        try:
            # 4. Test connection - ping command
            start_time = time.time()
            ping_result = client.admin.command('ping')
            latency_ms = (time.time() - start_time) * 1000
            
            assert ping_result['ok'] == 1.0, "Ping failed"
            assert latency_ms < 100, f"Ping latency too high: {latency_ms:.2f}ms"
            logger.info(f"âœ… MongoDB ping: {latency_ms:.2f}ms")
            
            # 5. List all databases
            db_list = client.list_database_names()
            assert len(db_list) > 0, "No databases found"
            logger.info(f"Databases found: {db_list}")
            
            # 6. Verify 'prisma' database exists
            assert 'prisma' in db_list, "Database 'prisma' not found"
            
            # 7. Connect to prisma database
            db = client['prisma']
            
            # 8. List all collections
            collections = db.list_collection_names()
            logger.info(f"Collections found: {collections}")
            
            # 9. Verify required collections
            required_collections = ['recordings', 'tasks', 'metadata', 'node4']
            for collection_name in required_collections:
                assert collection_name in collections, \
                    f"Collection '{collection_name}' not found"
            
            logger.info(f"âœ… All {len(required_collections)} required collections exist")
            
            # 10. Test simple query on 'recordings'
            recordings_collection = db['recordings']
            sample_recording = recordings_collection.find_one()
            
            # Note: sample_recording can be None if collection is empty
            # That's OK - we're just testing query capability
            logger.info(f"Query test: {'âœ… Success' if sample_recording is not None else 'âœ… Query works (empty collection)'}")
            
            # 11. Verify collection schemas (optional deep check)
            if sample_recording:
                # Validate expected fields
                expected_fields = ['uuid', 'start_time', 'end_time']
                for field in expected_fields:
                    assert field in sample_recording, \
                        f"Field '{field}' missing in recording document"
            
        except ConnectionFailure as e:
            pytest.fail(f"MongoDB connection failed: {e}")
        except ServerSelectionTimeoutError as e:
            pytest.fail(f"MongoDB server selection timeout: {e}")
        finally:
            # 12. Always close connection
            client.close()
            logger.info("âœ… MongoDB connection closed")
```

#### **Dependencies:**
```python
# requirements.txt
pymongo>=4.0.0
pytest>=7.0.0
```

#### **Configuration (environments.yaml):**
```yaml
new_production:
  mongodb:
    host: "10.10.100.108"
    port: 27017
    username: "prisma"
    password: "prisma"
    database: "prisma"
    auth_source: "prisma"
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ××” ×–×” authSource? ×œ××” ×¦×¨×™×š ××ª ×–×”?**
**×ª×©×•×‘×”:**
```
authSource = ××¡×“ × ×ª×•× ×™× ×©×‘×• × ××¦××•×ª credentials ×œ××©×ª××©

×“×•×’××”:
mongodb://prisma:prisma@host:27017/?authSource=prisma
                                      â†‘
                            Authentication DB

×œ××” ×¦×¨×™×š?
- MongoDB ×××—×¡×Ÿ users ×‘-DB × ×¤×¨×“
- ×‘×“×¨×š ×›×œ×œ: 'admin' (default)
- ×‘××§×¨×” ×©×œ× ×•: 'prisma' (custom)

×‘×œ×™ authSource × ×›×•×Ÿ â†’ Authentication ×™×›×©×œ!
```

#### **×©××œ×” 2: ××” ×”×›×•×•× ×” ×‘-Ping latency < 100ms?**
**×ª×©×•×‘×”:**
```
Ping Latency = ×–××Ÿ ×©×œ×•×§×— ×œ-ping command ×œ×—×–×•×¨

Thresholds:
- <10ms: ××¦×•×™×Ÿ (local network)
- 10-50ms: ×˜×•×‘ (same datacenter)
- 50-100ms: ×¡×‘×™×¨ (acceptable)
- >100ms: ×‘×¢×™×™×ª×™ (network issues)

×‘××§×¨×” ×©×œ× ×•:
- MongoDB ×‘-K8s cluster
- Test client external
- 100ms = threshold ×¡×‘×™×¨

Action ×× latency ×’×‘×•×”:
- Check network
- Check MongoDB load
- Check disk I/O
```

#### **×©××œ×” 3: ××” ×§×•×¨×” ×× Collection ×¨×™×§?**
**×ª×©×•×‘×”:**
```
find_one() ×¢×œ collection ×¨×™×§:
- Returns: None
- ×œ× ×–×•×¨×§ exception!

×–×” ×‘×¡×“×¨ ×œ×˜×¡×˜:
âœ… ×× ×—× ×• ×‘×•×“×§×™× ×©×”-query ×¢×•×‘×“
âœ… ×œ× ×‘×•×“×§×™× ×©×™×© data

×× ×¨×•×¦×™× ×œ×”×‘×˜×™×— data:
assert sample_recording is not None, "Collection is empty"

××‘×œ: ×‘-fresh deployment ××¤×©×¨ collection ×¨×™×§
```

#### **×©××œ×” 4: ××™×š ×‘×•×“×§×™× MongoDB schema?**
**×ª×©×•×‘×”:**
```python
# Sample a document and check fields:
sample = collection.find_one()

if sample:
    # Check required fields
    assert 'uuid' in sample
    assert 'start_time' in sample
    assert 'end_time' in sample
    
    # Check data types
    assert isinstance(sample['uuid'], str)
    assert isinstance(sample['start_time'], (int, float))
    
    # Check value validity
    assert sample['start_time'] < sample['end_time']

# Or use MongoDB schema validation:
db.command({
    "collMod": "recordings",
    "validator": {
        "$jsonSchema": {
            "required": ["uuid", "start_time", "end_time"]
        }
    }
})
```

#### **×©××œ×” 5: ××” ×”×”×‘×“×œ ×‘×™×Ÿ ×”×˜×¡×˜ ×”×–×” ×œ×‘×™×Ÿ Functional Test?**
**×ª×©×•×‘×”:**
```
Infrastructure Test (×”×˜×¡×˜ ×”×–×”):
- ×‘×•×“×§ MongoDB **×œ×‘×“**
- ×‘×™×“×•×“ ×-Focus Server
- Diagnose: ×”×× MongoDB ×‘×¨×™×?
- ×©××œ×”: "×”×× ×”×ª×©×ª×™×ª ×¢×•×‘×“×ª?"

Functional Test:
- ×‘×•×“×§ Focus Server **×¢×** MongoDB
- ××™× ×˜×’×¨×¦×™×” ××œ××”
- Diagnose: ×”×× ×”××¤×œ×™×§×¦×™×” ×¢×•×‘×“×ª?
- ×©××œ×”: "×”×× ×”×¤×™×¦'×¨ ×¢×•×‘×“?"

Example Flow:
1. Infrastructure Test fails â†’ ×‘×¢×™×” ×‘-MongoDB
2. Infrastructure Test passes + Functional Test fails â†’ ×‘×¢×™×” ×‘-Focus Server
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Metric | Threshold | ×¡×™×‘×” |
|--------|-----------|------|
| Connection success | 100% | ×—×•×‘×” ×œ×›×œ ×¤×¢×•×œ×” |
| Ping latency | <100ms | ×—×™×•× ×™ ×œ×‘×™×¦×•×¢×™× |
| Authentication | 100% success | ××‘×˜×—×” ×•×’×™×©×” |
| Collections exist | 100% | ×—×•×‘×” ×œ-queries |

---

<a name="test-4-mongodb-performance"></a>
## âš¡ ×˜×¡×˜ 4: MongoDB Quick Response Time Test (Performance)

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13808 |
| **Priority** | Medium |
| **Component** | MongoDB Performance |
| **Status** | Implemented âœ… |
| **Test File** | `tests/integration/infrastructure/test_basic_connectivity.py` |
| **Test Function** | `test_quick_mongodb_ping` |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ×‘×•×“×§ **×–××Ÿ ×ª×’×•×‘×” (response time)** ×©×œ MongoDB ×ª×—×ª ×ª× ××™ ×¢×•××¡ ×¨×’×™×œ×™×.

**×œ××” ×–×” ×§×¨×™×˜×™?**
MongoDB ××™×˜×™ â†’ Focus Server ××™×˜×™ â†’ ××©×ª××©×™× ×œ× ××¨×•×¦×™×!

Performance criteria:
- **Excellent**: <50ms
- **Acceptable**: 50-100ms
- **Problem**: >100ms

**×–×” ×œ× ×‘×“×™×§×ª functionality - ×–×” ×‘×“×™×§×ª ×‘×™×¦×•×¢×™×!**

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Test Data:**
```json
{
  "max_response_time_ms": 100,
  "acceptable_response_time_ms": 50
}
```

#### **×¦×¢×“×™ ×”×˜×¡×˜ (7 Steps):**

| # | Action | Data | Expected |
|---|--------|------|----------|
| 1 | Connect to MongoDB | Connection string | Connected |
| 2 | Record start time | `time.time()` | Timestamp captured |
| 3 | Send ping command | `client.admin.command('ping')` | Response received |
| 4 | Record end time | `time.time()` | Timestamp captured |
| 5 | Calculate latency | `(end - start) * 1000` | Latency in ms |
| 6 | Verify threshold | `latency < 100ms` | Pass/Fail |
| 7 | Log result | Latency value | Logged |

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

```python
# File: tests/integration/infrastructure/test_basic_connectivity.py

import time
import pytest
from pymongo import MongoClient

class TestMongoDBPerformance:
    """
    Performance tests for MongoDB infrastructure.
    """
    
    @pytest.mark.mongodb
    @pytest.mark.performance
    @pytest.mark.infrastructure
    def test_quick_mongodb_ping(self, config_manager):
        """
        Test MongoDB ping response time under normal load.
        
        Purpose:
        - Ensure MongoDB responds quickly
        - Detect performance degradation
        - Monitor database health
        
        Performance Criteria:
        - Excellent: <50ms
        - Acceptable: 50-100ms
        - Unacceptable: >100ms
        
        Expected:
        - Ping latency < 100ms
        - Consistent performance
        """
        
        # 1. Get MongoDB config
        mongo_config = config_manager.get_database_config()
        
        # 2. Create client
        client = MongoClient(
            host=mongo_config['host'],
            port=mongo_config['port'],
            username=mongo_config['username'],
            password=mongo_config['password'],
            authSource=mongo_config.get('auth_source', 'admin')
        )
        
        try:
            # 3. Measure ping time
            start_time = time.time()
            ping_result = client.admin.command('ping')
            end_time = time.time()
            
            # 4. Calculate latency in milliseconds
            latency_ms = (end_time - start_time) * 1000
            
            # 5. Assertions
            assert ping_result['ok'] == 1.0, "Ping failed"
            assert latency_ms < 100, f"MongoDB ping too slow: {latency_ms:.2f}ms"
            
            # 6. Log performance
            logger.info(f"âœ… MongoDB ping: {latency_ms:.2f}ms")
            
            # 7. Performance classification
            if latency_ms < 50:
                logger.info("âš¡ Excellent latency!")
            elif latency_ms < 100:
                logger.info("âœ“ Acceptable latency")
            else:
                logger.warning("âš ï¸ High latency detected!")
                
        finally:
            client.close()
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ×œ××” ×‘×•×“×§×™× ping ×•×œ× query ×××™×ª×™?**
**×ª×©×•×‘×”:**
```
Ping Command:
âœ… ×§×œ ×××•×“ (minimal overhead)
âœ… ×‘×•×“×§ network + authentication + basic health
âœ… consistent (×œ× ×ª×œ×•×™ ×‘-data)
âœ… ××”×™×¨ ×œ×¨×•×¥

Real Query:
âš ï¸ ×ª×œ×•×™ ×‘-data size
âš ï¸ ×ª×œ×•×™ ×‘××™× ×“×§×¡×™×
âš ï¸ ×œ× consistent
âš ï¸ ××©×¤×™×¢ ×¢×œ production

Ping = baseline performance
```

#### **×©××œ×” 2: ××” ×× ×”-latency ××©×ª× ×” ×”×¨×‘×” ×‘×™×Ÿ runs?**
**×ª×©×•×‘×”:**
```
Latency Variability:
- Normal: Â±10-20ms
- Problem: Â±50ms+

×¡×™×‘×•×ª ×œ-variability:
- Network congestion
- MongoDB load
- Disk I/O
- Background tasks (compaction, backups)

×¤×ª×¨×•×Ÿ:
# Run multiple pings and take median:
latencies = []
for _ in range(5):
    start = time.time()
    client.admin.command('ping')
    latencies.append((time.time() - start) * 1000)

median_latency = statistics.median(latencies)
p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
```

#### **×©××œ×” 3: ××™×š ××–×”×™× performance degradation ×œ××•×¨×š ×–××Ÿ?**
**×ª×©×•×‘×”:**
```
Monitoring Strategy:

1. Baseline Measurement:
   - Run test multiple times
   - Record median: ~15ms (example)

2. Continuous Monitoring:
   - Run every hour
   - Track trend

3. Alerting:
   - If latency > 2x baseline â†’ Warning
   - If latency > 100ms â†’ Critical

4. Historical Data:
   - Store in time-series DB
   - Visualize trends
   - Correlate with changes

Tools:
- Prometheus + Grafana
- CloudWatch
- Datadog
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Latency Range | Classification | Action |
|--------------|----------------|--------|
| <50ms | âš¡ Excellent | None |
| 50-100ms | âœ… Acceptable | Monitor |
| 100-200ms | âš ï¸ Warning | Investigate |
| >200ms | âŒ Critical | Immediate action |

---

<a name="test-5-mongodb-config"></a>
## âš™ï¸ ×˜×¡×˜ 5: MongoDB Connection Using Focus Server Config

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13807 |
| **Priority** | High |
| **Component** | ConfigManager, MongoDB |
| **Status** | Implemented âœ… |
| **Test File** | `tests/integration/infrastructure/test_basic_connectivity.py` |
| **Test Function** | `test_mongodb_connection` |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ××××ª ×©**ConfigManager ×˜×•×¢×Ÿ × ×›×•×Ÿ ××ª MongoDB configuration** ×•×©×”×—×™×‘×•×¨ ×¢×•×‘×“.

**×œ××” ×–×” ×§×¨×™×˜×™?**
Focus Server ××©×ª××© ×‘-ConfigManager ×œ× ×™×”×•×œ ×›×œ ×”-configs:
- MongoDB credentials
- API endpoints
- Timeouts
- Environments (dev, staging, production)

**×× ConfigManager ×œ× ×˜×•×¢×Ÿ × ×›×•×Ÿ â†’ Focus Server ×™×›×©×œ ×‘×”×ª×—×œ×”!**

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Test Data (environments.yaml):**
```yaml
new_production:
  mongodb:
    host: "10.10.100.108"
    port: 27017
    username: "prisma"
    password: "prisma"
    database: "prisma"
    auth_source: "prisma"
```

#### **×¦×¢×“×™ ×”×˜×¡×˜ (6 Steps):**

| # | Action | Data | Expected |
|---|--------|------|----------|
| 1 | Initialize ConfigManager | `env="new_production"` | Manager created |
| 2 | Get database config | `get_database_config()` | Config dict returned |
| 3 | Verify host | `config["host"]` | `"10.10.100.108"` |
| 4 | Verify port | `config["port"]` | `27017` |
| 5 | Create MongoDB client | Using config | Client created |
| 6 | Test connection | `ping` | Success |

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

```python
# File: tests/integration/infrastructure/test_basic_connectivity.py

import pytest
from pymongo import MongoClient
from config.config_manager import ConfigManager

class TestConfigManagerMongoDB:
    """
    Test MongoDB connection through ConfigManager.
    Validates configuration loading and connection establishment.
    """
    
    @pytest.mark.mongodb
    @pytest.mark.config
    @pytest.mark.high_priority
    def test_mongodb_connection_via_config_manager(self):
        """
        Test MongoDB connection using Focus Server's ConfigManager.
        
        Purpose:
        - Validate ConfigManager loads MongoDB config correctly
        - Ensure all connection parameters are accurate
        - Test connection using loaded config
        
        This test validates the configuration layer that Focus Server
        uses. If this fails, Focus Server won't start properly.
        
        Steps:
        1. Initialize ConfigManager for environment
        2. Load MongoDB configuration
        3. Validate each config parameter
        4. Create MongoDB client using config
        5. Test connection with ping
        
        Expected:
        - ConfigManager loads config successfully
        - All parameters match expected values
        - Connection successful
        """
        
        # 1. Initialize ConfigManager
        config_manager = ConfigManager("new_production")
        
        # 2. Get MongoDB configuration
        mongo_config = config_manager.get_database_config()
        
        # 3. Validate configuration parameters
        assert 'host' in mongo_config, "Missing 'host' in config"
        assert 'port' in mongo_config, "Missing 'port' in config"
        assert 'username' in mongo_config, "Missing 'username' in config"
        assert 'password' in mongo_config, "Missing 'password' in config"
        assert 'database' in mongo_config, "Missing 'database' in config"
        
        # 4. Validate specific values
        assert mongo_config['host'] == "10.10.100.108", \
            f"Unexpected host: {mongo_config['host']}"
        assert mongo_config['port'] == 27017, \
            f"Unexpected port: {mongo_config['port']}"
        assert mongo_config['database'] == "prisma", \
            f"Unexpected database: {mongo_config['database']}"
        
        logger.info("âœ… ConfigManager loaded MongoDB config correctly")
        logger.info(f"  Host: {mongo_config['host']}")
        logger.info(f"  Port: {mongo_config['port']}")
        logger.info(f"  Database: {mongo_config['database']}")
        
        # 5. Create MongoDB client using config
        # Option A: Manual connection string
        client = MongoClient(
            host=mongo_config['host'],
            port=mongo_config['port'],
            username=mongo_config['username'],
            password=mongo_config['password'],
            authSource=mongo_config.get('auth_source', 'admin')
        )
        
        # Option B: Using **kwargs unpacking (if config matches MongoClient params)
        # client = MongoClient(**mongo_config)
        
        try:
            # 6. Test connection with ping
            ping_result = client.admin.command('ping')
            assert ping_result['ok'] == 1.0, "Ping failed"
            
            logger.info("âœ… MongoDB connection via ConfigManager successful")
            
        finally:
            client.close()
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ××” ×–×” ConfigManager? ×œ××” ×¦×¨×™×š ××•×ª×•?**
**×ª×©×•×‘×”:**
```
ConfigManager = Centralized configuration management

Problem Without ConfigManager:
âŒ Hard-coded values scattered in code
âŒ Different configs for dev/staging/prod
âŒ Difficult to change
âŒ Security risk (passwords in code)

Solution With ConfigManager:
âœ… Single source of truth
âœ… Environment-based configs
âœ… Easy to change (edit YAML)
âœ… Secure (credentials from secrets)

Example:
config = ConfigManager("production")  # Auto-loads production config
mongo_config = config.get_database_config()  # Gets MongoDB config
```

#### **×©××œ×” 2: ××™×š ConfigManager ××˜×¤×œ ×‘×¡×‘×™×‘×•×ª ×©×•× ×•×ª?**
**×ª×©×•×‘×”:**
```yaml
# config/environments.yaml

development:
  mongodb:
    host: "localhost"
    port: 27017

staging:
  mongodb:
    host: "10.10.100.50"
    port: 27017

production:
  mongodb:
    host: "10.10.100.108"
    port: 27017
```

```python
# Usage:
config_dev = ConfigManager("development")
config_prod = ConfigManager("production")

# Same code, different environment!
mongo_config = config_dev.get_database_config()  # Gets dev config
mongo_config = config_prod.get_database_config()  # Gets prod config
```

#### **×©××œ×” 3: ××” ×§×•×¨×” ×× ConfigManager ×œ× ××•×¦× ××ª ×”×§×•×‘×¥?**
**×ª×©×•×‘×”:**
```python
# ConfigManager should handle this gracefully:

class ConfigManager:
    def __init__(self, environment):
        config_path = Path("config/environments.yaml")
        
        if not config_path.exists():
            raise FileNotFoundError(
                f"Configuration file not found: {config_path}"
            )
        
        # Load YAML
        with open(config_path) as f:
            all_configs = yaml.safe_load(f)
        
        if environment not in all_configs:
            raise ValueError(
                f"Environment '{environment}' not found in config. "
                f"Available: {list(all_configs.keys())}"
            )
        
        self.config = all_configs[environment]
```

#### **×©××œ×” 4: ××™×š ××˜×¤×œ×™× ×‘×¡×•×“×•×ª (secrets) ×‘×¦×•×¨×” ×‘×˜×•×—×”?**
**×ª×©×•×‘×”:**
```
Bad Practice âŒ:
mongodb:
  password: "mysecretpassword123"  # In Git!

Good Practices âœ…:

Option 1: Environment Variables
mongodb:
  password: ${MONGO_PASSWORD}  # From env var

Option 2: Secret Management Service
mongodb:
  password: ${vault:secret/mongo/password}  # From Vault

Option 3: Kubernetes Secrets
# Mounted as file or env var in pod

Implementation:
import os
password = os.getenv('MONGO_PASSWORD')
if not password:
    raise ValueError("MONGO_PASSWORD env var not set")
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Test Aspect | Requirement | ×¡×™×‘×” |
|------------|-------------|------|
| Config loads | 100% success | ×—×•×‘×” ×œ×”×ª×—×œ×” |
| All params present | 100% | ×—×•×‘×” ×œ×—×™×‘×•×¨ |
| Values correct | 100% match | ×—×•×‘×” ×œ××‘×˜×—×” |
| Connection works | 100% | ×—×•×‘×” ×œ×¤×•× ×§×¦×™×•× ×œ×™×•×ª |

---

<a name="test-6-mongodb-tcp"></a>
## ğŸ”Œ ×˜×¡×˜ 6: MongoDB Direct TCP Connection and Authentication

### ğŸ“Œ ×¤×¨×˜×™ ×”×˜×¡×˜

| ×¤×¨××˜×¨ | ×¢×¨×š |
|-------|-----|
| **Issue Key** | PZ-13806 |
| **Priority** | Critical |
| **Component** | MongoDB TCP, Authentication |
| **Status** | Implemented âœ… |
| **Test File** | `tests/integration/infrastructure/test_basic_connectivity.py` |
| **Test Function** | `test_mongodb_direct_connection` |

---

### ğŸ¯ ××˜×¨×ª ×”×˜×¡×˜

**××” ×”×˜×¡×˜ ×‘×•×“×§?**
×”×˜×¡×˜ ××××ª ×‘×¨××” ×”×›×™ × ××•×›×”:
1. **TCP connection** ×œ-MongoDB server
2. **Authentication** ×¢× credentials
3. **Basic operations**: ping, server info, list databases

**×œ××” ×–×” ×§×¨×™×˜×™?**
×–×” ×”×˜×¡×˜ ×”×›×™ ×‘×¡×™×¡×™ - **foundation layer**.

```
Layer 5: Application (Focus Server)
Layer 4: Configuration (ConfigManager)  â† Test #5
Layer 3: High-level ops (Collections)    â† Test #3
Layer 2: Performance (Ping latency)      â† Test #4
Layer 1: TCP + Auth                       â† Test #6 (THIS ONE)
```

**×× Layer 1 × ×›×©×œ â†’ ×”×›×œ × ×›×©×œ!**

---

### ğŸ“Š ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Test Data:**
```json
{
  "host": "10.10.100.108",
  "port": 27017,
  "username": "prisma",
  "password": "prisma",
  "auth_source": "prisma",
  "database": "prisma",
  "connection_timeout_ms": 10000,
  "server_selection_timeout_ms": 10000
}
```

#### **×¦×¢×“×™ ×”×˜×¡×˜ (9 Steps):**

| # | Action | Expected |
|---|--------|----------|
| 1 | Load MongoDB config | Config loaded |
| 2 | Create MongoClient | Client created |
| 3 | Establish TCP connection | Connection established |
| 4 | Authenticate | Authentication success |
| 5 | Send ping command | `{'ok': 1.0}` |
| 6 | Get server info | Version returned |
| 7 | List databases | Database list returned |
| 8 | Verify prisma DB exists | `'prisma' in db_list` |
| 9 | Close connection | Clean disconnect |

---

### ğŸ”§ ××™×š ×œ×××© ××ª ×”×˜×¡×˜ ×‘×§×•×“?

```python
# File: tests/integration/infrastructure/test_basic_connectivity.py

import pytest
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError, OperationFailure
from config.config_manager import ConfigManager

class TestMongoDBLowLevel:
    """
    Low-level MongoDB connectivity tests.
    Tests the foundational layer: TCP connection and authentication.
    """
    
    @pytest.mark.mongodb
    @pytest.mark.infrastructure
    @pytest.mark.critical
    def test_mongodb_direct_tcp_connection(self):
        """
        Test MongoDB direct TCP connection and authentication.
        
        Purpose:
        - Validate TCP connection to MongoDB server
        - Verify authentication mechanism works
        - Test basic commands (ping, server info)
        - Verify database accessibility
        
        This is the most fundamental MongoDB test - it validates
        the lowest layer of connectivity. All other MongoDB tests
        depend on this passing.
        
        Steps:
        1. Load MongoDB configuration
        2. Create MongoClient with explicit parameters
        3. Establish TCP connection
        4. Authenticate with credentials
        5. Execute ping command
        6. Get server version info
        7. List databases
        8. Verify target database exists
        9. Close connection cleanly
        
        Expected:
        - TCP connection successful
        - Authentication passes
        - Ping returns {'ok': 1.0}
        - Server version retrieved
        - Database list includes 'prisma'
        - Clean disconnect
        """
        
        # 1. Load configuration
        config_manager = ConfigManager("new_production")
        mongo_config = config_manager.get_database_config()
        
        # 2. Create MongoClient with explicit parameters and timeouts
        client = MongoClient(
            host=mongo_config['host'],  # 10.10.100.108
            port=mongo_config['port'],  # 27017
            username=mongo_config['username'],
            password=mongo_config['password'],
            authSource=mongo_config.get('auth_source', 'admin'),
            # Timeouts
            serverSelectionTimeoutMS=10000,  # 10 seconds
            connectTimeoutMS=10000,
            socketTimeoutMS=10000,
            # Connection pool
            maxPoolSize=10,
            minPoolSize=1
        )
        
        try:
            # 3. Test TCP connection + Authentication with ping
            logger.info("Testing TCP connection and authentication...")
            try:
                ping_result = client.admin.command('ping')
            except ConnectionFailure as e:
                pytest.fail(f"TCP connection failed: {e}")
            except OperationFailure as e:
                pytest.fail(f"Authentication failed: {e}")
            except ServerSelectionTimeoutError as e:
                pytest.fail(f"Server selection timeout: {e}")
            
            # 4. Validate ping response
            assert ping_result.get('ok') == 1.0, \
                f"Ping failed with response: {ping_result}"
            logger.info("âœ… TCP connection and authentication successful")
            
            # 5. Get server information
            logger.info("Retrieving server information...")
            server_info = client.server_info()
            
            # Validate server info structure
            assert 'version' in server_info, "Server info missing 'version'"
            version = server_info['version']
            logger.info(f"âœ… MongoDB Version: {version}")
            
            # Optional: Verify version meets minimum requirement
            # major_version = int(version.split('.')[0])
            # assert major_version >= 4, f"MongoDB version too old: {version}"
            
            # 6. List all databases
            logger.info("Listing databases...")
            db_list = client.list_database_names()
            
            assert isinstance(db_list, list), "Database list not a list"
            assert len(db_list) > 0, "No databases found (empty list)"
            logger.info(f"Found {len(db_list)} databases: {db_list}")
            
            # 7. Verify target database exists
            target_db = mongo_config['database']  # 'prisma'
            assert target_db in db_list, \
                f"Database '{target_db}' not found in {db_list}"
            logger.info(f"âœ… Target database '{target_db}' exists")
            
            # 8. Additional validation: Connect to database and verify access
            db = client[target_db]
            collections = db.list_collection_names()
            logger.info(f"Database '{target_db}' has {len(collections)} collections")
            
        except Exception as e:
            pytest.fail(f"MongoDB connectivity test failed: {e}")
            
        finally:
            # 9. Always close connection
            client.close()
            logger.info("âœ… MongoDB connection closed cleanly")
```

---

### â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª + ×ª×©×•×‘×•×ª

#### **×©××œ×” 1: ××” ×”×”×‘×“×œ ×‘×™×Ÿ ConnectionFailure ×œ-OperationFailure?**
**×ª×©×•×‘×”:**
```
ConnectionFailure:
- TCP connection × ×›×©×œ
- Cannot reach server
- Network issue
- MongoDB down

OperationFailure:
- TCP connection OK
- Authentication failed
- Wrong username/password
- Wrong authSource
- Permissions issue

Example:
try:
    client.admin.command('ping')
except ConnectionFailure:
    # Network/server problem
    print("Cannot reach MongoDB")
except OperationFailure:
    # Auth problem
    print("Wrong credentials")
```

#### **×©××œ×” 2: ××” ×–×” ServerSelectionTimeout? ××ª×™ ×–×” ×§×•×¨×”?**
**×ª×©×•×‘×”:**
```
ServerSelectionTimeout:
- MongoDB client tries to find suitable server
- Can't find one within timeout
- Common causes:
  âœ— MongoDB not running
  âœ— Network unreachable
  âœ— Wrong host/port
  âœ— Replica set misconfigured

Timeout Configuration:
serverSelectionTimeoutMS=10000  # 10 seconds

Recommendation:
- Dev/Testing: 5-10 seconds
- Production: 10-30 seconds
- CI/CD: 5 seconds (fail fast)
```

#### **×©××œ×” 3: ××” ×–×” Connection Pool? ×œ××” ×¦×¨×™×š ××•×ª×•?**
**×ª×©×•×‘×”:**
```
Connection Pool = ×××’×¨ ×—×™×‘×•×¨×™× ×¤×ª×•×—×™×

Without Pool:
Request 1 â†’ Open connection â†’ Use â†’ Close
Request 2 â†’ Open connection â†’ Use â†’ Close
Request 3 â†’ Open connection â†’ Use â†’ Close
    â†“
Slow! Open/close connection is expensive

With Pool:
Pool: [Conn1, Conn2, Conn3, ...]
Request 1 â†’ Get Conn1 from pool â†’ Use â†’ Return to pool
Request 2 â†’ Get Conn2 from pool â†’ Use â†’ Return to pool
    â†“
Fast! Reuse existing connections

Configuration:
maxPoolSize=10   # Max 10 concurrent connections
minPoolSize=1    # Keep at least 1 open
```

#### **×©××œ×” 4: ××™×š ×‘×•×“×§×™× ×©×”×¡×’×™×¨×” ×©×œ Connection ×ª×§×™× ×”?**
**×ª×©×•×‘×”:**
```python
# Always use try-finally:
client = MongoClient(...)
try:
    # Do work
    pass
finally:
    client.close()  # Always runs!

# Check connection state:
assert client.address is None, "Connection not closed"

# Check no hanging connections on MongoDB:
# On MongoDB server:
db.adminCommand({currentOp: true})
# Check 'inprog' array is empty

# In test:
# Run test
# Check MongoDB connections count
# Should be 0 after test completes
```

#### **×©××œ×” 5: ××” ×§×•×¨×” ×× ×™×© network hiccup ×‘×××¦×¢?**
**×ª×©×•×‘×”:**
```
pymongo Auto-Retry:
âœ… Automatically retries failed operations
âœ… Handles temporary network issues
âœ… Transparent to application

Configuration:
retryWrites=True   # Retry write operations
retryReads=True    # Retry read operations

Example:
# Network hiccup during ping:
1. client.admin.command('ping')
2. Network drops for 100ms
3. pymongo auto-retries
4. Network back
5. Ping succeeds
   â†“
Application doesn't even know!

If exhausts retries:
â†’ Raises exception
```

---

### ğŸ“ˆ ×§×¨×™×˜×¨×™×•× ×™ ×”×¦×œ×—×”

| Test Stage | Requirement | Impact if Fails |
|-----------|-------------|-----------------|
| TCP connection | 100% | Total system failure |
| Authentication | 100% | Cannot access data |
| Ping command | 100% | Basic health check fails |
| Server info | 100% | Cannot verify version |
| Database list | 100% | Cannot find databases |

---

<a name="summary"></a>
## ğŸ“‹ ×¡×™×›×•× - ××˜×¨×™×¦×ª ×”×©×•×•××” ×‘×™×Ÿ ×”×˜×¡×˜×™×

### ×˜×‘×œ×ª ×”×©×•×•××” ××§×™×¤×”:

| ×”×™×‘×˜ | Test #1<br/>SSH | Test #2<br/>K8s | Test #3<br/>MongoDB Health | Test #4<br/>MongoDB Perf | Test #5<br/>Config | Test #6<br/>TCP Auth |
|------|----------------|----------------|---------------------------|------------------------|-------------------|---------------------|
| **Priority** | High | High | High | Medium | High | **Critical** |
| **Layer** | Infrastructure | Orchestration | Data Layer | Performance | Configuration | Foundation |
| **Purpose** | Troubleshooting access | Pod monitoring | DB health | Performance | Config validation | Basic connectivity |
| **Isolates** | SSH connectivity | K8s cluster | MongoDB operations | Response time | ConfigManager | TCP + Auth |
| **Duration** | 3-5s | 3-5s | 2-3s | <1s | 1-2s | 1-2s |
| **Dependency** | Network | Network, K8s | Network, MongoDB | MongoDB | Config files | Network, MongoDB |
| **Failure Impact** | No troubleshooting | No monitoring | No data access | Performance issues | App won't start | Total MongoDB failure |
| **Automation** | âœ… Automated | âœ… Automated | âœ… Automated | âœ… Automated | âœ… Automated | âœ… Automated |

---

### ×–×¨×™××” ×œ×•×’×™×ª ×‘×™×Ÿ ×”×˜×¡×˜×™×:

```
1. Test #6 (TCP + Auth) - FOUNDATION
   â†“ If passes
   
2. Test #3 (MongoDB Health) - OPERATIONS
   â†“ If passes
   
3. Test #4 (Performance) - SPEED
   â†“ If passes
   
4. Test #5 (ConfigManager) - CONFIGURATION
   â†“ If passes
   
5. Test #1 (SSH) - MAINTENANCE ACCESS
   
6. Test #2 (Kubernetes) - ORCHESTRATION HEALTH
```

**×›×œ×œ×™ ××‘×—×•×Ÿ (Diagnosis Rules):**

| Failure Pattern | Diagnosis | Next Step |
|----------------|-----------|-----------|
| Test #6 fails | MongoDB TCP/Auth issue | Check MongoDB logs, network |
| Test #6 passes, #3 fails | MongoDB health issue | Check collections, disk space |
| Tests #3,#6 pass, #4 fails | Performance degradation | Check load, indexes |
| Tests #3,#4,#6 pass, #5 fails | Config problem | Check YAML files |
| Tests #1,#2 fail | Infrastructure down | Check SSH, K8s cluster |

---

<a name="qa"></a>
## â“ ×©××œ×•×ª ×¦×¤×•×™×•×ª ×›×œ×œ×™×•×ª ×‘×¤×’×™×©×” + ×ª×©×•×‘×•×ª

### ×©××œ×•×ª ××¡×˜×¨×˜×’×™×•×ª:

#### **×©××œ×” 1: ×œ××” ×¦×¨×™×š 6 ×˜×¡×˜×™× ×©×•× ×™× ×œ-MongoDB? ×–×” ×œ× redundant?**
**×ª×©×•×‘×”:**
```
×œ×! ×›×œ ×˜×¡×˜ ×‘×•×—×Ÿ ×©×›×‘×” ××—×¨×ª:

Test #6: ×”×× ×× ×™ ×™×›×•×œ ×œ×”×ª×—×‘×¨? (TCP + Auth)
Test #3: ×”×× ×”××‘× ×” ×ª×§×™×Ÿ? (Collections, Schema)
Test #4: ×”×× ×–×” ××”×™×¨? (Performance)
Test #5: ×”×× Config × ×›×•×Ÿ? (ConfigManager)

Analogy (×“×™××•×™):
- Test #6 = "×”×× ×™×© ×—×™×‘×•×¨ ×œ××™× ×˜×¨× ×˜?"
- Test #3 = "×”×× ×”××ª×¨ ×§×™×™×?"
- Test #4 = "×”×× ×”××ª×¨ ××”×™×¨?"
- Test #5 = "×”×× ×”×©××¨×ª×™ ××ª ×”-URL × ×›×•×Ÿ?"

×›×œ ×©×›×‘×” ×™×›×•×œ×” ×œ×›×©×œ ×‘× ×¤×¨×“!
```

#### **×©××œ×” 2: ××” ×¢×•×©×™× ×× ×˜×¡×˜ × ×›×©×œ ×‘-CI/CD pipeline?**
**×ª×©×•×‘×”:**
```
CI/CD Failure Strategy:

1. Identify Layer:
   - Infrastructure test fails â†’ ×‘×¢×™×” ×‘×ª×©×ª×™×ª
   - Functional test fails â†’ ×‘×¢×™×” ×‘×§×•×“

2. Immediate Actions:
   âœ… Block deployment (don't deploy broken code)
   âœ… Notify team (Slack, email)
   âœ… Create incident ticket
   âœ… Check logs

3. Investigation:
   - Check test logs
   - Check service logs
   - Check infrastructure monitoring
   - Reproduce locally

4. Resolution:
   - Fix issue
   - Re-run tests
   - If passes â†’ merge/deploy
   - If fails â†’ escalate

Example Flow:
Git Push â†’ CI runs tests â†’ Test #3 fails â†’ Block merge
                                          â†“
                            Team investigates â†’ MongoDB disk full
                                          â†“
                            Clear space â†’ Re-run â†’ Pass â†’ Deploy
```

#### **×©××œ×” 3: ××™×š ××ª×¢×“×¤×™× fix ×©×œ ×˜×¡×˜×™× ×›×©×”×¨×‘×” × ×›×©×œ×™×?**
**×ª×©×•×‘×”:**
```
Priority Order (××”×’×‘×•×” ×œ× ××•×š):

1. Critical Infrastructure (Test #6, #1, #2)
   â†’ ×—×•×‘×” ×œ×¤×¢×•×œ×” ×‘×¡×™×¡×™×ª
   
2. High Priority Operations (Test #3, #5)
   â†’ ×—×™×•× ×™ ×œ×¤×•× ×§×¦×™×•× ×œ×™×•×ª
   
3. Performance & Optimization (Test #4)
   â†’ ×—×©×•×‘ ××‘×œ ×œ× blocking

Decision Matrix:
| Severity | Production Impact | Fix Priority |
|----------|------------------|--------------|
| Critical | System down | P0 - Immediate |
| High | Features broken | P1 - Same day |
| Medium | Degraded perf | P2 - This week |
| Low | Minor issues | P3 - Backlog |

Example:
- Test #6 fails + Test #4 fails â†’ Fix #6 first!
- Test #1 fails + Test #5 fails â†’ Fix #5 first (blocks app start)
```

#### **×©××œ×” 4: ××™×š ××‘×˜×™×—×™× ×©×”×˜×¡×˜×™× ×¢×¦×× ×œ× broken?**
**×ª×©×•×‘×”:**
```
Test Quality Assurance:

1. Code Reviews:
   âœ… Peer review ×œ×›×œ test
   âœ… Senior approval

2. Test the Tests:
   âœ… Run locally before commit
   âœ… Verify against known-good state
   âœ… Test both pass and fail scenarios

3. Maintenance:
   âœ… Update when APIs change
   âœ… Refactor when brittle
   âœ… Document expected behavior

4. Monitoring:
   âœ… Track flaky tests (intermittent failures)
   âœ… Track test execution time
   âœ… Alert on unusual patterns

Flaky Test Detection:
# Run test 10 times:
for i in range(10):
    result = run_test()
    results.append(result)

# If not all pass or all fail â†’ Flaky!
if not (all(results) or not any(results)):
    mark_as_flaky()
```

#### **×©××œ×” 5: ××” ×”-ROI (Return on Investment) ×©×œ ×”××•×˜×•××¦×™×” ×”×–××ª?**
**×ª×©×•×‘×”:**
```
ROI Calculation:

Manual Testing:
- Time per test: 5 minutes
- 6 tests: 30 minutes
- Run per day: 3 times (dev, staging, prod)
- Total: 90 minutes/day
- Monthly: 90 min Ã— 22 days = 33 hours
- Cost: 33 hours Ã— $50/hour = $1,650/month

Automated Testing:
- Setup time: 40 hours (one-time)
- Maintenance: 2 hours/month
- Execution time: 30 seconds (vs 90 minutes)
- Cost: $0/month (runs automatically)

Break-Even:
$2,000 setup / $1,650 saved per month = 1.2 months

After 2 months: Net positive ROI!

Additional Benefits (hard to quantify):
âœ… Faster feedback (seconds vs hours)
âœ… Earlier bug detection
âœ… Consistent execution
âœ… No human error
âœ… Enables CI/CD
âœ… Confidence in deployments

Total ROI: ~500% within first year
```

---

### ×©××œ×•×ª ×˜×›× ×™×•×ª:

#### **×©××œ×” 6: ××™×š ××˜×¤×œ×™× ×‘×¡×‘×™×‘×•×ª ×©×•× ×•×ª (dev/staging/prod)?**
**×ª×©×•×‘×”:**
```python
# config/environments.yaml
environments:
  development:
    mongodb:
      host: "localhost"
      port: 27017
    kubernetes:
      context: "minikube"
    ssh:
      enabled: false  # No SSH in dev
  
  staging:
    mongodb:
      host: "staging-mongo.internal"
      port: 27017
    kubernetes:
      context: "staging-cluster"
    ssh:
      jump_host: "staging-jump.internal"
  
  production:
    mongodb:
      host: "10.10.100.108"
      port: 27017
    kubernetes:
      context: "prod-cluster"
    ssh:
      jump_host: "10.10.100.3"

# Test execution:
# Dev:
pytest --env=development

# Staging:
pytest --env=staging

# Prod:
pytest --env=production
```

#### **×©××œ×” 7: ××™×š ××•× ×¢×™× ×ª×œ×•×ª ×‘×™×Ÿ ×˜×¡×˜×™× (test isolation)?**
**×ª×©×•×‘×”:**
```python
# Bad Practice âŒ:
def test_create_user():
    user = create_user("test@example.com")
    # Doesn't clean up!

def test_login():
    # Depends on test_create_user!
    login("test@example.com")

# Good Practice âœ…:
@pytest.fixture
def clean_database():
    # Setup
    db.clear()
    yield
    # Teardown
    db.clear()

def test_create_user(clean_database):
    user = create_user("test@example.com")
    # Clean database after test

def test_login(clean_database):
    # Independent - creates own data
    create_user("test@example.com")
    login("test@example.com")

Principles:
1. Each test creates its own data
2. Each test cleans up after itself
3. Tests can run in any order
4. Tests can run in parallel
```

#### **×©××œ×” 8: ××™×š measuring test coverage?**
**×ª×©×•×‘×”:**
```bash
# Install coverage tool:
pip install pytest-cov

# Run with coverage:
pytest --cov=src --cov-report=html tests/

# Output:
Name                  Stmts   Miss  Cover
-----------------------------------------
src/__init__.py           5      0   100%
src/mongodb.py          120     10    92%
src/ssh.py               80      5    94%
-----------------------------------------
TOTAL                   205     15    93%

# View detailed report:
open htmlcov/index.html

Coverage Goals:
âœ… Critical paths: 100%
âœ… Infrastructure: 95%+
âœ… Utilities: 90%+
âœ… Overall: 85%+
```

---

## ğŸ“ ××•× ×—×•×Ÿ (Glossary) - ××™×œ×•×Ÿ ××•× ×—×™×

### Infrastructure Terms:

| ××•× ×— | ×”×¡×‘×¨ | ×“×•×’××” |
|------|------|-------|
| **Jump Host** | ×©×¨×ª ×‘×™× ×™×™× ×××•×‘×˜×— ×œ×’×™×©×” ×œ-production | Bastion server |
| **LoadBalancer** | ×× ×’× ×•×Ÿ ×—×œ×•×§×ª ×¢×•××¡×™× | MongoDB exposed via LB |
| **ClusterIP** | ×›×ª×•×‘×ª IP ×¤× ×™××™×ª ×‘-K8s | Service internal IP |
| **Namespace** | ×§×‘×•×¦×” ×œ×•×’×™×ª ×©×œ resources ×‘-K8s | `panda` namespace |
| **Pod** | ×™×—×™×“×ª ×¨×™×¦×” ×‘×¡×™×¡×™×ª ×‘-K8s | Container group |

### Testing Terms:

| ××•× ×— | ×”×¡×‘×¨ | ×“×•×’××” |
|------|------|-------|
| **Isolation Test** | ×˜×¡×˜ ×©×‘×•×“×§ ×§×•××¤×•× × ×˜ ××—×“ ×‘×œ×‘×“ | MongoDB without Focus Server |
| **Integration Test** | ×˜×¡×˜ ×©×‘×•×“×§ ×§×•××¤×•× × ×˜×•×ª ×™×—×“ | Focus Server + MongoDB |
| **Smoke Test** | ×˜×¡×˜×™× ×‘×¡×™×¡×™×™× ××”×™×¨×™× | Can I connect? |
| **Regression Test** | ×‘×“×™×§×” ×©×œ× × ×©×‘×¨×• ×“×‘×¨×™× ×§×™×™××™× | After changes |

### MongoDB Terms:

| ××•× ×— | ×”×¡×‘×¨ | ×“×•×’××” |
|------|------|-------|
| **authSource** | DB ×©×‘×• × ××¦××™× users | `prisma` |
| **Collection** | ×˜×‘×œ×” (×›××• ×‘-SQL) | `recordings` |
| **Document** | ×©×•×¨×” (×›××• ×‘-SQL) | Single recording |
| **Connection Pool** | ×××’×¨ ×—×™×‘×•×¨×™× ×¤×ª×•×—×™× | Reuse connections |

---

## âœ… Checklist ×œ×”×›× ×” ×œ×¤×’×™×©×”

### ×”×›× ×” ×˜×›× ×™×ª:
- [ ] ×§×¨××ª×™ ××ª ×›×œ 6 ×”×˜×¡×˜×™×
- [ ] ×”×‘× ×ª×™ ××ª ×”××˜×¨×” ×©×œ ×›×œ ×˜×¡×˜
- [ ] ×™×›×•×œ ×œ×”×¡×‘×™×¨ ××ª ×”×”×‘×“×œ×™× ×‘×™× ×™×”×
- [ ] ×™×•×“×¢ ××™×š ×œ×××© ×›×œ ×˜×¡×˜ (×‘×¨××” ×§×•× ×¡×¤×˜×•××œ×™×ª)
- [ ] ××›×™×¨ ××ª ×”-dependencies ×”× ×“×¨×©×™×
- [ ] ××‘×™×Ÿ ××ª ×”-test flow ×©×œ ×›×œ ×˜×¡×˜

### ×”×›× ×” ×§×•× ×¡×¤×˜×•××œ×™×ª:
- [ ] ××‘×™×Ÿ ××ª ×©×›×‘×•×ª ×”×ª×©×ª×™×ª (TCP â†’ Health â†’ Performance â†’ Config)
- [ ] ×™×›×•×œ ×œ×”×¡×‘×™×¨ ×œ××” ×¦×¨×™×š ×›×œ ×˜×¡×˜
- [ ] ×™×•×“×¢ ××” ×§×•×¨×” ×›×©×›×œ ×˜×¡×˜ × ×›×©×œ
- [ ] ××‘×™×Ÿ ××ª ×”×§×©×¨ ×‘×™×Ÿ ×”×˜×¡×˜×™×
- [ ] ×™×›×•×œ ×œ×ª×¢×“×£ fixes ×œ×¤×™ ×—×•××¨×”

### ×”×›× ×” ×œ×©××œ×•×ª:
- [ ] ××•×›×Ÿ ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª "×œ××”"
- [ ] ××•×›×Ÿ ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª ×˜×›× ×™×•×ª
- [ ] ××•×›×Ÿ ×œ×”×¦×™×’ alternatives/trade-offs
- [ ] ××•×›×Ÿ ×œ×”×¡×‘×™×¨ ROI
- [ ] ××•×›×Ÿ ×œ×“×•×Ÿ ×‘-CI/CD integration

---

## ğŸ¯ ×”××œ×¦×•×ª ××—×¨×•× ×•×ª ×œ×¤×’×™×©×”

### Do's âœ…:
1. **×”×ª×—×œ ×¢× Big Picture** - ×”×¡×‘×¨ ××ª ××˜×¨×ª ×‘×“×™×§×•×ª Infrastructure
2. **×”×©×ª××© ×‘×“×•×’×××•×ª** - "×–×” ×›××• ×œ×‘×“×•×§ ×©×™×© ×—×©××œ ×œ×¤× ×™ ×©××“×œ×™×§×™× ××—×©×‘"
3. **×”×“×’×© × ×—×™×¦×•×ª** - "×‘×œ×™ ×”×˜×¡×˜×™× ×”××œ×”, ×œ× × ×“×¢ ×©××©×”×• × ×©×‘×¨ ×¢×“ ×©×œ×§×•×— ×™×ª×œ×•× ×Ÿ"
4. **×”×¦×’ confidence** - "×”×˜×¡×˜×™× ×”××œ×” × ×•×ª× ×™× ×œ× ×• ×××•×Ÿ ×œ×¢×©×•×ª deployments"
5. **×“×‘×¨ ×¢×œ automation value** - "×—×•×¡×š X ×©×¢×•×ª ×‘×©×‘×•×¢"

### Don'ts âŒ:
1. **××œ ×ª×™×›× ×¡ ×œ×¤×¨×˜×™× ×§×˜× ×™× ××“×™** - ××œ× ×× ×©×•××œ×™×
2. **××œ ×ª× ×™×— ×™×“×¢ ××•×§×“×** - ×”×¡×‘×¨ ××•×©×’×™× ×‘×¡×™×¡×™×™×
3. **××œ ×ª×’×™×“ "×–×” ×¤×©×•×˜"** - ×–×” ××–×œ×–×œ ×‘××××¥
4. **××œ ×ª×”×™×” defensive** - ×× ×™×© ×‘×™×§×•×¨×ª, ×ª×§×©×™×‘
5. **××œ ×ª×‘×˜×™×— ××” ×©××ª×” ×œ× ×™×›×•×œ ×œ×¢××•×“ ×‘×•** - ×”×™×” realistic

### Key Messages ×œ×”×¢×‘×™×¨:
1. ğŸ¯ **Infrastructure tests are critical** - without them, we're blind
2. âš¡ **Automation saves time and increases confidence**
3. ğŸ” **Each test isolates a specific layer** - efficient debugging
4. ğŸ“Š **Measurable ROI** - pays for itself in weeks
5. ğŸš€ **Enables CI/CD** - faster, safer deployments

---

## ğŸ“ ×¦×•×¨ ×§×©×¨ ×œ×©××œ×•×ª × ×•×¡×¤×•×ª

×× ×™×© ×©××œ×•×ª × ×•×¡×¤×•×ª ××• ×¦×¨×™×š ×”×‘×”×¨×•×ª:
- ğŸ“§ Email: [your-email]
- ğŸ’¬ Slack: #focus-server-qa
- ğŸ“± Teams: QA Channel

---

**×‘×”×¦×œ×—×” ×‘×¤×’×™×©×”! ğŸš€**

*××¡××š ×–×” ×”×•×›×Ÿ ×¢×‘×•×¨ PZ-13756 - Focus Server Infrastructure Tests*
*×¢×•×“×›×Ÿ ×œ××—×¨×•× ×”: 27 ××•×§×˜×•×‘×¨ 2025*

