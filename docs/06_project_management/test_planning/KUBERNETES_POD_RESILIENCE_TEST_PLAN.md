# ×ª×•×›× ×™×ª ×‘×“×™×§×•×ª - Kubernetes Pod Resilience
==============================================

**×ª××¨×™×š ×™×¦×™×¨×”:** 2025-11-07  
**××˜×¨×”:** ×‘×“×™×§×ª resilience ×©×œ ×”××¢×¨×›×ª ×›×©-Kubernetes pods × ×•×¤×œ×™× ××• ×œ× ×¤×¢×™×œ×™×  
**×¢×“×™×¤×•×ª:** ×’×‘×•×”×” (High)  
**×§×©×•×¨ ×œ:** PZ-13756 (Infrastructure Resilience)

---

## ğŸ“‹ ×¡×§×™×¨×” ×›×œ×œ×™×ª

### ××˜×¨×ª ×”×‘×“×™×§×•×ª
×œ×‘×“×•×§ ××™×š ×”××¢×¨×›×ª ××’×™×‘×” ×›××©×¨ ××—×“ ××• ×™×•×ª×¨ ××”-pods ×”×§×¨×™×˜×™×™× ×‘-namespace `panda` × ×•×¤×œ×™×, ×œ× ×¤×¢×™×œ×™×, ××• ×œ× ×–××™× ×™×.

### Pods ×§×¨×™×˜×™×™× ×œ×‘×“×™×§×”
1. **MongoDB:** `mongodb-7cb5d67cc5-np7ch` (Deployment: mongodb)
2. **RabbitMQ:** `rabbitmq-panda-0` (StatefulSet: rabbitmq-panda)
3. **Focus Server:** `panda-panda-focus-server-78dbcfd9d9-kjj77` (Deployment: panda-panda-focus-server)
4. **SEGY Recorder:** `panda-panda-segy-recorder-84b4d85bcc-gtwnt` (Deployment: panda-panda-segy-recorder)

---

## ğŸ¯ ×ª×¨×—×™×©×™ ×‘×“×™×§×”

### ×§×˜×’×•×¨×™×” 1: Pod Failure Scenarios (×ª×¨×—×™×©×™ ×›×©×œ)

#### 1.1 Pod Deletion (××—×™×§×ª Pod)
**×ª×™××•×¨:** ××—×™×§×ª pod ×›×“×™ ×œ×‘×“×•×§ ××™×š Kubernetes ××˜×¤×œ ×‘-recreation  
**×ª×¨×—×™×©×™×:**
- ××—×™×§×ª pod ×‘×•×“×“
- ××—×™×§×ª pod ×‘×–××Ÿ ×©×™×© requests ×¤×¢×™×œ×™×
- ××—×™×§×ª pod ×‘×–××Ÿ ×©×™×© jobs ×¤×¢×™×œ×™×

**×¦×™×¤×™×•×ª:**
- Kubernetes ×™×•×¦×¨ pod ×—×“×© ××•×˜×•××˜×™×ª (ReplicaSet/StatefulSet)
- ×”××¢×¨×›×ª ×××©×™×›×” ×œ×¢×‘×•×“ ×œ××—×¨ ×”-recreation
- ××™×Ÿ data loss (×× ×™×© persistence)

#### 1.2 Pod Crash/Restart (×§×¨×™×¡×”/×”×¤×¢×œ×” ××—×“×©)
**×ª×™××•×¨:** Pod ×§×•×¨×¡ ××• restart  
**×ª×¨×—×™×©×™×:**
- Pod crash ×¢× restart policy
- Pod restart ×¢×§×‘ liveness probe failure
- Pod restart ×¢×§×‘ resource limits

**×¦×™×¤×™×•×ª:**
- Pod restart ××•×˜×•××˜×™
- ×”××¢×¨×›×ª ×××©×™×›×” ×œ×¢×‘×•×“ ×œ××—×¨ restart
- ××™×Ÿ data corruption

#### 1.3 Pod Not Ready (Pod ×œ× ××•×›×Ÿ)
**×ª×™××•×¨:** Pod ×¨×¥ ××‘×œ ×œ× ×¢×•×‘×¨ readiness probe  
**×ª×¨×—×™×©×™×:**
- Pod ×ª×§×•×¢ ×‘-ContainerCreating
- Pod ×ª×§×•×¢ ×‘-Pending
- Pod ×ª×§×•×¢ ×‘-CrashLoopBackOff

**×¦×™×¤×™×•×ª:**
- ×”××¢×¨×›×ª ×œ× ×× ×¡×” ×œ×”×©×ª××© ×‘-pod ×œ× ××•×›×Ÿ
- ×™×© fallback ××• retry mechanism
- ×”×•×“×¢×•×ª ×©×’×™××” ×‘×¨×•×¨×•×ª

---

### ×§×˜×’×•×¨×™×” 2: Scaling Scenarios (×ª×¨×—×™×©×™ Scaling)

#### 2.1 Scale Down to 0 (×”×§×˜× ×” ×œ-0)
**×ª×™××•×¨:** Scale down deployment ×œ-0 replicas  
**×ª×¨×—×™×©×™×:**
- Scale MongoDB ×œ-0
- Scale RabbitMQ ×œ-0
- Scale Focus Server ×œ-0
- Scale SEGY Recorder ×œ-0

**×¦×™×¤×™×•×ª:**
- ×”××¢×¨×›×ª ××—×–×™×¨×” ×©×’×™××•×ª ×‘×¨×•×¨×•×ª (503 Service Unavailable)
- ××™×Ÿ crashes ××• undefined behavior
- ×”××¢×¨×›×ª ××ª××•×©×©×ª ×œ××—×¨ scale up

#### 2.2 Scale Up/Down (×”×’×“×œ×”/×”×§×˜× ×”)
**×ª×™××•×¨:** ×©×™× ×•×™ ××¡×¤×¨ replicas  
**×ª×¨×—×™×©×™×:**
- Scale up ×-1 ×œ-2 (×× × ×ª××š)
- Scale down ×-2 ×œ-1
- Rolling update

**×¦×™×¤×™×•×ª:**
- ××™×Ÿ downtime ×‘××”×œ×š scaling
- ×”××¢×¨×›×ª ×××©×™×›×” ×œ×¢×‘×•×“
- Load balancing ×¢×•×‘×“ × ×›×•×Ÿ

---

### ×§×˜×’×•×¨×™×” 3: Network Issues (×‘×¢×™×•×ª ×¨×©×ª)

#### 3.1 Pod Network Isolation (×‘×™×“×•×“ ×¨×©×ª)
**×ª×™××•×¨:** Pod ×œ× ×™×›×•×œ ×œ×ª×§×©×¨ ×¢× pods ××—×¨×™×  
**×ª×¨×—×™×©×™×:**
- MongoDB ×œ× ×™×›×•×œ ×œ×ª×§×©×¨ ×¢× Focus Server
- RabbitMQ ×œ× ×™×›×•×œ ×œ×ª×§×©×¨ ×¢× Focus Server
- Focus Server ×œ× ×™×›×•×œ ×œ×ª×§×©×¨ ×¢× MongoDB/RabbitMQ

**×¦×™×¤×™×•×ª:**
- ×”×•×“×¢×•×ª ×©×’×™××” ×‘×¨×•×¨×•×ª
- Retry logic
- Graceful degradation

---

### ×§×˜×’×•×¨×™×” 4: Resource Exhaustion (×ª×©×œ×•× ××©××‘×™×)

#### 4.1 CPU/Memory Limits (××’×‘×œ×•×ª CPU/Memory)
**×ª×™××•×¨:** Pod ××’×™×¢ ×œ-resource limits  
**×ª×¨×—×™×©×™×:**
- Pod × ×”×¨×’ ×¢×§×‘ OOM (Out of Memory)
- Pod throttled ×¢×§×‘ CPU limits
- Node resource exhaustion

**×¦×™×¤×™×•×ª:**
- Pod restart ××•×˜×•××˜×™
- ×”××¢×¨×›×ª ×××©×™×›×” ×œ×¢×‘×•×“
- Resource monitoring ××“×•×•×— ×¢×œ ×”×‘×¢×™×”

---

## ğŸ“Š ××˜×¨×™×¦×ª ×‘×“×™×§×•×ª ×œ×¤×™ Pod

### MongoDB Pod Resilience

| ×ª×¨×—×™×© | ×¤×¢×•×œ×” | ×¦×™×¤×™×™×” | Priority |
|-------|-------|--------|----------|
| Pod Deletion | `kubectl delete pod mongodb-xxx` | Pod recreated, connection restored | P0 |
| Scale Down to 0 | `scale deployment mongodb 0` | 503 errors, no crashes | P0 |
| Pod Crash | Kill container process | Pod restart, data intact | P1 |
| Network Isolation | Block network to pod | Connection errors, retry logic | P2 |
| Resource Exhaustion | Exceed memory limits | Pod restart, monitoring alerts | P2 |

**×‘×“×™×§×•×ª × ×“×¨×©×•×ª:**
1. âœ… MongoDB pod deletion - verify recreation
2. âœ… MongoDB scale down to 0 - verify 503 errors
3. âœ… MongoDB pod restart - verify data persistence
4. âœ… MongoDB outage during job creation - verify graceful failure
5. âœ… MongoDB outage during live streaming - verify behavior
6. âœ… MongoDB recovery - verify system restoration

---

### RabbitMQ Pod Resilience

| ×ª×¨×—×™×© | ×¤×¢×•×œ×” | ×¦×™×¤×™×™×” | Priority |
|-------|-------|--------|----------|
| Pod Deletion | `kubectl delete pod rabbitmq-panda-0` | Pod recreated, queue intact | P0 |
| Scale Down to 0 | `scale statefulset rabbitmq-panda 0` | 503 errors, no crashes | P0 |
| Pod Crash | Kill container process | Pod restart, messages preserved | P1 |
| Network Isolation | Block network to pod | Connection errors, retry logic | P2 |
| Resource Exhaustion | Exceed memory limits | Pod restart, queue intact | P2 |

**×‘×“×™×§×•×ª × ×“×¨×©×•×ª:**
1. âœ… RabbitMQ pod deletion - verify recreation
2. âœ… RabbitMQ scale down to 0 - verify 503 errors
3. âœ… RabbitMQ pod restart - verify message persistence
4. âœ… RabbitMQ outage during job creation - verify graceful failure
5. âœ… RabbitMQ outage during ROI commands - verify behavior
6. âœ… RabbitMQ recovery - verify system restoration

---

### Focus Server Pod Resilience

| ×ª×¨×—×™×© | ×¤×¢×•×œ×” | ×¦×™×¤×™×™×” | Priority |
|-------|-------|--------|----------|
| Pod Deletion | `kubectl delete pod panda-panda-focus-server-xxx` | Pod recreated, jobs continue | P0 |
| Scale Down to 0 | `scale deployment panda-panda-focus-server 0` | 503 errors, no crashes | P0 |
| Pod Crash | Kill container process | Pod restart, active jobs handled | P1 |
| Network Isolation | Block network to pod | Connection errors, retry logic | P2 |
| Resource Exhaustion | Exceed memory limits | Pod restart, jobs preserved | P2 |

**×‘×“×™×§×•×ª × ×“×¨×©×•×ª:**
1. âœ… Focus Server pod deletion - verify recreation
2. âœ… Focus Server scale down to 0 - verify 503 errors
3. âœ… Focus Server pod restart - verify active jobs
4. âœ… Focus Server outage during job creation - verify graceful failure
5. âœ… Focus Server outage during live streaming - verify behavior
6. âœ… Focus Server recovery - verify system restoration

---

### SEGY Recorder Pod Resilience

| ×ª×¨×—×™×© | ×¤×¢×•×œ×” | ×¦×™×¤×™×™×” | Priority |
|-------|-------|--------|----------|
| Pod Deletion | `kubectl delete pod panda-panda-segy-recorder-xxx` | Pod recreated, recordings continue | P1 |
| Scale Down to 0 | `scale deployment panda-panda-segy-recorder 0` | Recording stops, no crashes | P1 |
| Pod Crash | Kill container process | Pod restart, recordings resume | P2 |
| Network Isolation | Block network to pod | Recording errors, retry logic | P3 |
| Resource Exhaustion | Exceed memory limits | Pod restart, recordings preserved | P3 |

**×‘×“×™×§×•×ª × ×“×¨×©×•×ª:**
1. âœ… SEGY Recorder pod deletion - verify recreation
2. âœ… SEGY Recorder scale down to 0 - verify recording stops
3. âœ… SEGY Recorder pod restart - verify recording persistence
4. âœ… SEGY Recorder outage during recording - verify behavior
5. âœ… SEGY Recorder recovery - verify recording restoration

---

## ğŸ”„ ×ª×¨×—×™×©×™ ×›×©×œ ××¨×•×‘×™× (Multiple Failures)

### 2 Pods Down Simultaneously

| Pods | ×ª×¨×—×™×© | ×¦×™×¤×™×™×” | Priority |
|------|-------|--------|----------|
| MongoDB + RabbitMQ | Both down | Complete outage, clear errors | P1 |
| MongoDB + Focus Server | Both down | Complete outage, clear errors | P1 |
| RabbitMQ + Focus Server | Both down | Complete outage, clear errors | P1 |
| Focus Server + SEGY Recorder | Both down | Jobs fail, recordings stop | P2 |

**×‘×“×™×§×•×ª × ×“×¨×©×•×ª:**
1. âœ… Multiple pods down - verify error handling
2. âœ… Multiple pods recovery - verify restoration order
3. âœ… Cascading failures - verify no infinite loops

---

## ğŸ“ ××‘× ×” ×”×˜×¡×˜×™× ×”××•×¦×¢

### ××‘× ×” ×§×‘×¦×™×
```
tests/infrastructure/resilience/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_mongodb_pod_resilience.py
â”œâ”€â”€ test_rabbitmq_pod_resilience.py
â”œâ”€â”€ test_focus_server_pod_resilience.py
â”œâ”€â”€ test_segy_recorder_pod_resilience.py
â”œâ”€â”€ test_multiple_pods_resilience.py
â””â”€â”€ test_pod_recovery_scenarios.py
```

### Test Classes Structure

```python
# Example structure for each pod
class TestMongoDBPodResilience:
    """MongoDB pod resilience tests."""
    
    def test_mongodb_pod_deletion_recreation(self):
        """Test MongoDB pod deletion and automatic recreation."""
        pass
    
    def test_mongodb_scale_down_to_zero(self):
        """Test MongoDB scale down to 0 replicas."""
        pass
    
    def test_mongodb_pod_restart_during_job_creation(self):
        """Test MongoDB pod restart during job creation."""
        pass
    
    def test_mongodb_outage_graceful_degradation(self):
        """Test graceful degradation when MongoDB is down."""
        pass
    
    def test_mongodb_recovery_after_outage(self):
        """Test system recovery after MongoDB outage."""
        pass
```

---

## ğŸ› ï¸ Helper Functions × ×“×¨×©×•×ª

### ×‘-KubernetesManager

```python
def get_pod_by_name(self, pod_name: str, namespace: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Get pod by exact name."""
    pass

def wait_for_pod_ready(self, pod_name: str, namespace: Optional[str] = None, timeout: int = 120) -> bool:
    """Wait for pod to become ready."""
    pass

def wait_for_pod_deletion(self, pod_name: str, namespace: Optional[str] = None, timeout: int = 60) -> bool:
    """Wait for pod to be deleted."""
    pass

def get_pod_status(self, pod_name: str, namespace: Optional[str] = None) -> str:
    """Get current pod status."""
    pass

def restart_pod(self, pod_name: str, namespace: Optional[str] = None) -> bool:
    """Restart a pod by deleting it (Kubernetes will recreate)."""
    pass

def scale_statefulset(self, statefulset_name: str, replicas: int, namespace: Optional[str] = None) -> bool:
    """Scale StatefulSet (for RabbitMQ)."""
    pass
```

---

## ğŸ“ Test Template

### Template ×œ×›×œ ×ª×¨×—×™×©

```python
@pytest.mark.infrastructure
@pytest.mark.resilience
@pytest.mark.kubernetes
@pytest.mark.slow
class TestMongoDBPodResilience:
    """MongoDB pod resilience tests."""
    
    def test_mongodb_pod_deletion_recreation(
        self,
        k8s_manager: KubernetesManager,
        mongodb_manager: MongoDBManager,
        focus_server_api: FocusServerAPI
    ):
        """
        Test: MongoDB Pod Deletion and Recreation
        
        Steps:
        1. Get current MongoDB pod name
        2. Verify MongoDB is accessible
        3. Delete MongoDB pod
        4. Wait for pod deletion
        5. Wait for new pod to be created
        6. Wait for new pod to be ready
        7. Verify MongoDB connection restored
        8. Verify system functionality restored
        
        Expected:
        - Pod deleted successfully
        - New pod created automatically
        - New pod becomes ready
        - MongoDB connection restored
        - System functionality restored
        """
        logger.info("=" * 80)
        logger.info("TEST: MongoDB Pod Deletion and Recreation")
        logger.info("=" * 80)
        
        # Step 1: Get current MongoDB pod
        namespace = k8s_manager.k8s_config.get("namespace", "panda")
        pods = k8s_manager.get_pods(namespace=namespace, label_selector="app=mongodb")
        assert len(pods) > 0, "MongoDB pod not found"
        original_pod = pods[0]
        original_pod_name = original_pod['name']
        
        logger.info(f"Original MongoDB pod: {original_pod_name}")
        
        # Step 2: Verify MongoDB is accessible
        assert mongodb_manager.connect(), "MongoDB should be accessible before deletion"
        logger.info("âœ… MongoDB accessible before deletion")
        mongodb_manager.disconnect()
        
        # Step 3: Delete MongoDB pod
        logger.info(f"\nDeleting MongoDB pod '{original_pod_name}'...")
        assert k8s_manager.delete_pod(original_pod_name, namespace=namespace), \
            "Failed to delete MongoDB pod"
        logger.info("âœ… MongoDB pod deleted")
        
        # Step 4: Wait for pod deletion
        logger.info("Waiting for pod deletion...")
        deleted = False
        for attempt in range(30):  # 30 seconds timeout
            pods = k8s_manager.get_pods(namespace=namespace, label_selector="app=mongodb")
            if not any(p['name'] == original_pod_name for p in pods):
                deleted = True
                logger.info("âœ… Pod deleted")
                break
            time.sleep(1)
        
        assert deleted, f"Pod {original_pod_name} not deleted within 30 seconds"
        
        # Step 5: Wait for new pod to be created
        logger.info("Waiting for new pod to be created...")
        new_pod_name = None
        for attempt in range(60):  # 60 seconds timeout
            pods = k8s_manager.get_pods(namespace=namespace, label_selector="app=mongodb")
            if pods:
                new_pod = pods[0]
                if new_pod['name'] != original_pod_name:
                    new_pod_name = new_pod['name']
                    logger.info(f"âœ… New pod created: {new_pod_name}")
                    break
            time.sleep(1)
        
        assert new_pod_name, "New MongoDB pod not created within 60 seconds"
        
        # Step 6: Wait for new pod to be ready
        logger.info(f"Waiting for pod '{new_pod_name}' to be ready...")
        ready = False
        for attempt in range(120):  # 120 seconds timeout
            pod = k8s_manager.get_pod_by_name(new_pod_name, namespace=namespace)
            if pod and pod.get('status') == 'Running' and pod.get('ready') == 'True':
                ready = True
                logger.info("âœ… Pod is ready")
                break
            time.sleep(1)
        
        assert ready, f"Pod {new_pod_name} not ready within 120 seconds"
        
        # Step 7: Verify MongoDB connection restored
        logger.info("Verifying MongoDB connection...")
        connection_restored = False
        for attempt in range(30):  # 30 seconds timeout
            if mongodb_manager.connect():
                connection_restored = True
                logger.info("âœ… MongoDB connection restored")
                mongodb_manager.disconnect()
                break
            time.sleep(1)
        
        assert connection_restored, "MongoDB connection not restored within 30 seconds"
        
        # Step 8: Verify system functionality restored
        logger.info("Verifying system functionality...")
        # Try to create a job to verify system works
        try:
            config = {
                "displayTimeAxisDuration": 10,
                "nfftSelection": 1024,
                "displayInfo": {"height": 1000},
                "channels": {"min": 1, "max": 10},
                "frequencyRange": {"min": 0, "max": 500},
                "start_time": None,
                "end_time": None,
                "view_type": 0
            }
            response = focus_server_api.configure_streaming_job(ConfigureRequest(**config))
            logger.info(f"âœ… System functionality restored - job created: {response.job_id}")
        except Exception as e:
            logger.warning(f"âš ï¸  System functionality test failed: {e}")
            # Don't fail the test - MongoDB is restored, system may need more time
        
        logger.info("=" * 80)
        logger.info("âœ… TEST PASSED: MongoDB Pod Deletion and Recreation")
        logger.info("=" * 80)
```

---

## ğŸ¯ ×¡×“×¨ ×¢×“×™×¤×•×™×•×ª ×œ×™×™×©×•×

### Phase 1: Critical Pods (P0) - ×©×‘×•×¢ 1
1. âœ… MongoDB pod resilience tests
2. âœ… RabbitMQ pod resilience tests
3. âœ… Focus Server pod resilience tests

### Phase 2: Secondary Pods (P1) - ×©×‘×•×¢ 2
4. âœ… SEGY Recorder pod resilience tests
5. âœ… Multiple pods resilience tests

### Phase 3: Advanced Scenarios (P2-P3) - ×©×‘×•×¢ 3
6. âœ… Network isolation tests
7. âœ… Resource exhaustion tests
8. âœ… Recovery scenarios tests

---

## ğŸ“Š Metrics ×•-Monitoring

### Metrics ×œ×‘×“×™×§×”
- Pod restart count
- Pod ready time (time to become ready after restart)
- Service downtime duration
- Error rate during outage
- Recovery time

### Monitoring Points
- Pod status changes
- Deployment/StatefulSet events
- Service endpoint availability
- Application logs during outage
- System resource usage

---

## âš ï¸ ××–×”×¨×•×ª ×•×”×’×‘×œ×•×ª

### ×”×’×‘×œ×•×ª
1. **Production Environment:** ×œ× ×œ×”×¨×™×¥ ×˜×¡×˜×™× ××œ×” ×‘-production
2. **Data Loss:** ×œ×‘×“×•×§ ×©××™×Ÿ data loss ×‘××”×œ×š pod failures
3. **Cleanup:** ×œ×•×•×“× ×©×›×œ ×”-pods ××•×—×–×¨×™× ×œ××¦×‘ ×ª×§×™×Ÿ ×‘×¡×•×£ ×”×˜×¡×˜×™×
4. **Timeouts:** ×œ×”×’×“×™×¨ timeouts ××ª××™××™× ×œ×›×œ ×¤×¢×•×œ×”

### Best Practices
1. **Isolation:** ×›×œ ×˜×¡×˜ ×¦×¨×™×š ×œ×”×™×•×ª independent
2. **Cleanup:** ×ª××™×“ ×œ× ×§×•×ª ×‘×¡×•×£ ×”×˜×¡×˜ (restore pods)
3. **Verification:** ×œ×•×•×“× ×©×”××¢×¨×›×ª ×—×–×¨×” ×œ××¦×‘ ×ª×§×™×Ÿ ×œ×¤× ×™ ×¡×™×•×
4. **Logging:** ×œ×•×’ ××¤×•×¨×˜ ×©×œ ×›×œ ×©×œ×‘

---

## ğŸ”— ×§×™×©×•×¨×™× ×¨×œ×•×•× ×˜×™×™×

- `src/infrastructure/kubernetes_manager.py` - Kubernetes operations
- `src/infrastructure/mongodb_manager.py` - MongoDB operations
- `tests/infrastructure/test_rabbitmq_outage_handling.py` - Existing RabbitMQ tests
- `tests/infrastructure/test_k8s_job_lifecycle.py` - Existing K8s tests

---

## ğŸ“… Timeline

| Phase | Duration | Tasks |
|-------|----------|-------|
| **Phase 1** | 1 week | MongoDB, RabbitMQ, Focus Server resilience tests |
| **Phase 2** | 1 week | SEGY Recorder, Multiple pods tests |
| **Phase 3** | 1 week | Advanced scenarios, Network isolation, Resource exhaustion |
| **Total** | 3 weeks | Complete resilience test suite |

---

**×ª×•×›× ×™×ª ×–×• ××¡×¤×§×ª ×‘×¡×™×¡ ××§×™×£ ×œ×‘×“×™×§×ª resilience ×©×œ Kubernetes pods ×‘××¢×¨×›×ª.**

