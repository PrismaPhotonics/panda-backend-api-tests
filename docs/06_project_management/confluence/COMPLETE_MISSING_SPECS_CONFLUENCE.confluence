h1. Missing Specifications - Complete Report

||Document Type||Technical Specification Gap Analysis||
||Date||October 22, 2025||
||Scope||Backend Testing & Automation Framework||
||Status||{status:colour=Red|title=URGENT|subtle=false} *Blocking 82+ tests*||

----

h2. Executive Summary

{info:title=Overview}
This document catalogs *19 critical specification gaps* affecting the Focus Server automation framework. These gaps prevent proper test validation, allowing bugs to reach production and creating false confidence in test results.
{info}

h3. Impact Overview

* *82+ automated tests* cannot properly validate quality
* *50+ hardcoded values* never confirmed by product team
* *28 performance tests* have disabled assertions
* *Multiple critical endpoints* lack defined behavior

{warning:title=Risk}
Without these specifications, our test suite provides false confidence - tests pass but don't catch real issues.
{warning}

----

h2. ðŸ”´ Critical Priority Issues (Immediate Action Required)

----

h3. ðŸ”´ Issue #1: Performance Assertions Disabled

{panel:title=Overview|borderColor=#dc3545}
*Category:* Performance Testing
*Tests Affected:* 28 tests
*Code Location:* {{tests/integration/performance/test_performance_high_priority.py:146-170}}
{panel}

h4. The Problem

Performance tests measure P95/P99 latency and error rates, but *all assertions are disabled*. Tests collect metrics but never fail, even if performance is terrible.

{panel:title=Current Code Behavior|bgColor=#fff3cd}
{code:python}
# TODO: Uncomment after specs meeting
# assert p95 < THRESHOLD_P95_MS   # âŒ DISABLED!
# assert p99 < THRESHOLD_P99_MS   # âŒ DISABLED!

# Only logs warning instead of failing
if p95 >= THRESHOLD_P95_MS:
    logger.warning(f"Would fail: P95={p95}ms")
{code}
{panel}

h4. What's Missing

||Endpoint||Metric||Current Guess||Need Decision||
|POST /config|P95 latency|500ms|*?*|
|POST /config|P99 latency|1000ms|*?*|
|POST /config|Error rate|5%|*?*|
|GET /metadata|P95 latency|\-|*?*|
|GET /channels|Response time|1000ms|*?*|

h4. Business Impact

{panel:title=Real Scenario|borderColor=#dc3545|bgColor=#f8d7da}
Developer adds N+1 query bug:
* Before: P95 = 150ms (/)
* After: P95 = 1200ms (x)
* Test Result: PASS (/) (assertions disabled!)
* *â†’ Bug goes to production*
{panel}

h4. Questions to Answer

# What are acceptable P95/P99 latency thresholds for each endpoint?
# What's the maximum error rate before we should fail?
# What's the measurement window/sample size?

{note}
*Documentation:* See {{CONFLUENCE_SPECS_MEETING.md:46-85}}
{note}

----

h3. ðŸ”´ Issue #2: ROI Change Limit \- Hardcoded 50%

{panel:title=Overview|borderColor=#dc3545}
*Category:* Data Validation
*Tests Affected:* 6 tests
*Code Location:* {{src/utils/validators.py:395}}
{panel}

h4. The Problem

Code has hardcoded {{max_change_percent = 50.0}} that was *never confirmed* by product team. This value determines whether user's ROI change requests are accepted or rejected.

{code:python}
def validate_roi_change_safety(
    old_roi: ROIConfig,
    new_roi: ROIConfig,
    max_change_percent: float = 50.0  # âŒ NEVER CONFIRMED!
):
    """Rejects if ROI change > 50%"""
{code}

h4. What's Missing

# *Confirmation:* Is 50% correct? Or should it be 30%? 70%? Different value?
# *Cooldown period:* Is there a time delay between consecutive changes?
# *On-exceed behavior:* What happens \- reject? warn? throttle?

h4. Business Impact

{panel:title=Scenario A - Too Restrictive|bgColor=#fff3cd}
* User wants: 100 â†’ 160 sensors (60% change)
* System: REJECTED (x)
* Impact: User frustration, legitimate use blocked
{panel}

{panel:title=Scenario B - Too Permissive|bgColor=#fff3cd}
* User does: 2000 â†’ 1020 sensors (49% change)
* System: ACCEPTED (/)
* Impact: Data discontinuity, quality issues
{panel}

*No one knows which scenario is correct!*

h4. Affected Tests

* {{test_roi_sensor_change_within_limit}}
* {{test_roi_sensor_change_exceeds_limit}}
* {{test_roi_frequency_change_validation}}
* {{test_roi_channel_change_validation}}
* {{test_roi_combined_changes}}
* {{test_roi_change_cooldown_period}}

----

h3. ðŸ”´ Issue #3: NFFT Validation Accepts Any Value

{panel:title=Overview|borderColor=#dc3545}
*Category:* Data Validation
*Tests Affected:* 6 tests
*Code Location:* {{src/utils/validators.py:194-227}}
{panel}

h4. The Problem

{warning:title=Code vs Config Mismatch}
*Config file says:* Valid NFFT = \[256, 512, 1024, 2048\]
*Code does:* Accepts ANY positive integer, only warns
{warning}

{code:python}
def validate_nfft_value(nfft: int) -> bool:
    if not is_power_of_2(nfft):
        warnings.warn(f"NFFT={nfft} not power of 2")  # âš ï¸ Only warns
    return True  # âœ… Always passes!
{code}

h4. What's Missing

# Should code *enforce* the config list? (reject invalid values)
# Or keep *warning-only* behavior? (accept but warn)
# What's the absolute maximum NFFT? (currently unlimited)
# Can users override with custom values?

h4. Real Examples

{panel:title=Case 1: NFFT=3000 (not power of 2)|bgColor=#ffe5d0}
* Validation: PASS (/)
* Result: FFT runs 5\-10x slower
* Impact: Performance degradation not caught
{panel}

{panel:title=Case 2: NFFT=16384 (huge, but power of 2)|bgColor=#ffe5d0}
* Validation: PASS (/)
* Memory: ~64MB per request
* Impact: 100 requests = 6.4GB â†’ OOM crash
{panel}

----

h3. ðŸ”´ Issue #8: MongoDB Outage Behavior Undefined

{panel:title=Overview|borderColor=#dc3545}
*Category:* Reliability & Error Handling
*Tests Affected:* 5 tests
*Code Location:* Test failures in integration suite
{panel}

h4. The Problem

Tests fail because *expected behavior during MongoDB outage is not specified*:

{panel:title=Test Failures|borderColor=#dc3545|bgColor=#f8d7da}
{noformat}
Expected: HTTP 503 (Service Unavailable)
Actual:   HTTP 500 (Internal Server Error)

Expected: Recovery in 5 seconds
Actual:   Recovery takes 15 seconds
{noformat}
{panel}

h4. What's Missing

# *HTTP status code:* Should system return 503 or 500 during DB outage?
# *Recovery time:* Maximum time to recover after DB comes back?
# *Caching policy:* Should system cache data during outage?
# *User message:* What error message to show?

h4. Business Impact

||Without Spec||With Spec||
|Tests can't verify correct behavior|Clear behavior definition|
|Production incidents have unclear expectations|Tests can validate properly|
|SLA for recovery time undefined|Operations knows expected recovery time|

----

h3. ðŸ”´ Issue #10: SingleChannel API Returns 422 for All Requests

{panel:title=Overview|borderColor=#dc3545}
*Category:* API Contract
*Tests Affected:* 11 out of 13 tests (85% failure rate)
*Code Location:* {{tests/integration/api/test_singlechannel_view_mapping.py}}
{panel}

h4. The Problem

*POST /configure endpoint returns 422 (Unprocessable Entity) for ALL requests.* Not clear if:
* Endpoint is not implemented yet
* Payload format is wrong
* Feature is disabled

{panel:title=Test Results|borderColor=#dc3545|bgColor=#f8d7da}
13 total tests:
* 11 FAIL (422 status code)
* 2 PASS (basic checks)
* *Failure rate: 85%*
{panel}

h4. What's Missing

# *Is endpoint valid?* Should it exist or is it deprecated?
# *Payload format:* What's the correct request structure?
# *Feature status:* Is SingleChannel view implemented?
# *Expected behavior:* What should a valid request return?

h4. Business Impact

{tip:title=Need Clarity}
* If implemented â†’ need correct payload format
* If not implemented â†’ need to skip/remove tests
* If deprecated â†’ need to update documentation
{tip}

----

h2. ðŸŸ  High Priority Issues (Next Sprint)

----

h3. ðŸŸ  Issue #4: Frequency Range \- No Absolute Limits

{panel:title=Overview|borderColor=#fd7e14}
*Category:* Data Validation
*Tests Affected:* 16 tests
*Code Location:* {{src/models/focus_server_models.py:46-57}}
{panel}

h4. The Problem

{code:python}
class FrequencyRange(BaseModel):
    min_freq: float = Field(gt=0)  # Only: must be > 0
    max_freq: float = Field(gt=0)  # Only: must be > 0
    # âŒ No absolute maximum!
    # âŒ No minimum span!
{code}

{warning}
*Accepts extreme values:*
* min_freq = 0.0001 Hz (/) (too low, impractical)
* max_freq = 999999999 Hz (/) (999 MHz, impossible)
* min_freq = max_freq (/) (single frequency \- valid?)
{warning}

h4. What's Missing

# Absolute maximum frequency? (e.g., 48000 Hz for audio?)
# Absolute minimum frequency? (e.g., 1 Hz?)
# Minimum span required? (e.g., max \- min >= 10 Hz?)
# Is single frequency (min == max) valid?

----

h3. ðŸŸ  Issue #5: Sensor Range \- No Min/Max ROI Size

{panel:title=Overview|borderColor=#fd7e14}
*Category:* Data Validation
*Tests Affected:* 15 tests
*Code Location:* {{src/utils/validators.py:116-151}}
{panel}

h4. The Problem

System has 2222 total sensors, but validation only checks:
* (/) min_sensor >= 1
* (/) max_sensor <= 2222
* (/) min <= max

*What it DOESN'T check:*
* (x) ROI size too small (e.g., single sensor)
* (x) ROI size too large (e.g., all 2222 sensors)

h4. Edge Cases Without Specs

{panel:title=Case 1: Single Sensor ROI|bgColor=#fff7e7}
{{min_sensor = 500, max_sensor = 500}}
Validation: PASS (/)
*Question:* Is this a valid use case?
{panel}

{panel:title=Case 2: Entire Cable|bgColor=#fff7e7}
{{min_sensor = 1, max_sensor = 2222}}
Validation: PASS (/)
Impact: Enormous data rate, high CPU/memory
*Question:* Should this be limited?
{panel}

----

h3. ðŸŸ  Issue #9: RabbitMQ Commands \- No Timeouts

{panel:title=Overview|borderColor=#fd7e14}
*Category:* External Integration
*Tests Affected:* 8 tests
*Code Location:* {{src/external/rabbitmq/}}
{panel}

h4. The Problem

RabbitMQ command execution has *no timeout or retry logic*.

{code:python}
def execute_rabbitmq_command(cmd):
    result = subprocess.run(cmd, shell=True)  # âŒ No timeout!
    return result
    # If RabbitMQ hangs â†’ test hangs forever
{code}

h4. What's Missing

# *Command timeouts:* How long to wait before giving up?
# *Retry logic:* Should we retry failed commands?
# *Exponential backoff:* Wait between retries?
# *Circuit breaker:* Stop trying after N failures?

----

h3. ðŸŸ  Issue #11: Live/Historical Threshold \- 1 Hour Hardcoded

{panel:title=Overview|borderColor=#fd7e14}
*Category:* Business Logic
*Code Location:* {{src/utils/helpers.py:200-220}}
{panel}

h4. The Problem

Code has hardcoded {{LIVE_HISTORICAL_THRESHOLD = 3600}} (1 hour) that determines whether to use "live" or "historical" mode.

{code:python}
def get_data_mode(timestamp):
    if time.time() - timestamp < 3600:  # âŒ Hardcoded 1 hour
        return "live"
    return "historical"
{code}

h4. What's Missing

# Confirmation: Is 1 hour correct?
# Or should it be: 30 min? 2 hours? Configurable?
# Does this affect performance/resource usage?

----

h3. ðŸŸ  Issue #15: Data Quality \- No Validation Limits

{panel:title=Overview|borderColor=#fd7e14}
*Category:* Data Quality
*Code Location:* {{src/utils/validators.py:229-324}}
{panel}

h4. The Problem

No validation for data quality metrics:
* No amplitude range limits (too high/low values accepted)
* No missing data percentage threshold
* No signal\-to\-noise ratio requirements

h4. What's Missing

# *Amplitude limits:* Min/max acceptable values?
# *Missing data:* Maximum percentage of missing samples?
# *SNR requirements:* Minimum signal quality?
# *Outlier detection:* When to reject anomalous data?

----

h3. ðŸŸ  Issue #16: Error Handling \- HTTP Status Semantics Unclear

{panel:title=Overview|borderColor=#fd7e14}
*Category:* API Contract
*Code Location:* Multiple API files
{panel}

h4. The Problem

Not clear when to use each HTTP status code:

||Status||Current Usage||Question||
|200 OK|Data available|What if data exists but empty?|
|208 Already Reported|???|When to use this?|
|400 Bad Request|Validation failed|Clear (/)|
|422 Unprocessable|???|vs 400?|
|500 Internal Error|System error|Clear (/)|
|503 Service Unavailable|???|When to use vs 500?|

----

h2. ðŸŸ¡ Medium Priority Issues (Future Consideration)

h3. Summary of Medium Priority

||Issue||Category||Tests||
|#6: API Timeouts Arbitrary|Performance|3|
|#7: Config Edge Cases No Assertions|API Contract|8|
|#12: Polling Helper Hardcoded|Test Infrastructure|Multiple|
|#13: Default Values Mismatch|Configuration|Multiple|
|#17: Time Validation No Limits|Data Validation|Multiple|
|#18: Task Lifecycle No Cleanup|Resource Management|\-|

{tip:title=Note}
See individual sections in full document for details on each medium priority issue.
{tip}

----

h2. ðŸ”µ Low Priority Issues (Documentation Only)

||Issue||Category||Priority||
|#19: K8s Resource Limits|Infrastructure|Low|
|#20: Security No Authentication|Security|Low|

----

h2. ðŸ“Š Summary Statistics

h3. By Priority

||Priority||Count||Tests Affected||
|{status:colour=Red|title=Critical}|5|56+|
|{status:colour=Yellow|title=High}|8|39+|
|{status:colour=Grey|title=Medium}|4|15+|
|{status:colour=Blue|title=Low}|2|\-|
|*Total*|*19*|*110+*|

h3. By Category

||Category||Issues||Tests Affected||
|Data Validation|7|48|
|Performance|3|31|
|API Contract|3|22|
|Configuration|2|8+|
|External Integration|2|13|
|Infrastructure|1|\-|
|Security|1|\-|

h3. Top 5 Most Impactful

# *Performance Assertions Disabled* \- 28 tests can't validate
# *Frequency Range No Limits* \- 16 tests blocked
# *Sensor Range No Limits* \- 15 tests blocked
# *SingleChannel API Fails* \- 11 tests failing (85%)
# *RabbitMQ No Timeouts* \- 8 tests can hang

----

h2. ðŸ“‹ Recommended Action Plan

h3. Phase 1: Critical Issues (Week 1)

{panel:title=Meeting Agenda (2-3 hours)|bgColor=#e7f3ff}
# Performance thresholds (Issue #1) \- 30 min
# ROI change limit (Issue #2) \- 20 min
# NFFT validation (Issue #3) \- 20 min
# MongoDB behavior (Issue #8) \- 15 min
# SingleChannel endpoint (Issue #10) \- 15 min
{panel}

{tip:title=Deliverables}
* (/) Documented decisions for all 5 issues
* (/) Numeric values for thresholds
* (/) Clear behavior definitions
{tip}

h3. Phase 2: Implementation (Week 2\-3)

# Update {{src/utils/validators.py}}
# Update {{config/settings.yaml}}
# Enable assertions in performance tests
# Add missing validations
# Run full test suite

h3. Phase 3: High Priority (Week 4)

Address remaining high priority issues:
* Frequency/sensor ranges (Issues #4, #5)
* RabbitMQ timeouts (Issue #9)
* Live/historical threshold (Issue #11)

h3. Phase 4: Medium Priority (Following Month)

* API timeouts and edge cases (Issues #6, #7)
* Polling and defaults (Issues #12, #13)
* Data quality validation (Issue #15)

----

h2. ðŸ“Ž Supporting Documentation

h3. Key Documents

# *Confluence Specs Meeting Doc:* {{CONFLUENCE_SPECS_MEETING.md}}
  \- Detailed breakdown of top 7 critical issues
  \- Questions to answer in specs meeting

# *Critical Specs List:* {{CRITICAL_MISSING_SPECS_LIST.md}}
  \- Complete catalog of 200+ missing specs
  \- YAML format with all details

# *Code Evidence:* {{CODE_EVIDENCE_MISSING_SPECS.md}}
  \- Direct code examples
  \- Line\-by\-line analysis

# *Test Results:* {{documentation/testing/}}
  \- SingleChannel test results
  \- MongoDB issues documentation

h3. Code Files to Review

Primary files needing updates:
* {{src/utils/validators.py}} \- validation functions
* {{src/models/focus_server_models.py}} \- data models
* {{config/settings.yaml}} \- configuration
* {{tests/integration/performance/}} \- performance tests
* {{tests/integration/api/}} \- API tests

----

h2. â“ Pre\-Meeting Preparation

h3. For Product Team

{info:title=Please Review}
# *Performance expectations* \- What are acceptable response times?
# *Business rules* \- ROI change limits, data quality standards
# *Feature status* \- Is SingleChannel implemented?
{info}

h3. For Development Team

{info:title=Please Prepare}
# *Technical constraints* \- What's feasible? (e.g., NFFT limits)
# *Current behavior* \- How does system behave now?
# *Implementation effort* \- Time estimates for each fix
{info}

h3. For QA Team

{info:title=Please Provide}
# *Test coverage* \- Which tests are affected?
# *False positive/negative* \- Current pain points
# *Priority feedback* \- Which specs are most blocking?
{info}

----

h2. ðŸŽ¯ Success Criteria

{tip:title=Meeting is Successful When}
* (/) All critical issues (1\-5, 8, 10) have documented decisions
* (/) Numeric values assigned for all thresholds
* (/) Clear yes/no for all edge cases (min==max, etc.)
* (/) Action items assigned with owners
{tip}

{tip:title=Implementation is Complete When}
* (/) All affected tests can pass/fail correctly
* (/) No TODO comments waiting for specs
* (/) Documentation updated in Xray/Jira
* (/) Test suite provides reliable quality gate
{tip}

----

{info}
*Document Version:* 1.0
*Last Updated:* October 22, 2025
*Contact:* QA Automation Team
*Status:* {status:colour=Green|title=Ready for Meeting}
{info}

