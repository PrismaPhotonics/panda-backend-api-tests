# ×˜×¡×˜ 4: MongoDB Recovery â€“ Recordings Indexed After Outage
## PZ-13687 - × ×™×ª×•×— ××§×™×£ ×•××¢××™×§

---

## ğŸ“‹ ×ª×§×¦×™×¨ ××”×™×¨ ×œ×¤×’×™×©×” (Quick Brief)

| **×©×“×”** | **×¢×¨×š** |
|---------|---------|
| **Jira ID** | PZ-13687 |
| **×©× ×”×˜×¡×˜** | MongoDB Recovery â€“ Recordings Indexed After Outage |
| **×¢×“×™×¤×•×ª** | ğŸ”´ **CRITICAL** |
| **×¡×•×’** | Infrastructure / Resilience / Recovery Test |
| **×¡×˜×˜×•×¡ ××•×˜×•××¦×™×”** | âš ï¸ **Partial** (Requires Kubernetes) |
| **××©×š ×¨×™×¦×” ×¦×¤×•×™** | ~30-60 ×©× ×™×•×ª (+ recovery time) |
| **××•×¨×›×‘×•×ª ××™××•×©** | ğŸ”´ **×××•×“ ×’×‘×•×”×”** |
| **×§×•×‘×¥ ×˜×¡×˜** | `tests/performance/test_mongodb_outage_resilience.py` |
| **Test Class** | `TestMongoDBOutageResilience` |
| **×©×•×¨×•×ª** | 386-451 |
| **×ª×œ×•×™×•×ª** | Kubernetes, MongoDB, Focus Server API |

---

## ğŸ¯ ××” ×”××˜×¨×” ×©×œ ×”×˜×¡×˜? (Test Objectives)

### ××˜×¨×” ××¡×˜×¨×˜×’×™×ª (Strategic Goal):
×œ×•×•×“× ×©×”××¢×¨×›×ª **×œ× ×××‘×“×ª × ×ª×•× ×™×** ×•**××ª××•×©×©×ª ××•×˜×•××˜×™×ª** ×œ××—×¨ ×”×¤×¡×§×ª MongoDB. ×¡×¤×¦×™×¤×™×ª - recordings ×©× ×•×¡×¤×• **×‘××”×œ×š ×”×”×¤×¡×§×”** ×¦×¨×™×›×™× ×œ×”×™×•×ª **××™× ×“×§×¡×™× ××•×˜×•××˜×™×ª** ××—×¨×™ ×”×”×ª××•×©×©×•×ª.

### ××˜×¨×•×ª ×¡×¤×¦×™×¤×™×•×ª (Specific Goals):
1. **××™××•×ª recovery mechanism** - ×”×× ×™×© ×× ×’× ×•×Ÿ ××•×˜×•××˜×™ ×œ××™×ª×•×¨ × ×ª×•× ×™× ×—×“×©×™×?
2. **×× ×™×¢×ª data loss** - ××£ recording ×œ× "× ×¢×œ×"
3. **××™××•×ª indexing** - recordings ×—×“×©×™× ××•×¤×™×¢×™× ×‘-MongoDB
4. **×ª×™×¢×•×“ ×”×ª× ×”×’×•×ª** - ××” ×§×•×¨×” ×‘×“×™×•×§ ×‘××”×œ×š ×•××—×¨×™ outage

---

## ğŸ§ª ××” ×× ×™ ×¨×•×¦×” ×œ×‘×“×•×§? (What We're Testing)

### ×”×¡×¦× ×¨×™×• ×©×× ×—× ×• ×‘×•×“×§×™×:

**Timeline ×©×œ ×”×˜×¡×˜**:

```
Time T0:  MongoDB ×¤×•×¢×œ ×ª×§×™×Ÿ
          Focus Server ×¤×•×¢×œ ×ª×§×™×Ÿ
          ×™×© N recordings ×‘-MongoDB

Time T1:  MongoDB × ×›×‘×” (scale down to 0 replicas)
          MongoDB ×œ× ×–××™×Ÿ
          Focus Server ×œ× ×™×›×•×œ ×œ×’×©×ª ×œ-MongoDB

Time T2:  Recording ×—×“×© × ×•×¡×£ ×œ××—×¡×•×Ÿ (file system)
          Recording ×–×” ×œ× ×‘××™× ×“×§×¡ (MongoDB down!)

Time T3:  MongoDB ×—×•×–×¨ ×œ×¤×¢×•×œ×” (scale up to 1 replica)
          MongoDB ×–××™×Ÿ ×©×•×‘
          Focus Server ××ª×—×‘×¨ ×œ-MongoDB

Time T4:  Focus Server ××–×”×” recording ×—×“×© (recovery mechanism)
          Recording × ××™× ×“×§×¡ ××•×˜×•××˜×™×ª
          ×¢×›×©×™×• ×™×© N+1 recordings ×‘-MongoDB

Expected: Recording ×—×“×© ××•×¤×™×¢ ×‘-MongoDB ×œ×œ× ×”×ª×¢×¨×‘×•×ª ×™×“× ×™×ª!
```

---

## ğŸ”¥ ××” ×”× ×—×™×¦×•×ª ×©×œ ×”×˜×¡×˜? (Why Is This Critical?)

### ×¡×™×›×•× ×™× ×× ×œ× ×‘×•×“×§×™×:

#### 1ï¸âƒ£ **××™×‘×•×“ × ×ª×•× ×™×** (Data Loss)
**×ª×¨×—×™×©**:  
MongoDB × ×›×‘×” ×œ××©×š 10 ×“×§×•×ª (maintenance, crash, network issue).  
×‘××”×œ×š ×”×–××Ÿ ×”×–×”, 50 recordings ×—×“×©×™× × ×•×¡×¤×™× ×œ××—×¡×•×Ÿ.  
×œ××—×¨ ×”×”×ª××•×©×©×•×ª â†’ **50 recordings ×œ× ××•×¤×™×¢×™× ×‘××¢×¨×›×ª!**

**×”×©×¤×¢×”**:
- ××©×ª××©×™× ×œ× ×¨×•××™× ××ª ×”× ×ª×•× ×™×
- ××™×‘×•×“ ××™×“×¢ ×§×¨×™×˜×™
- ×¦×¨×™×š manual reindexing â†’ ×¢×‘×•×“×” ×™×“× ×™×ª

---

#### 2ï¸âƒ£ **×—×•×¡×¨ ×¡× ×›×¨×•×Ÿ ×‘×™×Ÿ Storage ×œ-Database** (Data Inconsistency)
**×ª×¨×—×™×©**:  
File system ××›×™×œ 1000 recordings.  
MongoDB ××›×™×œ ×¨×§ 950 recordings.  
**Gap ×©×œ 50 recordings!**

**×”×©×¤×¢×”**:
- ××™ ×¢×§×‘×™×•×ª × ×ª×•× ×™×
- ×‘×œ×‘×•×œ ××©×ª××©×™×: "××™×¤×” ×”-recordings?"
- ×§×©×” ×œ×–×”×•×ª ××” ×—×¡×¨

---

#### 3ï¸âƒ£ **Manual Recovery Required** (×¢×‘×•×“×” ×™×“× ×™×ª)
**×ª×¨×—×™×©**:  
××—×¨×™ outage, ×¦×•×•×ª DevOps ×¦×¨×™×š **×™×“× ×™×ª** ×œ×¨×•×¥ script ×œ××™× ×“×•×§×¡ recordings ×—×“×©×™×.

**×”×©×¤×¢×”**:
- ×¢×•××¡ ×¢×œ DevOps
- ×–××Ÿ downtime ××¨×•×š
- ×¡×™×›×•×Ÿ ×œ×˜×¢×•×ª ×× ×•×©

---

#### 4ï¸âƒ£ **Loss of Trust** (××•×‘×“×Ÿ ×××•×Ÿ)
**×ª×¨×—×™×©**:  
×œ×§×•×— ×¨×•××” ×©-recordings "× ×¢×œ××•" ××—×¨×™ ×ª×§×œ×”.

**×”×©×¤×¢×”**:
- ××•×‘×“×Ÿ ×××•×Ÿ ×‘××¢×¨×›×ª
- ×œ×§×•×— ×¢×•×‘×¨ ×œ××ª×—×¨×”
- ×ª×“××™×ª ×¨×¢×”

---

## ğŸ› ï¸ ××™×š ×× ×™ ×××© ××•×ª×• ×‘×§×•×“? (Code Implementation)

### ×§×•×‘×¥ ×”×˜×¡×˜:
**Path**: `tests/performance/test_mongodb_outage_resilience.py`  
**Test Class**: `TestMongoDBOutageResilience`  
**Lines**: 386-451

---

### ×§×•×“ ××œ× ×¢× ×”×¡×‘×¨×™×:

```python
@pytest.mark.integration
@pytest.mark.infrastructure
@pytest.mark.resilience
@pytest.mark.mongodb_outage
@pytest.mark.slow
def test_mongodb_outage_cleanup_and_restore(self, focus_server_api):
    """
    Test MongoDB can be restored after an outage and recordings are indexed.
    
    Test Steps:
        1. Create MongoDB outage (scale down to 0 replicas)
        2. Verify outage is active (MongoDB unreachable)
        3. Add new recording file to storage (simulates data arriving during outage)
        4. Restore MongoDB (scale up to 1 replica)
        5. Verify MongoDB is reachable
        6. Wait for Focus Server to detect and index new recording
        7. Verify new recording appears in MongoDB
        8. Verify count increased by +1
    
    Expected:
        - MongoDB recovery triggers automatic indexing of new recordings
        - No data loss occurs
        - No manual intervention required
    
    Jira: PZ-13687
    Priority: CRITICAL
    """
    test_name = "mongodb_outage_cleanup_and_restore"
    self.logger.info(f"Starting test: {test_name}")
    
    try:
        # =====================================================
        # Step 1: Baseline - Count existing recordings
        # =====================================================
        self.log_test_step("Counting existing recordings")
        
        # Connect to MongoDB
        assert self.mongodb_manager.connect(), "MongoDB is not reachable before test"
        
        # Get initial count of recordings in node4 collection
        initial_count = self.mongodb_manager.count_documents("node4", {})
        self.logger.info(f"Initial recordings count: {initial_count}")
        
        self.mongodb_manager.disconnect()
        
        # =====================================================
        # Step 2: Create MongoDB outage
        # =====================================================
        self.log_test_step("Creating MongoDB outage by scaling down")
        
        # Scale down MongoDB deployment to 0 replicas (simulates crash/maintenance)
        self.mongodb_manager.scale_down_mongodb(replicas=0)
        time.sleep(5)  # Give Kubernetes time to react
        
        # =====================================================
        # Step 3: Verify outage is active
        # =====================================================
        self.log_test_step("Verifying MongoDB outage is active")
        
        # Try to connect - should fail
        assert not self.mongodb_manager.connect(), \
            "MongoDB is still reachable after scaling down (outage not effective)"
        
        self.logger.info("âœ… MongoDB is unreachable (outage confirmed)")
        
        # =====================================================
        # Step 4: Add new recording file during outage
        # =====================================================
        self.log_test_step("Adding new recording file to storage during outage")
        
        # Simulate new recording arriving during outage
        # In real scenario: data acquisition system writes new file
        # For test: we manually create a test recording file
        
        # Generate unique recording ID
        test_recording_id = f"test_recording_{int(time.time())}"
        
        # Add recording to file system
        # Note: Actual implementation depends on storage structure
        # This is a simplified simulation
        self.logger.info(f"Simulating new recording: {test_recording_id}")
        
        # In real implementation:
        # - Create file in /data/recordings/{test_recording_id}.dat
        # - Create metadata file
        # - Ensure proper permissions
        
        # For this test, we'll mark it for tracking
        self.test_recording_id = test_recording_id
        
        # =====================================================
        # Step 5: Restore MongoDB
        # =====================================================
        self.log_test_step("Restoring MongoDB (scaling up)")
        
        # Scale up MongoDB deployment to 1 replica
        self.mongodb_manager.restore_mongodb()
        time.sleep(15)  # Give MongoDB time to fully start and become ready
        
        # =====================================================
        # Step 6: Verify MongoDB is reachable
        # =====================================================
        self.log_test_step("Verifying MongoDB is reachable after restoration")
        
        # Connect to MongoDB
        assert self.mongodb_manager.connect(), \
            "MongoDB is not reachable after restoration (recovery failed)"
        
        self.logger.info("âœ… MongoDB is reachable (recovery successful)")
        
        # =====================================================
        # Step 7: Wait for Focus Server recovery indexing
        # =====================================================
        self.log_test_step("Waiting for Focus Server to detect and index new recording")
        
        # Focus Server should have a recovery mechanism that:
        # 1. Detects MongoDB is back online
        # 2. Scans storage for new recordings
        # 3. Indexes missing recordings
        
        # Wait for recovery process (with timeout)
        max_wait_time = 60  # seconds
        check_interval = 5  # seconds
        elapsed_time = 0
        
        recording_indexed = False
        
        while elapsed_time < max_wait_time:
            # Check if new recording was indexed
            # Query MongoDB for our test recording
            
            # Note: Actual query depends on schema
            # Example: Find recording by ID or timestamp
            
            # For this simplified test, we check if count increased
            current_count = self.mongodb_manager.count_documents("node4", {})
            
            if current_count > initial_count:
                recording_indexed = True
                self.logger.info(
                    f"âœ… New recording indexed! "
                    f"Count increased from {initial_count} to {current_count}"
                )
                break
            
            # Wait before next check
            self.logger.debug(
                f"Waiting for indexing... "
                f"({elapsed_time}s/{max_wait_time}s)"
            )
            time.sleep(check_interval)
            elapsed_time += check_interval
        
        # =====================================================
        # Step 8: Verify new recording appears
        # =====================================================
        self.log_test_step("Verifying new recording appears in MongoDB")
        
        # Assert that recording was indexed
        assert recording_indexed, \
            f"New recording was not indexed within {max_wait_time}s after MongoDB recovery"
        
        # Get final count
        final_count = self.mongodb_manager.count_documents("node4", {})
        
        # Verify count increased by at least +1
        assert final_count >= initial_count + 1, \
            f"Recording count did not increase as expected: {initial_count} â†’ {final_count}"
        
        self.logger.info(f"âœ… Recording count: {initial_count} â†’ {final_count} (+{final_count - initial_count})")
        
        # =====================================================
        # Step 9: Verify no side effects from restoration
        # =====================================================
        self.log_test_step("Verifying no side effects from restoration")
        self._verify_no_side_effects(test_name)
        
        # =====================================================
        # Step 10: Cleanup
        # =====================================================
        self.log_test_step("Cleaning up test recording")
        
        # In real implementation: delete test recording file
        # For now, just log
        self.logger.info(f"Test recording {test_recording_id} should be cleaned up")
        
        # Disconnect from MongoDB
        self.mongodb_manager.disconnect()
        
        self.logger.info(f"âœ… Test completed successfully: {test_name}")
        
    except Exception as e:
        self.logger.error(f"âŒ Test failed: {test_name} - {e}")
        raise
```

---

## ğŸ“ ××” ×œ×•××“×™× ××”×˜×¡×˜ ×”×–×”?

### ×ª×•×¦××•×ª ×¦×¤×•×™×•×ª:

```
Test: mongodb_outage_cleanup_and_restore
=============================================================
Step 1: Initial recordings count: 1500
Step 2: Scaling MongoDB down to 0 replicas...
Step 3: âœ… MongoDB is unreachable (outage confirmed)
Step 4: Simulating new recording: test_recording_1697812345
Step 5: Restoring MongoDB (scaling up to 1 replica)...
Step 6: âœ… MongoDB is reachable (recovery successful)
Step 7: Waiting for Focus Server to detect and index...
        - After 10s: count = 1500 (not yet indexed)
        - After 15s: count = 1501 (indexed!)
        âœ… New recording indexed!
Step 8: âœ… Recording count: 1500 â†’ 1501 (+1)
=============================================================
âœ… Test completed successfully
```

---

### ××” ×× ×”×˜×¡×˜ × ×›×©×œ?

#### Failure Scenario 1: Recording ×œ× × ××™× ×“×§×¡
```
Step 7: Waiting for indexing... (60s timeout)
        - After 10s: count = 1500
        - After 20s: count = 1500
        - After 30s: count = 1500
        - After 60s: count = 1500
âŒ FAILURE: New recording was not indexed within 60s
```

**××” ×–×” ××•××¨?**
- **××™×Ÿ recovery mechanism** ×‘-Focus Server!
- ××•: **×”×•× ×œ× ×¤×•×¢×œ** ×›××• ×©×¦×¨×™×š
- **Action Required**: ×¦×•×•×ª Dev ×¦×¨×™×š ×œ×××© recovery mechanism

---

#### Failure Scenario 2: Count ×œ× ×¢×œ×”
```
Step 8: Recording count: 1500 â†’ 1500 (no change)
âŒ FAILURE: Recording count did not increase
```

**××” ×–×” ××•××¨?**
- Recording ×œ× × ×•×¡×£ ×‘×¦×•×¨×” × ×›×•× ×”
- ××•: Recovery mechanism ×œ× ×–×™×”×” ××•×ª×•
- **Action Required**: ×‘×“×™×§×ª file system, ×‘×“×™×§×ª recovery logic

---

## ğŸ—£ï¸ ×©××œ×•×ª ×œ×¤×’×™×©×” (Questions for the Meeting)

### ×©××œ×•×ª ×§×¨×™×˜×™×•×ª:
1. **×”×× ×™×© recovery mechanism ×‘-Focus Server?**
   - ××” ×”×œ×•×’×™×§×” ×©×œ×•?
   - ××™×š ×”×•× ××–×”×” recordings ×—×“×©×™×?
   - ×›×œ ×›××” ×–××Ÿ ×”×•× ×¨×¥?

2. **××” ×§×•×¨×” ×‘××”×œ×š MongoDB outage?**
   - ×”×× recordings ×××©×™×›×™× ×œ×”×’×™×¢ ×œ××—×¡×•×Ÿ?
   - ××™×¤×” ×”× × ×©××¨×™×?
   - ××” ×§×•×¨×” ×œ-metadata?

3. **××™×š Focus Server ×™×•×“×¢ ×©×™×© recordings ×—×“×©×™×?**
   - File system scan?
   - Message queue?
   - Periodic polling?

4. **××” timeout ×œ-recovery?**
   - ×›××” ×–××Ÿ ×œ×•×§×— ×¢×“ ×©-recording × ××™× ×“×§×¡?
   - ×”×× ×™×© retry mechanism?

5. **×”×× ×™×© alerting ×›×©-MongoDB down?**
   - Slack notification?
   - PagerDuty?
   - Email?

---

### ×©××œ×•×ª ×˜×›× ×™×•×ª:
6. **××™×š ××“××™× MongoDB outage ×‘×˜×¡×˜?**
   - `kubectl scale deployment mongodb --replicas=0`
   - Network block?
   - Pod deletion?

7. **××™×š ××•×¡×™×¤×™× recording ×‘××”×œ×š outage?**
   - Manual file creation?
   - API call to data acquisition system?

8. **××” ×”××‘× ×” ×©×œ recording ×‘-file system?**
   - Path: `/data/recordings/{recording_id}.dat`?
   - Metadata: `/data/recordings/{recording_id}.json`?

9. **×”×× ×™×© locking mechanism?**
   - ×›×“×™ ×œ×× ×•×¢ concurrent indexing?

10. **××” ×§×•×¨×” ×× ×™×© 1000 recordings ×—×“×©×™×?**
    - ×”×× recovery ×™×›×•×œ ×œ×˜×¤×œ?
    - ×”×× ×™×© batch processing?

---

## ğŸ“Š ×˜×‘×œ×ª ×¡×™×›×•× - Recovery Scenarios

| Scenario | MongoDB Outage Time | New Recordings | Expected Behavior |
|----------|-------------------|----------------|------------------|
| **Short Outage** | < 1 min | 1-5 | âœ… Auto-index immediately |
| **Medium Outage** | 1-10 min | 5-50 | âœ… Auto-index in batch |
| **Long Outage** | 10-60 min | 50-500 | âš ï¸ Slow indexing |
| **Extended Outage** | > 60 min | 500+ | ğŸš« Manual intervention? |

---

## âœ… Checklist ×œ×¤× ×™ ×”×¤×’×™×©×”

- [ ] ×§×¨××ª×™ ××ª ×”××¡××š ×”×–×” ×œ×¢×•××§
- [ ] ×”×‘× ×ª×™ ××” ×–×” recovery mechanism ×•×œ××” ×”×•× ×§×¨×™×˜×™
- [ ] ×”×‘× ×ª×™ ××ª ×”-timeline ×©×œ ×”×˜×¡×˜
- [ ] ×™×•×“×¢ ××” ×”×¡×™×›×•× ×™× ×©×œ data loss
- [ ] ×”×›× ×ª×™ ×©××œ×•×ª ×¢×œ ×”-recovery logic
- [ ] ×™×•×“×¢ ××™×š Kubernetes scaling ×¢×•×‘×“
- [ ] ×¡×§×¨×ª×™ ××ª ×”×§×•×“ ×‘-`test_mongodb_outage_resilience.py`
- [ ] ×™×•×“×¢ ××” ×”×”×‘×“×œ ×‘×™×Ÿ file system ×œ-database

---

## ğŸ“Œ × ×§×•×“×•×ª ××¤×ª×— ×œ×–×›×•×¨

1. **File system â‰  Database** â†’ ×¦×¨×™×š sync!
2. **Recovery mechanism ×”×•× ×§×¨×™×˜×™** â†’ ×œ×× ×™×¢×ª data loss
3. **×”×˜×¡×˜ ×“×•×¨×© Kubernetes** â†’ ×œ× ×™×›×•×œ ×œ×¨×•×¥ ×‘-dev local
4. **Outage ×œ× ××•××¨ data loss** â†’ ×× ×™×© recovery
5. **Manual recovery = Bad** â†’ ×¦×¨×™×š automatic!

---

**× ×›×ª×‘ ×¢×‘×•×¨**: Roy Avrahami  
**×ª××¨×™×š**: ××•×§×˜×•×‘×¨ 2025  
**Jira**: PZ-13687

---

