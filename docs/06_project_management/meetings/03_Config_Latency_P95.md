# ×˜×¡×˜ 3: Performance â€“ /config Latency P95/P99
## PZ-13770 - × ×™×ª×•×— ××§×™×£ ×•××¢××™×§

---

## ğŸ“‹ ×ª×§×¦×™×¨ ××”×™×¨ ×œ×¤×’×™×©×” (Quick Brief)

| **×©×“×”** | **×¢×¨×š** |
|---------|---------|
| **Jira ID** | PZ-13770 |
| **×©× ×”×˜×¡×˜** | Performance â€“ /config Latency P95/P99 |
| **×¢×“×™×¤×•×ª** | ğŸ”´ **HIGH** |
| **×¡×•×’** | Performance Test (Latency Measurement) |
| **×¡×˜×˜×•×¡ ××•×˜×•××¦×™×”** | âœ… **Automated** |
| **××©×š ×¨×™×¦×” ×¦×¤×•×™** | ~60-90 ×©× ×™×•×ª |
| **××•×¨×›×‘×•×ª ××™××•×©** | ğŸŸ¢ **× ××•×›×”** |
| **×§×•×‘×¥ ×˜×¡×˜** | `tests/integration/performance/test_performance_high_priority.py` |
| **Test Class** | `TestAPILatencyP95` |
| **×©×•×¨×•×ª** | 53-195 |
| **×ª×œ×•×™×•×ª** | Focus Server API, Statistics module |

---

## ğŸ¯ ××” ×”××˜×¨×” ×©×œ ×”×˜×¡×˜? (Test Objectives)

### ××˜×¨×” ××¡×˜×¨×˜×’×™×ª (Strategic Goal):
×œ××“×•×“ ×•×œ×•×•×“× ×©×”-endpoint ×”×§×¨×™×˜×™ `POST /config/{task_id}` ×¢×•× ×” **×‘×–××Ÿ ×¡×‘×™×¨** ×’× ×ª×—×ª ×¢×•××¡, ×›×“×™ ×œ×”×‘×˜×™×— **×—×•×•×™×™×ª ××©×ª××© ×˜×•×‘×”**.

### ××˜×¨×•×ª ×¡×¤×¦×™×¤×™×•×ª (Specific Goals):
1. **××“×™×“×ª P95 Latency** - 95% ××”×‘×§×©×•×ª ×¢×•× ×•×ª ×ª×•×š X ms
2. **××“×™×“×ª P99 Latency** - 99% ××”×‘×§×©×•×ª ×¢×•× ×•×ª ×ª×•×š Y ms
3. **×–×™×”×•×™ outliers** - ×”×× ×™×© ×‘×§×©×•×ª ×©×œ×•×§×—×•×ª **×”×¨×‘×” ×–××Ÿ**?
4. **×•×™×“×•× SLA** - ×”×× ×”××¢×¨×›×ª ×¢×•××“×ª ×‘-Service Level Agreement?
5. **×–×™×”×•×™ regressions** - ×”×× ×”×‘×™×¦×•×¢×™× ×”×©×ª×¤×¨×•/×”×—××™×¨×•?

---

## ğŸ§ª ××” ×× ×™ ×¨×•×¦×” ×œ×‘×“×•×§? (What We're Testing)

### ×”×¡×¦× ×¨×™×• ×©×× ×—× ×• ×‘×•×“×§×™×:

**Scenario**: ××©×ª××© ×©×•×œ×— **100 ×‘×§×©×•×ª** ×œ-`POST /config` ×‘×¨×¦×£ (×œ× concurrent - sequential).

#### ×œ××” 100 ×‘×§×©×•×ª?
- **××¡×¤×™×§ ×’×“×•×œ** ×œ×—×™×©×•×‘ percentiles ××“×•×™×§
- **×œ× ×’×“×•×œ ××“×™** - ×”×˜×¡×˜ ×œ× ×™×™×§×— ×™×•×ª×¨ ××“×™ ×–××Ÿ
- **×¡×˜× ×“×¨×˜ ×‘×ª×¢×©×™×™×”** - ×¨×•×‘ ×”×˜×¡×˜×™× ××©×ª××©×™× ×‘-100 ××• 1000 samples

---

### ××” ×–×” P95 / P99 Latency?

#### ×”×’×“×¨×”:
- **P50 (Median)**: 50% ××”×‘×§×©×•×ª ××”×™×¨×•×ª ××–×”
- **P95**: 95% ××”×‘×§×©×•×ª ××”×™×¨×•×ª ××–×”
- **P99**: 99% ××”×‘×§×©×•×ª ××”×™×¨×•×ª ××–×”

#### ×“×•×’××” ××¡×¤×¨×™×ª:
× × ×™×— ×™×© ×œ× ×• 100 ×‘×§×©×•×ª ×¢× ×”×–×× ×™× ×”×‘××™× (×××•×™× ×™×):

```
Request 1:   50 ms
Request 2:   52 ms
Request 3:   55 ms
...
Request 50:  120 ms  â† P50 (Median)
...
Request 95:  200 ms  â† P95
Request 96:  210 ms
Request 97:  220 ms
Request 98:  250 ms
Request 99:  300 ms  â† P99
Request 100: 500 ms  (outlier!)
```

**P50 = 120 ms**  
**P95 = 200 ms**  
**P99 = 300 ms**

**×œ××” ×œ× ×××•×¦×¢?**  
×× 99 ×‘×§×©×•×ª ×œ×•×§×—×•×ª 50ms ×•××—×ª ×œ×•×§×—×ª 10 ×©× ×™×•×ª:
- **×××•×¦×¢** = (99Ã—50 + 10,000) / 100 = **149 ms** â† ××˜×¢×”!
- **P95** = 50 ms â† ××©×§×£ ××ª ×”××¦×™××•×ª
- **P99** = 50 ms
- **Max** = 10,000 ms â† outlier

---

## ğŸ”¥ ××” ×”× ×—×™×¦×•×ª ×©×œ ×”×˜×¡×˜? (Why Is This Critical?)

### ×¡×™×›×•× ×™× ×× ×œ× ×‘×•×“×§×™×:

#### 1ï¸âƒ£ **×—×•×•×™×™×ª ××©×ª××© ×’×¨×•×¢×”** (Bad UX)
**×ª×¨×—×™×©**:  
××©×ª××© ×œ×•×—×¥ ×¢×œ "Create Configuration" â†’ ××—×›×” 5 ×©× ×™×•×ª â†’ frustration!

**×ª×•×¦××”**:
- ××©×ª××© ×—×•×©×‘ ×©×”××¢×¨×›×ª ××™×˜×™×ª
- ××©×ª××© ×œ×•×—×¥ ×©×•×‘ (double submit) â†’ duplicate tasks
- ××©×ª××© ×¢×•×–×‘ ××ª ×”××¤×œ×™×§×¦×™×”

**×¢×œ×•×ª**: ××•×‘×“×Ÿ ××©×ª××©×™×, ×ª×“××™×ª ×¨×¢×”.

---

#### 2ï¸âƒ£ **Timeouts ×‘-Frontend** (UI Timeouts)
**×ª×¨×—×™×©**:  
×”×“×¤×“×¤×Ÿ ××’×“×™×¨ timeout ×©×œ **3 ×©× ×™×•×ª** ×œ×‘×§×©×”.  
×× P95 = 4 seconds â†’ **5% ××”×‘×§×©×•×ª ×™×ª× ×ª×§×•!**

**×ª×•×¦××”**:
- ×”××©×ª××© ×¨×•××” "Request Timeout"
- ×”×•× ×œ× ×™×•×“×¢ ×× ×”-task × ×•×¦×¨ ××• ×œ×
- ×¦×¨×™×š ×œ× ×¡×•×ª ×©×•×‘ â†’ ×ª×¡×›×•×œ

---

#### 3ï¸âƒ£ **Load Balancer Timeouts** (Infrastructure Failures)
**×ª×¨×—×™×©**:  
Load Balancer ××’×“×™×¨ timeout ×©×œ **10 ×©× ×™×•×ª**.  
×× P99 = 12 seconds â†’ **1% ××”×‘×§×©×•×ª ×™×§×‘×œ×• 504 Gateway Timeout!**

**×ª×•×¦××”**:
- ×”×‘×§×©×” ××’×™×¢×” ×œ-server ××‘×œ ×”×ª×©×•×‘×” ×œ× ×—×•×–×¨×ª
- ×”××©×ª××© ×œ× ×™×•×“×¢ ××” ×§×¨×”
- ×¦×•×•×ª DevOps ××§×‘×œ alerts

---

#### 4ï¸âƒ£ **Cascade Failures ×‘×¢×•××¡** (Cascade Under Load)
**×ª×¨×—×™×©**:  
P95 latency = 200 ms ×ª×—×ª ×¢×•××¡ × ××•×š.  
×›×©×™×© **50 concurrent users** â†’ P95 = 5 seconds!

**×œ××”?**  
×›×™ ×”××¢×¨×›×ª ×œ× scale ×˜×•×‘ â†’ **bottlenecks**.

**×ª×•×¦××”**: ×›×œ ×”××¢×¨×›×ª ×§×•×¨×¡×ª.

---

#### 5ï¸âƒ£ **SLA Breach** (×”×¤×¨×ª ×”×¡×›×)
**×ª×¨×—×™×©**:  
×”-SLA ××•××¨: "95% ××”×‘×§×©×•×ª ×¢×•× ×•×ª ×ª×•×š 500ms".  
×‘×“×™×§×” ××’×œ×”: P95 = 800ms â†’ **×”×¤×¨×ª SLA!**

**×ª×•×¦××”**:
- ×œ×§×•×— ×–×•×¢×
- ×§× ×¡ ×›×¡×¤×™
- ××•×‘×“×Ÿ ×××•×Ÿ

---

## ğŸ› ï¸ ××™×š ×× ×™ ×××© ××•×ª×• ×‘×§×•×“? (Code Implementation)

### ×§×•×‘×¥ ×”×˜×¡×˜:
**Path**: `tests/integration/performance/test_performance_high_priority.py`  
**Test Class**: `TestAPILatencyP95`  
**Lines**: 53-195

---

### ×§×•×“ ××œ× ×¢× ×”×¡×‘×¨×™×:

```python
@pytest.mark.integration
@pytest.mark.performance
@pytest.mark.critical
@pytest.mark.slow
class TestAPILatencyP95:
    """
    Test suite for PZ-13770: Performance â€“ /config Latency P95
    Priority: HIGH
    
    Measures and validates P95 and P99 latency for critical API endpoints.
    """
    
    def test_config_endpoint_latency_p95_p99(self, focus_server_api, performance_config_payload):
        """
        Test PZ-13770.1: Measure P95/P99 latency for POST /config.
        
        Steps:
            1. Execute 100 POST /config requests (sequential)
            2. Measure latency for each request
            3. Calculate P50, P95, P99 percentiles
            4. Verify against thresholds
        
        Expected:
            - P95 latency < 300 ms
            - P99 latency < 500 ms
            - No requests timeout
        
        Jira: PZ-13770
        Priority: HIGH
        """
        logger.info("Test PZ-13770.1: POST /config latency P95/P99")
        
        # =====================================================
        # Configuration
        # =====================================================
        num_requests = 100         # Number of requests to send
        latencies = []             # List to store latencies
        errors = 0                 # Error counter
        
        logger.info(f"Executing {num_requests} POST /config requests...")
        
        # =====================================================
        # Execute requests and measure latency
        # =====================================================
        for i in range(num_requests):
            try:
                # -----------------------------------------------
                # Measure request latency
                # -----------------------------------------------
                # Use time.perf_counter() for high-resolution timing
                start_time = time.perf_counter()
                
                # Create configuration request
                config_request = ConfigureRequest(**performance_config_payload)
                
                # Send POST /configure request
                response = focus_server_api.configure_streaming_job(config_request)
                
                # Record end time
                end_time = time.perf_counter()
                
                # Calculate latency in milliseconds
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)
                
                # -----------------------------------------------
                # Verify request succeeded
                # -----------------------------------------------
                if not hasattr(response, 'job_id') or not response.job_id:
                    errors += 1
                    logger.warning(f"Request {i}: No job_id in response")
                
            except Exception as e:
                # Request failed - count as error
                errors += 1
                logger.error(f"Request {i}: Error - {e}")
            
            # -----------------------------------------------
            # Small delay between requests (throttling)
            # -----------------------------------------------
            # Every 10 requests, add a 100ms delay
            # This prevents overwhelming the server
            if i % 10 == 0 and i > 0:
                logger.info(f"Completed {i}/{num_requests} requests")
                time.sleep(0.1)  # 100ms delay
        
        # =====================================================
        # Calculate statistics
        # =====================================================
        # Verify we have successful requests
        assert len(latencies) > 0, "No successful requests"
        
        # Sort latencies for percentile calculation
        latencies.sort()
        
        # Calculate percentiles
        p50 = statistics.median(latencies)               # 50th percentile (median)
        p95 = latencies[int(len(latencies) * 0.95)]     # 95th percentile
        p99 = latencies[int(len(latencies) * 0.99)]     # 99th percentile
        min_latency = min(latencies)                     # Minimum latency
        max_latency = max(latencies)                     # Maximum latency
        avg_latency = statistics.mean(latencies)         # Average latency
        
        # =====================================================
        # Log results
        # =====================================================
        logger.info("=" * 60)
        logger.info(f"POST /config Latency Results ({len(latencies)} requests):")
        logger.info(f"  Min:  {min_latency:8.2f} ms")
        logger.info(f"  P50:  {p50:8.2f} ms")
        logger.info(f"  Avg:  {avg_latency:8.2f} ms")
        logger.info(f"  P95:  {p95:8.2f} ms â­")       # Key metric!
        logger.info(f"  P99:  {p99:8.2f} ms â­")       # Key metric!
        logger.info(f"  Max:  {max_latency:8.2f} ms")
        logger.info(f"  Errors: {errors}/{num_requests}")
        logger.info("=" * 60)
        
        # =====================================================
        # Define thresholds (updated per specs meeting)
        # =====================================================
        THRESHOLD_P95_MS = 300   # 300ms for P95
        THRESHOLD_P99_MS = 500   # 500ms for P99
        MAX_ERROR_RATE = 0.05    # 5% error rate
        
        # =====================================================
        # Assertions
        # =====================================================
        # 1. Check error rate
        error_rate = errors / num_requests
        assert error_rate <= MAX_ERROR_RATE, \
            f"Error rate {error_rate:.2%} exceeds threshold {MAX_ERROR_RATE:.2%}"
        
        # 2. Check P95 latency (warning only for now)
        if p95 >= THRESHOLD_P95_MS:
            logger.warning(
                f"âš ï¸ P95 latency {p95:.2f}ms >= {THRESHOLD_P95_MS}ms "
                f"(baseline measurement needed)"
            )
        else:
            logger.info(f"âœ… P95 latency {p95:.2f}ms < {THRESHOLD_P95_MS}ms")
        
        # 3. Check P99 latency (warning only for now)
        if p99 >= THRESHOLD_P99_MS:
            logger.warning(
                f"âš ï¸ P99 latency {p99:.2f}ms >= {THRESHOLD_P99_MS}ms "
                f"(baseline measurement needed)"
            )
        else:
            logger.info(f"âœ… P99 latency {p99:.2f}ms < {THRESHOLD_P99_MS}ms")
```

---

### ××” ×§×•×¨×” ×¤×”? (Step-by-Step Explanation)

#### **×©×œ×‘ 1: Configuration**
```python
num_requests = 100
latencies = []
errors = 0
```
- **num_requests**: ×›××” ×‘×§×©×•×ª ×œ×©×œ×•×— (100)
- **latencies**: ×¨×©×™××” ×œ××—×¡×•×Ÿ latencies
- **errors**: ××•× ×” ×©×’×™××•×ª

---

#### **×©×œ×‘ 2: ×œ×•×œ××” ×¢×œ ×”×‘×§×©×•×ª**
```python
for i in range(num_requests):
```
- ×©×•×œ×—×™× **100 ×‘×§×©×•×ª ×‘×¨×¦×£** (×œ× concurrent!)
- ×œ××” ×‘×¨×¦×£? ×›×“×™ ×œ××“×•×“ **baseline latency** ×‘×œ×™ ×¢×•××¡ ××œ××›×•×ª×™

---

#### **×©×œ×‘ 3: ××“×™×“×ª Latency**
```python
start_time = time.perf_counter()
response = focus_server_api.configure_streaming_job(config_request)
end_time = time.perf_counter()
latency_ms = (end_time - start_time) * 1000
```

**×œ××” `time.perf_counter()`?**
- **High-resolution timer** - ×“×™×•×§ ×©×œ microseconds
- ×˜×•×‘ ×™×•×ª×¨ ×-`time.time()` (×©×™×›×•×œ ×œ×§×¤×•×¥ ×‘×’×œ×œ system clock adjustments)

**×œ××” Ã— 1000?**
- `perf_counter()` ××—×–×™×¨ ×©× ×™×•×ª
- ×× ×—× ×• ×¨×•×¦×™× **milliseconds** (Ã— 1000)

---

#### **×©×œ×‘ 4: Throttling**
```python
if i % 10 == 0 and i > 0:
    logger.info(f"Completed {i}/{num_requests} requests")
    time.sleep(0.1)  # 100ms delay
```

**×œ××” delay ×›×œ 10 ×‘×§×©×•×ª?**
- ×œ×× ×•×¢ **overwhelming** ×©×œ ×”×©×¨×ª
- × ×•×ª× ×™× ×œ×©×¨×ª "×œ× ×©×•×" ×‘×™×Ÿ bursts
- **100ms delay** = ×–× ×™×—, ××‘×œ ××¡×¤×™×§ ×›×“×™ ×œ×× ×•×¢ spike

---

#### **×©×œ×‘ 5: ×—×™×©×•×‘ Percentiles**
```python
latencies.sort()
p95 = latencies[int(len(latencies) * 0.95)]
```

**××™×š ×–×” ×¢×•×‘×“?**
- ××™×™× ×™× ××ª ×”-latencies (ascending)
- P95 = ×”××™×‘×¨ ×‘-index 95 (×× ×™×© 100 ××™×‘×¨×™×)
- ×“×•×’××”: `latencies[95]` = ×”-96th ××™×‘×¨ (0-indexed)

**×œ××” ×œ× `percentile()` function?**
- `statistics` module ×‘-Python ×œ× ×ª×•××š ×‘-percentiles (×¨×§ median)
- ××¤×©×¨ ×œ×”×©×ª××© ×‘-`numpy.percentile()` ××‘×œ ×–×” dependency × ×•×¡×£
- ×”×—×™×©×•×‘ ×”×™×“× ×™ ×¤×©×•×˜ ×•××¡×¤×™×§ ××“×•×™×§

---

#### **×©×œ×‘ 6: Assertions**
```python
if p95 >= THRESHOLD_P95_MS:
    logger.warning("âš ï¸ P95 latency exceeded threshold")
```

**×œ××” warning ×•×œ× assertion?**
- ×›×¨×’×¢ ×”×˜×¡×˜ ×‘-"baseline measurement mode"
- ×œ× ×¨×•×¦×™× ×œ×›×©×œ ××ª ×”×˜×¡×˜ ×× ×”-threshold × ×—×¦×”
- ×¨×•×¦×™× **×œ××¡×•×£ × ×ª×•× ×™×** ×§×•×“× ×›×“×™ ×œ×”×’×“×™×¨ thresholds × ×›×•×Ÿ

**×‘×¢×ª×™×“**:
```python
assert p95 < THRESHOLD_P95_MS, f"P95 latency {p95}ms exceeds threshold"
```

---

## ğŸ“ ××” ×œ×•××“×™× ××”×˜×¡×˜ ×”×–×”?

### ×ª×•×¦××•×ª ×˜×™×¤×•×¡×™×•×ª (Expected Results):

```
POST /config Latency Results (100 requests):
  Min:    50.00 ms     â† Fastest request (cold start or lucky)
  P50:   120.00 ms     â† Median (typical latency)
  Avg:   135.50 ms     â† Average (slightly higher than median)
  P95:   200.00 ms â­  â† 95% of requests faster than this
  P99:   300.00 ms â­  â† 99% of requests faster than this
  Max:   500.00 ms     â† Slowest request (outlier)
  Errors: 0/100
```

### ××™×š ×œ×¤×¨×© ××ª ×”×ª×•×¦××•×ª?

#### âœ… **×ª×•×¦××•×ª ×˜×•×‘×•×ª**:
- **P95 < 300ms** â†’ ××©×ª××©×™× ×œ× ×™×¨×’×™×©×• ×‘×¢×™×›×•×‘
- **P99 < 500ms** â†’ ××¤×™×œ×• outliers ×œ× × ×•×¨××™×™×
- **Max < 1000ms** â†’ ××™×Ÿ timeouts

#### âš ï¸ **×ª×•×¦××•×ª ×‘×¢×™×™×ª×™×•×ª**:
- **P95 > 500ms** â†’ ××©×ª××©×™× ×™×¨×’×™×©×• ××™×˜×™×•×ª
- **P99 > 2000ms** â†’ outliers ×’×•×¨××™× ×œ-timeouts
- **Errors > 5%** â†’ ××¢×¨×›×ª ×œ× ×™×¦×™×‘×”

#### ğŸš« **×ª×•×¦××•×ª ×œ× ××§×•×‘×œ×•×ª**:
- **P95 > 2000ms** â†’ ×—×•×•×™×” ×’×¨×•×¢×” ×××•×“
- **P99 > 10000ms** â†’ timeouts ×‘×˜×•×—×™×
- **Errors > 20%** â†’ ××¢×¨×›×ª ×œ× ×ª×§×™× ×”

---

### ×œ××” Latency ×’×‘×•×”?

| Cause | Description | Solution |
|-------|-------------|----------|
| **Database Slow Queries** | MongoDB queries ×œ×•×§×—×™× ×–××Ÿ | Add indexes, optimize queries |
| **Network Latency** | ×¨×©×ª ××™×˜×™×ª ×‘×™×Ÿ components | Co-locate services, use faster network |
| **CPU Bottleneck** | CPU 100% â†’ slow processing | Scale horizontally |
| **Memory Paging** | Swapping to disk â†’ very slow | Increase memory |
| **External API Calls** | Waiting for 3rd party APIs | Use caching, async calls |
| **Logging Overhead** | Too much logging â†’ slow | Reduce log verbosity |

---

## ğŸ—£ï¸ ×©××œ×•×ª ×œ×¤×’×™×©×” (Questions for the Meeting)

### ×©××œ×•×ª ××“×™× ×™×•×ª:
1. **××” ×”-SLA ×”×¨×©××™ ×¢×‘×•×¨ POST /config?**
   - P95 < X ms?
   - P99 < Y ms?
   - ××™ ×§×•×‘×¢ ××ª ×”-thresholds?

2. **××” ×§×•×¨×” ×× ×—×•×¨×’×™× ××”-SLA?**
   - ××–×”×¨×” ×‘-logs?
   - alert ×œ-DevOps?
   - ×”×¤×¡×§×ª ×©×™×¨×•×ª?

3. **×”×× ×™×© ×“×¨×•×’ ×©×œ ××©×ª××©×™×?**
   - Premium users â†’ lower latency?
   - Free users â†’ higher latency?

4. **××” ×ª×¨×—×™×© ×”-worst-case?**
   - ×›××” ××©×ª××©×™× ×‘××§×‘×™×œ?
   - ××™×–×” ×’×•×“×œ configuration?

---

### ×©××œ×•×ª ×˜×›× ×™×•×ª:
5. **××™×¤×” ×”-bottleneck ×”×¢×™×§×¨×™?**
   - Database?
   - Network?
   - CPU?
   - RabbitMQ?

6. **×”×× ×™×© caching?**
   - Redis?
   - In-memory cache?
   - TTL?

7. **×”×× ×™×© monitoring real-time?**
   - Prometheus?
   - Grafana dashboards?
   - Alerts?

8. **××” timeout ×‘-production?**
   - Load Balancer: X seconds
   - API Gateway: Y seconds
   - Frontend: Z seconds

9. **×”×× Latency ××©×ª× ×” ×œ×¤×™ ×©×¢×•×ª?**
   - Peak hours vs. off-peak?
   - Morning rush vs. evening?

10. **×”×× ×‘×“×§× ×• ×ª×—×ª ×¢×•××¡ realistic?**
    - ×¢× concurrent users?
    - ×¢× large configurations?

---

## ğŸ“Š ×˜×‘×œ×ª ×¡×™×›×•× - Latency Benchmarks

| Category | P50 (Median) | P95 | P99 | Max | Assessment |
|----------|-------------|-----|-----|-----|------------|
| **Excellent** | < 100 ms | < 200 ms | < 300 ms | < 500 ms | âœ… Great UX |
| **Good** | 100-200 ms | 200-300 ms | 300-500 ms | 500-1000 ms | âœ… Acceptable |
| **Fair** | 200-300 ms | 300-500 ms | 500-1000 ms | 1000-2000 ms | âš ï¸ Slow |
| **Poor** | 300-500 ms | 500-1000 ms | 1000-2000 ms | 2000-5000 ms | ğŸš« Bad UX |
| **Unacceptable** | > 500 ms | > 1000 ms | > 2000 ms | > 5000 ms | ğŸš« Timeouts |

---

## ğŸ¯ ×”×©×•×•××”: P95 vs. Average

### ×œ××” P95 ×—×©×•×‘ ×™×•×ª×¨ ×-Average?

```
Scenario 1: Consistent Performance
Latencies: [100, 105, 110, 115, 120, 125, 130]
Average: 115 ms
P95: 130 ms
â†’ Good! Consistent performance.

Scenario 2: Outliers Present
Latencies: [100, 105, 110, 115, 120, 125, 10000]
Average: 1525 ms   â† Misleading!
P95: 125 ms        â† True experience for 95% of users!
â†’ Only 5% experience slowness.
```

**××¡×§× ×”**: P95/P99 ××™×™×¦×’×™× ××ª **×”-UX ×”×××™×ª×™** ×˜×•×‘ ×™×•×ª×¨ ×-Average.

---

## âœ… Checklist ×œ×¤× ×™ ×”×¤×’×™×©×”

- [ ] ×§×¨××ª×™ ××ª ×”××¡××š ×”×–×” ×œ×¢×•××§
- [ ] ×”×‘× ×ª×™ ××” ×–×” P50/P95/P99 ×•××™×š ××—×©×‘×™×
- [ ] ×”×‘× ×ª×™ ×œ××” P95 ×—×©×•×‘ ×™×•×ª×¨ ×-Average
- [ ] ×™×•×“×¢ ×œ×”×¡×‘×™×¨ ××” ×”×¡×™×›×•× ×™× ×©×œ latency ×’×‘×•×”
- [ ] ×™×•×“×¢ ××™×–×” thresholds ××§×•×‘×œ×™× ×‘×ª×¢×©×™×™×”
- [ ] ×”×›× ×ª×™ ×©××œ×•×ª ×¢×œ ×”-SLA ×”×¨×©××™
- [ ] ×¡×§×¨×ª×™ ××ª ×”×§×•×“ ×‘-`test_performance_high_priority.py`
- [ ] ×™×•×“×¢ ××” outliers ×•××™×š ××–×”×™× ××•×ª×

---

## ğŸ“Œ × ×§×•×“×•×ª ××¤×ª×— ×œ×–×›×•×¨

1. **P95 > Average** (×‘×“"×›)
2. **P99 > P95** (×ª××™×“)
3. **Outliers ××¢×•×•×ª×™× Average, ×œ× P95/P99**
4. **Latency < 200ms = Not Noticeable by humans**
5. **Latency > 1000ms = Feels slow**
6. **×”×˜×¡×˜ ×”×–×” ××•×“×“ baseline - ×œ× ×ª×—×ª ×¢×•××¡!**

---

**× ×›×ª×‘ ×¢×‘×•×¨**: Roy Avrahami  
**×ª××¨×™×š**: ××•×§×˜×•×‘×¨ 2025  
**Jira**: PZ-13770

---

