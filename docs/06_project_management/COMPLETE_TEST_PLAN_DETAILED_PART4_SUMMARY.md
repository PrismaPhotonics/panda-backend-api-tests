# ×ª×•×›× ×™×ª ×‘×“×™×§×•×ª Focus Server - ×—×œ×§ 4: ×¡×™×›×•× ×•××™×œ×•×Ÿ ××•×©×’×™×
## Infrastructure, Security, Summary & Technical Glossary

---

## ğŸ—ï¸ INFRASTRUCTURE TESTS

### TEST: SSH Access to Production

**Jira ID**: PZ-13900  
**Priority**: High  
**Status**: TODO

**××˜×¨×”**: ×•×™×“×•× ×’×™×©×ª SSH ×œservers

**×œ××” × ×—×•×¥?**
SSH ×”×•× ×”×“×¨×š ×”×™×—×™×“×” ×œ:
- **Troubleshooting** - ×‘×“×™×§×ª ×‘×¢×™×•×ª
- **Logs** - ×§×¨×™××ª ×œ×•×’×™×
- **kubectl** - × ×™×”×•×œ Kubernetes
- **k9s** - ×××©×§ ×—×–×•×ª×™ ×œ-K8s
- **×ª×—×–×•×§×”** - ×¢×“×›×•× ×™× ×•×ª×™×§×•× ×™×

**×—×™×‘×•×¨×™×:**
```
Local PC
   â†“ SSH
Jump Host (10.10.100.3)
   â†“ SSH
Target Host (10.10.100.113)
   â†“ kubectl
Kubernetes Cluster
```

**×™×™×©×•×:**
```python
import paramiko

def test_ssh_access_to_production(self):
    """Test PZ-13900: SSH Access"""
    
    # Connect to jump host
    ssh_jump = paramiko.SSHClient()
    ssh_jump.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh_jump.connect(
        '10.10.100.3',
        username='root',
        password='***'  # From environment
    )
    
    # Test commands on jump host
    stdin, stdout, stderr = ssh_jump.exec_command('hostname')
    hostname = stdout.read().decode().strip()
    assert hostname is not None
    logger.info(f"âœ“ Jump host: {hostname}")
    
    # Test whoami
    stdin, stdout, stderr = ssh_jump.exec_command('whoami')
    user = stdout.read().decode().strip()
    assert user == 'root'
    logger.info(f"âœ“ User: {user}")
    
    # Test uptime
    stdin, stdout, stderr = ssh_jump.exec_command('uptime')
    uptime = stdout.read().decode().strip()
    logger.info(f"âœ“ Uptime: {uptime}")
    
    ssh_jump.close()
    
    # Connect to target host
    ssh_target = paramiko.SSHClient()
    ssh_target.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh_target.connect(
        '10.10.100.113',
        username='prisma',
        password='***'
    )
    
    # Test kubectl
    stdin, stdout, stderr = ssh_target.exec_command('kubectl version --client')
    kubectl_output = stdout.read().decode()
    assert 'Client Version' in kubectl_output
    logger.info("âœ“ kubectl is available")
    
    # Test k9s
    stdin, stdout, stderr = ssh_target.exec_command('k9s version')
    k9s_output = stdout.read().decode()
    assert 'Version' in k9s_output or 'k9s' in k9s_output
    logger.info("âœ“ k9s is available")
    
    ssh_target.close()
    
    logger.info("âœ… SSH access validated")
```

---

### TEST: Kubernetes Cluster Connection

**Jira ID**: PZ-13899  
**Priority**: High  
**Status**: TODO

**××˜×¨×”**: ×•×™×“×•× ×—×™×‘×•×¨ ×œ-Kubernetes ×•×‘×¨×™××•×ª pods

**××” ×‘×•×“×§×™×?**
- ×—×™×‘×•×¨ ×œ-K8s cluster
- ×¨×©×™××ª pods ×‘-namespace `panda`
- status ×©×œ ×›×œ pod (Running/Ready)
- resource usage (CPU/Memory)

**Kubernetes Concepts:**

| ××•×©×’ | ×”×¡×‘×¨ | ×“×•×’××” |
|------|------|-------|
| **Pod** | ×™×—×™×“×ª ×¨×™×¦×” ×§×˜× ×” (container) | `panda-focus-server-abc123` |
| **Namespace** | ×”×¤×¨×“×” ×œ×•×’×™×ª | `panda` |
| **Service** | endpoint ×¤× ×™××™ | `panda-focus-server.panda:5000` |
| **Deployment** | × ×™×”×•×œ pods | `panda-focus-server` |

**×™×™×©×•×:**
```python
from kubernetes import client, config

def test_kubernetes_cluster_connection(self):
    """Test PZ-13899: Kubernetes Connection"""
    
    # Load kubeconfig
    config.load_kube_config()
    v1 = client.CoreV1Api()
    
    # List pods in panda namespace
    pods = v1.list_namespaced_pod(namespace="panda")
    logger.info(f"Found {len(pods.items)} pods in 'panda' namespace")
    
    # Check each pod
    for pod in pods.items:
        pod_name = pod.metadata.name
        pod_status = pod.status.phase
        
        logger.info(f"Pod: {pod_name}")
        logger.info(f"  Status: {pod_status}")
        
        # Verify Running
        assert pod_status == "Running", \
            f"Pod {pod_name} is not Running (status: {pod_status})"
        
        # Check containers ready
        if pod.status.container_statuses:
            for container in pod.status.container_statuses:
                assert container.ready, \
                    f"Container {container.name} in pod {pod_name} is not ready"
                logger.info(f"  Container {container.name}: Ready")
    
    logger.info("âœ… All pods are Running and Ready")
```

---

### TEST: MongoDB Connection

**Jira ID**: PZ-13898  
**Priority**: High  
**Status**: TODO

**××˜×¨×”**: ×•×™×“×•× ×—×™×‘×•×¨ ×œ-MongoDB ×•×‘×¨×™××•×ª DB

**××” ×‘×•×“×§×™×?**
- ×—×™×‘×•×¨ ×œ-MongoDB
- ×§×™×•× collections × ×“×¨×©×•×ª
- sampling ×©×œ documents
- schema validation

**MongoDB Collections:**

| Collection | ×ª×™××•×¨ | ×©×™××•×© |
|------------|-------|-------|
| **base_paths** | × ×ª×™×‘×™ recordings | ××™×¤×•×™ ×œ××™×§×•××™ storage |
| **node2** | metadata ×©×œ recordings | ×¤×¨×˜×™× ×¢×œ recordings |
| **node4** | extended metadata | × ×ª×•× ×™× × ×•×¡×¤×™× |

**×™×™×©×•×:**
```python
from pymongo import MongoClient

def test_mongodb_connection(self):
    """Test PZ-13898: MongoDB Connection"""
    
    # Connect
    client = MongoClient(
        "mongodb://prisma:prisma@10.10.100.108:27017/?authSource=prisma"
    )
    
    db = client['prisma']
    
    # Verify connection
    server_info = client.server_info()
    logger.info(f"âœ“ MongoDB version: {server_info['version']}")
    
    # List collections
    collections = db.list_collection_names()
    logger.info(f"âœ“ Found {len(collections)} collections")
    
    # Verify required collections exist
    required = ['base_paths', 'node2', 'node4']
    for coll_name in required:
        assert coll_name in collections, \
            f"Required collection '{coll_name}' not found"
        logger.info(f"âœ“ Collection '{coll_name}' exists")
    
    # Sample a document from node4
    node4 = db['node4']
    sample_doc = node4.find_one()
    
    if sample_doc:
        logger.info(f"âœ“ Sample document from node4:")
        logger.info(f"  Fields: {list(sample_doc.keys())}")
        
        # Verify required fields
        required_fields = ['uuid', 'start_time', 'end_time', 'deleted']
        for field in required_fields:
            assert field in sample_doc, f"Field '{field}' missing in node4"
        
        logger.info("âœ“ node4 schema is valid")
    
    client.close()
    logger.info("âœ… MongoDB connection validated")
```

---

## ğŸ”’ SECURITY TESTS

### TEST: Robustness to Malformed Inputs

**Jira ID**: PZ-13572  
**Priority**: High  
**Type**: Security Test  
**Status**: ×—×œ×§×™

**××˜×¨×”**: ×œ×•×•×“× ×©×”××¢×¨×›×ª **×œ× ×§×•×¨×¡×ª** ××§×œ×˜×™× ××–×™×§×™×

**××” ×‘×•×“×§×™×?**

1. **Malformed JSON**
2. **SQL Injection attempts**
3. **XSS attempts**
4. **Oversized payloads** (10MB+)
5. **CORS headers**

**×ª×¨×—×™×©×™×:**

**1. Malformed JSON:**
```json
{
  "invalid_json": unclosed,
  "missing": "quote
}
```

**Expected**: HTTP 400 (×œ× 500!)

**2. SQL Injection:**
```http
GET /metadata?task_id=' OR '1'='1
```

**Expected**: ××¡×•× ×Ÿ, ×œ× ×’×•×¨× crash

**3. XSS:**
```json
{
  "task_id": "<script>alert('xss')</script>"
}
```

**Expected**: ××¡×•× ×Ÿ ××• encoded

**4. Oversized Payload:**
```python
huge_payload = {
    "data": "x" * (10 * 1024 * 1024)  # 10MB
}
```

**Expected**: HTTP 413 (Payload Too Large) ××• ×“×—×™×™×” ××—×¨×ª

**5. CORS:**
```http
OPTIONS /configure HTTP/1.1
Origin: https://evil.com
```

**Expected**: CORS headers × ×›×•× ×™×

**×™×™×©×•×:**
```python
def test_security_resilience(self, focus_server_api):
    """Test PZ-13572: Security Robustness"""
    
    # TEST 1: Malformed JSON
    try:
        response = requests.post(
            f"{base_url}/configure",
            data='{"invalid": unclosed',  # Bad JSON
            headers={'Content-Type': 'application/json'}
        )
        # Should be 400, not 500
        assert response.status_code == 400
        logger.info("âœ“ Malformed JSON returns 400 (not 500)")
    except:
        logger.info("âœ“ Malformed JSON handled gracefully")
    
    # TEST 2: SQL Injection
    try:
        response = requests.get(
            f"{base_url}/metadata",
            params={"task_id": "' OR '1'='1"}
        )
        # Should not crash (500)
        assert response.status_code != 500
        logger.info("âœ“ SQL injection attempt handled")
    except Exception as e:
        logger.info(f"âœ“ SQL injection attempt blocked: {e}")
    
    # TEST 3: Oversized payload
    huge_data = "x" * (10 * 1024 * 1024)  # 10MB
    try:
        response = requests.post(
            f"{base_url}/configure",
            json={"data": huge_data},
            timeout=5
        )
        # Should reject gracefully
        assert response.status_code in [400, 413, 413]
        logger.info("âœ“ Oversized payload rejected")
    except:
        logger.info("âœ“ Oversized payload handled")
    
    # TEST 4: CORS
    response = requests.options(f"{base_url}/configure")
    if 'Access-Control-Allow-Origin' in response.headers:
        logger.info(f"âœ“ CORS headers present: {response.headers['Access-Control-Allow-Origin']}")
    
    logger.info("âœ… Security resilience validated")
```

---

## ğŸ“Š ×¡×™×›×•× ×¡×•×¤×™ ×©×œ ×›×œ ×”×˜×¡×˜×™×

### ×¡×˜×˜×™×¡×˜×™×§×”

| ×§×˜×’×•×¨×™×” | ×¡×”"×› | ×××•××© | TODO | ××—×•×– |
|----------|------|-------|------|------|
| **Integration** | 44 | 35 | 9 | 80% |
| **SingleChannel** | 15 | 15 | 0 | 100% |
| **Dynamic ROI** | 13 | 13 | 0 | 100% |
| **Infrastructure** | 6 | 3 | 3 | 50% |
| **Performance** | 5 | 3 | 2 | 60% |
| **Security** | 2 | 1 | 1 | 50% |
| **E2E** | 3 | 2 | 1 | 67% |
| **Data Quality** | 5 | 5 | 0 | 100% |
| **TOTAL** | **93** | **77** | **16** | **83%** |

### Coverage Matrix

| ×¨×›×™×‘ | ×›×™×¡×•×™ | ×˜×¡×˜×™× |
|------|-------|-------|
| **POST /configure** | 95% | 40+ tests |
| **GET /waterfall** | 90% | 30+ tests |
| **GET /metadata** | 85% | 10+ tests |
| **GET /sensors** | 100% | 5 tests |
| **GET /channels** | 100% | 5 tests |
| **RabbitMQ Commands** | 90% | 15+ tests |
| **MongoDB Queries** | 75% | 8 tests |
| **Kubernetes** | 60% | 4 tests |

---

## ğŸ“– ××™×œ×•×Ÿ ××•×©×’×™× ×˜×›× ×™×™× - ××§×™×£

### Core Concepts (××•×©×’×™ ×™×¡×•×“)

#### NFFT (Number of FFT Points)
**×”×’×“×¨×”**: ××¡×¤×¨ × ×§×•×“×•×ª ×”-FFT (Fast Fourier Transform)  
**×¢×¨×›×™× ×ª×§×¤×™×**: 128, 256, 512, 1024, 2048, 4096 (×—×–×§×•×ª ×©×œ 2)  
**××©××¢×•×ª**: ×§×•×‘×¢ ××ª ×¨×–×•×œ×•×¦×™×™×ª ×”×ª×“×¨ ×‘× ×™×ª×•×—

**Trade-offs:**
- **NFFT ×§×˜×Ÿ** â†’ ×¨×–×•×œ×•×¦×™×™×ª ×ª×“×¨ × ××•×›×”, ×¢×“×›×•× ×™× ××”×™×¨×™×, CPU × ××•×š
- **NFFT ×’×“×•×œ** â†’ ×¨×–×•×œ×•×¦×™×™×ª ×ª×“×¨ ×’×‘×•×”×”, ×¢×“×›×•× ×™× ××™×˜×™×™×, CPU ×’×‘×•×”

**× ×•×¡×—×”**:
```
Frequency Bins = NFFT / 2
Rows per Second = PRR / NFFT
```

**×“×•×’××”**:
```
NFFT = 1024
PRR = 1000 samples/sec

Frequency Bins = 1024 / 2 = 512 bins
Rows/sec = 1000 / 1024 = 0.98 rows/sec
```

---

#### PRR (Pulse Repetition Rate)
**×”×’×“×¨×”**: ×§×¦×‘ ×—×–×¨×ª ×”×“×¤×§×™× - ×›××” ×¤×¢××™× ×œ×©× ×™×™×” ×”××¢×¨×›×ª ×“×•×’××ª  
**×™×—×™×“×•×ª**: samples/sec ××• Hz  
**×˜×•×•×— ×˜×™×¤×•×¡×™**: 1000-2000 samples/sec

**××©××¢×•×ª**:
- **PRR ×’×‘×•×”** â†’ ×™×›×•×œ×ª ×œ×‘×“×•×§ ×ª×“×¨×™× ×’×‘×•×”×™× ×™×•×ª×¨
- **PRR × ××•×š** â†’ ×’×‘×•×œ Nyquist × ××•×š ×™×•×ª×¨

**×§×©×¨ ×œ-Nyquist**:
```
Nyquist Frequency = PRR / 2

×× PRR = 1000 â†’ Nyquist = 500 Hz
×× PRR = 2000 â†’ Nyquist = 1000 Hz
```

**××™×¤×” ××§×‘×œ×™×?**
```python
metadata = focus_server_api.get_live_metadata()
prr = metadata.prr  # From live system
```

---

#### Nyquist Frequency
**×”×’×“×¨×”**: ×”×ª×“×¨ ×”××§×¡×™××œ×™ ×©× ×™×ª×Ÿ ×œ×“×’×•× × ×›×•×Ÿ  
**× ×•×¡×—×”**: `Nyquist = PRR / 2`  
**××©×¤×˜ Nyquist-Shannon**: ×ª×“×¨ ×”×“×’×™××” ×—×™×™×‘ ×œ×”×™×•×ª ×œ×¤×—×•×ª ×¤×™ 2 ××”×ª×“×¨ ×”××§×¡×™××œ×™

**×œ××” ×—×©×•×‘?**
×—×¨×™×’×” ×-Nyquist â†’ **Aliasing**:
```
×ª×“×¨ ×××™×ª×™: 600 Hz
PRR: 1000 (Nyquist = 500 Hz)

×”×ª×•×¦××”: ×”×ª×“×¨ ×™×™×¨××” ×›××• 400 Hz (WRONG!)
×”×¡×™×‘×”: 600 Hz "××ª×§×¤×œ" ×—×–×¨×” ×œ×˜×•×•×— 0-500
```

**×“×•×’××ª Aliasing ×‘×—×™×™×:**
```
×’×œ×’×œ ××¡×ª×•×‘×‘ ××”×¨ (600 RPM)
××¦×œ××” ×¦×•×œ××ª ×‘-500 FPS
×‘×¡×¨×˜×•×Ÿ: ×”×’×œ×’×œ × ×¨××” ××¡×ª×•×‘×‘ ×œ××—×•×¨!
×–×” Aliasing.
```

---

#### Spectrogram (×¡×¤×§×˜×•×’×¨××”)
**×”×’×“×¨×”**: ×™×™×¦×•×’ ×ª×œ×ª-×××“×™ ×©×œ ××•×ª (×–××Ÿ Ã— ×ª×“×¨ Ã— ×¢×•×¦××”)  
**×¦×™×¨×™×**:
- **X (×–××Ÿ)**: ×”×ª×§×“××•×ª ×‘×–××Ÿ
- **Y (×ª×“×¨)**: ×ª×“×¨×™× ×©× ×‘×“×§×™×
- **Z (×¢×•×¦××”)**: ×¦×‘×¢ - ×›×”×”=×—×œ×©, ×‘×”×™×¨=×—×–×§

**×“×•×’××”**:
```
     ×ª×“×¨ (Hz)
      â†‘
 500 |  [××“×•×]     [×¦×”×•×‘]
 400 |  [×›×ª×•×]     [×›×ª×•×]
 300 |  [×¦×”×•×‘]     [×™×¨×•×§]
 200 |  [×™×¨×•×§]     [×›×—×•×œ]
 100 |  [×›×—×•×œ]     [×›×—×•×œ]
   0 |____________â†’ ×–××Ÿ (sec)
      0    5    10   15   20
```

**××” ×›×œ pixel ××•××¨?**
- ××™×§×•× (x, y) â†’ ×–××Ÿ ×•×ª×“×¨
- ×¦×‘×¢ â†’ ×¢×•×¦××ª ×”××•×ª ×‘×ª×“×¨ ×–×” ×‘××•×ª×• ×–××Ÿ

---

#### Throughput (×ª×¤×•×§×”)
**×”×’×“×¨×”**: ×›××•×ª ×”× ×ª×•× ×™× ×©×”××¢×¨×›×ª ××¢×‘×“×ª/××©×“×¨×ª ×œ×™×—×™×“×ª ×–××Ÿ  
**×™×—×™×“×•×ª**: Mbps (Megabits per second)

**× ×•×¡×—×”**:
```
Throughput (Mbps) = (Rows/sec Ã— Bytes/row Ã— 8 bits/byte) / 1,000,000
```

**×“×•×’××”**:
```
Rows/sec = 0.98
Bytes/row = 102,400
Throughput = 0.98 Ã— 102,400 Ã— 8 / 1,000,000 = 0.80 Mbps
```

**×§×˜×’×•×¨×™×•×ª**:
- **Low**: < 1 Mbps
- **Medium**: 1-10 Mbps
- **High**: 10-50 Mbps
- **Very High**: > 50 Mbps

---

### API & Network Concepts

#### Endpoint
**×”×’×“×¨×”**: URL ×©××¡×¤×§ ×¤×•× ×§×¦×™×•× ×œ×™×•×ª ×¡×¤×¦×™×¤×™×ª  
**×“×•×’×××•×ª**:
```
POST   /configure
GET    /metadata/{task_id}
GET    /waterfall/{task_id}/{row_count}
GET    /sensors
GET    /channels
POST   /recordings_in_time_range
```

---

#### HTTP Status Codes (××“×¨×™×š ××œ×)

**2xx - Success:**
| Code | Name | Usage in Focus Server |
|------|------|----------------------|
| **200** | OK | /configure accepted, /sensors returned |
| **201** | Created | /waterfall has data |
| **208** | Already Reported | **Historic playback complete** |

**4xx - Client Errors:**
| Code | Name | Usage |
|------|------|-------|
| **400** | Bad Request | Invalid configuration, missing fields |
| **404** | Not Found | Task/consumer not found |
| **413** | Payload Too Large | Request body too big |
| **422** | Unprocessable Entity | Validation error |

**5xx - Server Errors:**
| Code | Name | Usage |
|------|------|-------|
| **500** | Internal Server Error | Server crash/bug |
| **503** | Service Unavailable | Dependency down (MongoDB/RabbitMQ) |

---

#### JSON Payload
**×”×’×“×¨×”**: ×¤×•×¨××˜ ×˜×§×¡×˜ ×œ×”×¢×‘×¨×ª × ×ª×•× ×™×  
**×××¤×™×™× ×™×**:
- ×§×¨×™× ×œ×‘× ×™ ××“×
- ×§×œ ×œparse
- ×ª××™×›×” ×‘× esting

**×“×•×’××”**:
```json
{
  "nfftSelection": 1024,
  "channels": {
    "min": 0,
    "max": 50
  },
  "nested": {
    "deep": {
      "value": 123
    }
  }
}
```

---

### Infrastructure Concepts

#### MongoDB
**×”×’×“×¨×”**: NoSQL ××¡×“ × ×ª×•× ×™× ××‘×•×¡×¡ documents  
**×©×™××•×© ×‘-Focus Server**:
- ××—×¡×•×Ÿ **recordings metadata**
- ×©××™×¨×ª **tasks configuration**
- ××™×¤×•×™ **time ranges** ×œ-recordings

**Collections:**
```
prisma DB
â”œâ”€â”€ base_paths    (recording paths)
â”œâ”€â”€ node2         (metadata)
â””â”€â”€ node4         (extended metadata)
```

**Query Example:**
```javascript
db.node4.find({
  start_time: {$lte: 1700000600},
  end_time: {$gte: 1700000000},
  deleted: false
})
```

---

#### RabbitMQ
**×”×’×“×¨×”**: Message Broker ×œ×ª×§×©×•×¨×ª ××¡×™× ×›×¨×•× ×™×ª  
**×¤×¨×•×˜×•×§×•×œ**: AMQP (Advanced Message Queuing Protocol)

**×¨×›×™×‘×™×:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Publisher   â”‚â”€â”€â”€â”€â”€â”€>â”‚   Exchange   â”‚â”€â”€â”€â”€â”€â”€>â”‚    Queue     â”‚â”€â”€â”€â”€â”€â”€> Consumer
â”‚ (Test Code)  â”‚       â”‚baby_analyzer â”‚       â”‚  commands    â”‚       â”‚(Baby Analyzer)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Routing Keys:**
- `roi` - Region of Interest commands
- `caxis` - Color axis adjustments
- `colormap` - Colormap selection

**Connection:**
```
Host: 10.10.100.107
Port: 5672 (AMQP)
Port: 15672 (Management UI)
Credentials: prisma/prisma
```

---

#### Kubernetes (K8s)
**×”×’×“×¨×”**: ××¢×¨×›×ª ××•×¨×›×™×¡×˜×¨×¦×™×” ×©×œ containers  
**×©×™××•×©**: × ×™×”×•×œ Baby Analyzer pods

**×”×™×¨×¨×›×™×”:**
```
Cluster (10.10.100.102:6443)
â””â”€â”€ Namespace: panda
    â”œâ”€â”€ Pod: baby-analyzer-job-abc123
    â”œâ”€â”€ Pod: focus-server-xyz789
    â”œâ”€â”€ Service: panda-focus-server.panda:5000
    â””â”€â”€ Deployment: panda-focus-server
```

**×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª:**
```bash
# List pods
kubectl get pods -n panda

# Describe pod
kubectl describe pod <pod-name> -n panda

# Logs
kubectl logs <pod-name> -n panda

# Port forward
kubectl port-forward <pod-name> 5000:5000 -n panda
```

---

#### gRPC
**×”×’×“×¨×”**: Remote Procedure Call framework ×œ-streaming  
**×©×™××•×©**: ×”×¢×‘×¨×ª **×¡×¤×§×˜×•×’×¨××•×ª ×‘×–××Ÿ ×××ª**

**Advantages:**
- **Binary protocol** â†’ ××”×™×¨ ×-JSON
- **Streaming** â†’ continuous data flow
- **Typed** â†’ proto files ××’×“×™×¨×™× schema

**Connection:**
```python
import grpc

channel = grpc.insecure_channel(
    f"{stream_url}:{stream_port}"
)
stub = DataStreamServiceStub(channel)

# Stream data
for message in stub.StreamSpectrograms(request):
    process_spectrogram(message)
```

---

### Testing Concepts

#### Integration Test
**×”×’×“×¨×”**: ×‘×•×“×§ ××™× ×˜×¨××§×¦×™×” ×‘×™×Ÿ 2+ ×§×•××¤×•× × ×˜×•×ª  
**×“×•×’××”**: Focus Server â†’ MongoDB â†’ RabbitMQ  
**××˜×¨×”**: ×œ×•×•×“× ×©×”×§×•××¤×•× × ×˜×•×ª ×¢×•×‘×“×•×ª **×‘×™×—×“**

---

#### Unit Test
**×”×’×“×¨×”**: ×‘×•×“×§ ×¤×•× ×§×¦×™×”/××—×œ×§×” **×‘×•×“×“×ª**  
**×“×•×’××”**: ×‘×“×™×§×ª `generate_task_id()`  
**××˜×¨×”**: ×œ×•×•×“× ×œ×•×’×™×§×” ××‘×•×“×“×ª

---

#### E2E Test (End-to-End)
**×”×’×“×¨×”**: ×‘×•×“×§ ×ª×¨×—×™×© **××œ×** ××”×ª×—×œ×” ×œ×¡×•×£  
**×“×•×’××”**: Configure â†’ Poll â†’ Get Data â†’ Complete  
**××˜×¨×”**: ×œ×•×•×“× ×©×”××¢×¨×›×ª **×›×•×œ×”** ×¢×•×‘×“×ª

---

#### Performance Test
**×”×’×“×¨×”**: ×‘×•×“×§ **×‘×™×¦×•×¢×™×** ×•**×ª×¤×•×§×”**  
**××“×“×™×**:
- **Latency**: ×–××Ÿ ×ª×’×•×‘×”
- **Throughput**: ×›××•×ª × ×ª×•× ×™×/sec
- **Resource Usage**: CPU/Memory

**×“×•×’××”**:
```python
# Measure latency
start = time.time()
response = api.configure(...)
latency = time.time() - start

assert latency < 2.0, f"Too slow: {latency:.2f}s"
```

---

#### Negative Test
**×”×’×“×¨×”**: ×‘×•×“×§ ×©**inputs ×œ× ×ª×§×¤×™× × ×“×—×™×**  
**×“×•×’×××•×ª**:
- min > max
- ×¢×¨×›×™× ×©×œ×™×œ×™×™×
- ×©×“×•×ª ×—×¡×¨×™×

**××˜×¨×”**: ×œ×•×•×“× **error handling** ×•**validation**

---

### Data Concepts

#### Task / Job
**×”×’×“×¨×”**: ×™×—×™×“×ª ×¢×‘×•×“×” ×©××‘×¦×¢×ª processing  
**××–×”×”**: `task_id` ××• `job_id`  
**××¦×‘×™×**:
- configured
- running
- completed
- failed

**Lifecycle:**
```
Created â†’ Configured â†’ Running â†’ Completed â†’ Cleaned up
```

---

#### Recording Window
**×”×’×“×¨×”**: ×˜×•×•×— ×–××Ÿ ×©×‘×• ×™×© recording ×–××™×Ÿ  
**×¤×•×¨××˜**: `[start_timestamp, end_timestamp]`

**×“×•×’××”**:
```json
[
  [1700000000, 1700000600],  // 10-minute recording
  [1700000600, 1700001200],  // Next 10 minutes
  [1700001200, 1700001800]   // Another 10 minutes
]
```

**×©×™××•×©**:
```python
# Find recordings in range
request = {
    "start_time": 1700000300,
    "end_time": 1700001500
}

# Returns overlapping windows:
# [1700000000, 1700000600] âœ“ overlaps
# [1700000600, 1700001200] âœ“ overlaps
# [1700001200, 1700001800] âœ“ overlaps
```

---

#### View Type
**×”×’×“×¨×”**: ××¦×‘ ×ª×¦×•×’×”  
**×¢×¨×›×™×**:
- **0** = MULTICHANNEL (××¡×¤×¨ sensors)
- **1** = SINGLECHANNEL (sensor ××—×“)

**MULTICHANNEL:**
```
channels: {min: 0, max: 50}
â†’ 50 sensors
â†’ stream_amount = 1
â†’ channel_to_stream_index = {"0": 0, "1": 0, ..., "49": 0}
```

**SINGLECHANNEL:**
```
channels: {min: 7, max: 7}
â†’ 1 sensor
â†’ stream_amount = 1
â†’ channel_to_stream_index = {"7": 0}
```

---

#### ROI (Region of Interest)
**×”×’×“×¨×”**: ×˜×•×•×— ×”-sensors ×©×¨×•×¦×™× ×œ×¨××•×ª  
**×¤×¨××˜×¨×™×**: start, end

**×“×•×’××”**:
```
ROI: [50, 150]
â†’ Monitor sensors 50, 51, 52, ..., 150
â†’ Total: 100 sensors
```

**Dynamic ROI:**
×©×™× ×•×™ ROI ×ª×•×š ×›×“×™ ×¨×™×¦×” ×œ×œ× ×”×¤×¡×§×”:
```
Initial: [0, 100]
   â†“ (send ROI command via RabbitMQ)
Changed: [50, 150]
   â†“ (Baby Analyzer reinitializes)
New data: sensors 50-150 only
```

---

#### CAxis (Color Axis)
**×”×’×“×¨×”**: ×˜×•×•×— ×¢×¨×›×™ amplitude ×œ-colormap  
**×¤×¨××˜×¨×™×**: caxis_min, caxis_max

**×“×•×’××”**:
```
CAxis: [-100, 0] dB

Mapping:
-100 dB â†’ Blue (cold)
-60 dB â†’ Green
-20 dB â†’ Red (hot)
0 dB â†’ White (max)
```

**×©×™× ×•×™ CAxis:**
```
From: [-100, 0] â†’ full range
To: [-80, -20] â†’ focused range

Result: Better contrast for signals in -80 to -20 dB range
```

---

### Time & Format Concepts

#### yymmddHHMMSS Format
**×”×’×“×¨×”**: ×¤×•×¨××˜ ×–××Ÿ ×©×œ Focus Server  
**××‘× ×”**: 12 digits

**×¤×™×¨×•×§:**
```
"251027143045"

25    = year (2025)
10    = month (October)
27    = day
14    = hour (24h format)
30    = minute
45    = second
```

**×”××¨×”:**
```python
# datetime â†’ yymmddHHMMSS
def datetime_to_yymmddHHMMSS(dt: datetime) -> str:
    return dt.strftime("%y%m%d%H%M%S")

# Example
dt = datetime(2025, 10, 27, 14, 30, 45)
result = datetime_to_yymmddHHMMSS(dt)
# "251027143045"
```

**yymmddHHMMSS â†’ datetime:**
```python
def yymmddHHMMSS_to_datetime(time_str: str) -> datetime:
    return datetime.strptime(time_str, "%y%m%d%H%M%S")

# Example
time_str = "251027143045"
dt = yymmddHHMMSS_to_datetime(time_str)
# datetime(2025, 10, 27, 14, 30, 45)
```

---

#### Epoch Timestamp
**×”×’×“×¨×”**: ××¡×¤×¨ ×©× ×™×•×ª ×-1 January 1970 00:00:00 UTC  
**×“×•×’××”**: `1700000000` = November 14, 2023

**×”××¨×”:**
```python
# datetime â†’ epoch
import time
epoch = int(time.time())
# 1730034645

# epoch â†’ datetime
dt = datetime.fromtimestamp(epoch)
```

---

### Pytest Concepts

#### Fixture
**×”×’×“×¨×”**: setup code ×©×¨×¥ ×œ×¤× ×™ ×”×˜×¡×˜  
**×©×™××•×©**: ×”×›× ×ª resources (DB connection, API client)

**×“×•×’××”:**
```python
@pytest.fixture
def focus_server_api(config_manager):
    """Create FocusServerAPI instance."""
    api = FocusServerAPI(config_manager)
    yield api
    # Cleanup (if needed)

# Usage
def test_something(focus_server_api):
    # focus_server_api is ready to use!
    response = focus_server_api.get_channels()
```

---

#### Marker
**×”×’×“×¨×”**: ×ª×’×™×ª ×œ×¡×™×•×•×’ ×˜×¡×˜×™×  
**×“×•×’×××•×ª**:
```python
@pytest.mark.integration
@pytest.mark.api
@pytest.mark.critical
@pytest.mark.smoke
def test_something():
    pass
```

**×”×¨×¦×” ×œ×¤×™ markers:**
```bash
# Run only critical tests
pytest -m critical

# Run integration tests
pytest -m integration

# Run API tests that are NOT slow
pytest -m "api and not slow"
```

---

#### Parametrize
**×”×’×“×¨×”**: ×”×¨×¦×ª ××•×ª×• ×˜×¡×˜ ×¢× inputs ×©×•× ×™×

**×“×•×’××”:**
```python
@pytest.mark.parametrize("nfft", [128, 256, 512, 1024, 2048, 4096])
def test_nfft_value(focus_server_api, nfft):
    """Test each NFFT value."""
    payload = {"nfftSelection": nfft, ...}
    response = focus_server_api.configure(...)
    assert response.job_id

# This creates 6 separate tests!
```

---

## ğŸ¯ ×ª×•×›× ×™×ª ×¢×‘×•×“×” ××¤×•×¨×˜×ª ×œ××•×˜×•××¦×™×”

### Phase 1: High Priority Integration (2-3 weeks)

**×˜×¡×˜×™× ×œ×™×™×©×•×:**
- [ ] PZ-13909: Historic Missing end_time
- [ ] PZ-13907: Historic Missing start_time
- [ ] Completion ×©×œ Historic tests suite

**××©××‘×™× × ×“×¨×©×™×:**
- ×¤×™×ª×•×—: 1 QA Engineer
- ×–××Ÿ: 2-3 ×©×‘×•×¢×•×ª
- ×ª×©×ª×™×ª: ×’×™×©×” ×œ-MongoDB ×¢× historic data

**Deliverables:**
```
tests/integration/api/
â”œâ”€â”€ test_historic_playback_validation.py (NEW)
â”‚   â”œâ”€â”€ TestMissingTimeFields
â”‚   â”‚   â”œâ”€â”€ test_missing_start_time
â”‚   â”‚   â””â”€â”€ test_missing_end_time
â”‚   â”œâ”€â”€ TestInvalidTimeRanges
â”‚   â”‚   â”œâ”€â”€ test_end_before_start
â”‚   â”‚   â””â”€â”€ test_future_timestamps
â”‚   â””â”€â”€ TestHistoricEdgeCases
â”‚       â”œâ”€â”€ test_very_old_timestamps
â”‚       â””â”€â”€ test_zero_duration
```

---

### Phase 2: Infrastructure Tests (1-2 weeks)

**×˜×¡×˜×™× ×œ×™×™×©×•×:**
- [ ] PZ-13900: SSH Access
- [ ] PZ-13899: Kubernetes Connection
- [ ] PZ-13898: MongoDB Connection

**××©××‘×™× × ×“×¨×©×™×:**
- ×’×™×©×ª SSH ×œ-production
- kubeconfig file
- MongoDB credentials

**Deliverables:**
```
tests/infrastructure/
â”œâ”€â”€ test_ssh_connectivity.py (NEW)
â”œâ”€â”€ test_kubernetes_health.py (NEW)
â””â”€â”€ test_mongodb_health.py (UPDATE)
```

---

### Phase 3: Performance & Load (1-2 weeks)

**×˜×¡×˜×™× ×œ×™×™×©×•×:**
- [ ] PZ-13571: Configure latency p95
- [ ] Memory load tests
- [ ] Concurrent task limits

**×›×œ×™× × ×“×¨×©×™×:**
- Locust ××• pytest-benchmark
- ××“×™×“×•×ª resource usage
- monitoring tools

---

### Phase 4: Security Hardening (1 week)

**×˜×¡×˜×™× ×œ×™×™×©×•×:**
- [ ] PZ-13572: Malformed inputs (×œ×”×©×œ×™×)
- [ ] OWASP Top 10 tests
- [ ] Penetration testing basics

**×›×œ×™×:**
- OWASP ZAP
- Security headers validation
- Input fuzzing

---

## âœ… ×¡×™×›×•× ×•×”××œ×¦×•×ª

### ××¦×‘ × ×•×›×—×™

**Strong Points (×—×•×–×§×•×ª):**
- âœ… 83% ××”×˜×¡×˜×™× ×××•××©×™×
- âœ… ×›×™×¡×•×™ ××¦×•×™×Ÿ ×©×œ Happy Path
- âœ… Negative tests ××§×™×¤×™×
- âœ… SingleChannel ×•-ROI ××œ××™×

**Gaps (×—×¡×¨×™×):**
- âš ï¸ Infrastructure tests ×—×œ×§×™×™×
- âš ï¸ Performance tests ×œ× ××•×©×œ××™×
- âš ï¸ Security tests ×‘×¡×™×¡×™×™×
- âš ï¸ E2E ×¢× gRPC ×—×œ×§×™

### ×”××œ×¦×•×ª ×œ×¤×’×™×©×”

**×œ×”×“×’×™×©:**
1. **83% coverage** - ×¨×•×‘ ×”×˜×¡×˜×™× ××•×›× ×™×
2. **Critical tests** - ×›×œ ×”×˜×¡×˜×™× ×”×§×¨×™×˜×™×™× ×××•××©×™× (Nyquist, validations)
3. **Architecture** - ×”×¤×¨×“×” ×‘×¨×•×¨×” ×‘×™×Ÿ ×˜×¡×˜×™×, ×§×•×“ × ×§×™
4. **Documentation** - ×›×œ ×˜×¡×˜ ××ª×•×¢×“ ×”×™×˜×‘

**×œ×¦×™×™×Ÿ ×›×—×¡×¨:**
1. Infrastructure automation - ×“×•×¨×© ×”×©×œ××”
2. E2E ×¢× gRPC - ×“×•×¨×© proto files
3. Performance baseline - ×¦×¨×™×š ×§×•×•×™ ×‘×¡×™×¡

**×©××œ×•×ª ×œ×¤×’×™×©×”:**
1. ××”× ×”-SLAs ×œ×‘×™×¦×•×¢×™×? (latency, throughput)
2. ××”× ×’×‘×•×œ×•×ª ×”××¢×¨×›×ª? (max sensors, max NFFT)
3. ×”×× ×™×© minimum thresholds? (min throughput)
4. Edge cases behavior? (min==max, freq==Nyquist)

---

*××¡××š ×–×” ××›×¡×” 93 ×˜×¡×˜×™× ×‘×¤×™×¨×•×˜ ××œ×*

