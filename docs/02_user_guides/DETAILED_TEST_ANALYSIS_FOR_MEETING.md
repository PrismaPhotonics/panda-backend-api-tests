# × ×™×ª×•×— ××¤×•×¨×˜ ×œ×¤×¨×˜×™ ×¤×¨×˜×™× - ×›×œ ×”×˜×¡×˜×™× ×-Jira

## ××˜×¨×ª ×”××¡××š
××¡××š ×–×” ××›×™×Ÿ ××•×ª×š ×œ×¤×’×™×©×” ×•××›×¡×” ×›×œ ×©××œ×” ××¤×©×¨×™×ª ×œ×’×‘×™ ×›×œ ×˜×¡×˜:
- **××” ×”××˜×¨×”** ×©×œ ×›×œ ×˜×¡×˜
- **××” ×‘×•×“×§×™×** ×‘×“×™×•×§
- **×œ××” ×–×” ×—×™×•× ×™** (×”× ×—×™×¦×•×ª)
- **××™×š ×××©×™×** ×–××ª ×‘×§×•×“
- **×©××œ×•×ª ×¦×¤×•×™×•×ª** ×•×ª×©×•×‘×•×ª ××•×›× ×•×ª

---

## ğŸ“Š ×¡×™×›×•× ×›×œ×œ×™

### ×¡×˜×˜×™×¡×˜×™×§×”
- **×¡×”"×› ×˜×¡×˜×™×**: 13
- **×§×˜×’×•×¨×™×” ×¢×™×§×¨×™×ª**: Data Quality & Integrity
- **×¤×•×§×•×¡**: MongoDB, PostgreSQL, Historic Playback
- **Priority Distribution**:
  - Critical: 2 ×˜×¡×˜×™×
  - High: 4 ×˜×¡×˜×™×  
  - Medium: 7 ×˜×¡×˜×™×

### ×—×œ×•×§×” ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª

#### 1. **MongoDB Infrastructure Tests** (5 ×˜×¡×˜×™×)
- `PZ-13809`: Verify Required Collections Exist
- `PZ-13810`: Verify Critical Indexes Exist
- `PZ-13811`: Validate Recordings Document Schema
- `PZ-13812`: Verify Recordings Have Complete Metadata
- `PZ-13598`: Mongo Collections and Schema (Parent)

#### 2. **MongoDB Data Quality Tests** (3 ×˜×¡×˜×™×)
- `PZ-13683`: MongoDB Collections Exist
- `PZ-13684`: node4 Schema Validation
- `PZ-13685`: Recordings Metadata Completeness
- `PZ-13686`: MongoDB Indexes Validation

#### 3. **Data Lifecycle & Classification** (1 ×˜×¡×˜)
- `PZ-13705`: Historical vs Live Recordings Classification

#### 4. **Historic Playback & Integrity** (1 ×˜×¡×˜)
- `PZ-13867`: Historic Playback - Data Integrity Validation

#### 5. **PostgreSQL Tests** (1 ×˜×¡×˜)
- `PZ-13599`: Postgres Connectivity and Catalogs

---

# ğŸ”¬ × ×™×ª×•×— ××¤×•×¨×˜ ×©×œ ×›×œ ×˜×¡×˜

---

## ×˜×¡×˜ #1: PZ-13867 - Historic Playback Data Integrity

### ğŸ¯ ××” ×”××˜×¨×” ×©×œ ×”×˜×¡×˜?
**×”××˜×¨×” ×”××¨×›×–×™×ª**: ×œ×•×•×“× ×©×›×œ ×”× ×ª×•× ×™× ×©×—×•×–×¨×™× ×-Historic Playback ×ª×§×™× ×™×, ××¡×•×“×¨×™× ×›×¨×•× ×•×œ×•×’×™×ª, ×•×©×œ××™× - ×œ×œ× × ×ª×•× ×™× ×¤×’×•××™× ××• ×—×¡×¨×™×.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### 1. **Timestamp Integrity (×©×œ××•×ª ×—×•×ª××•×ª ×–××Ÿ)**
```python
# ×‘×•×“×§×™× ×©×œ×•×©×” ×“×‘×¨×™×:
assert row.startTimestamp <= row.endTimestamp  # ×–××Ÿ ×”×ª×—×œ×” ×œ× ××—×¨×™ ×–××Ÿ ×¡×™×•×
assert row.startTimestamp >= last_timestamp    # ×¡×“×¨ ×›×¨×•× ×•×œ×•×’×™ × ×©××¨
assert no_duplicate_timestamps                 # ××™×Ÿ ×›×¤×™×œ×•×™×•×ª
```

**×œ××” ×–×” ×§×¨×™×˜×™?**
- ×× ×–××Ÿ ×”×ª×—×œ×” > ×–××Ÿ ×¡×™×•× â†’ ×”× ×ª×•× ×™× ×¤×’×•××™×
- ×× ×”×¡×“×¨ ×œ× ×›×¨×•× ×•×œ×•×’×™ â†’ ×”×¦×’×” ×œ× × ×›×•× ×” ×‘-UI
- ×›×¤×™×œ×•×™×•×ª â†’ × ×ª×•× ×™× ××™×•×ª×¨×™×, ×‘×–×‘×•×– ××©××‘×™×

#### 2. **Sensor Data Completeness (×©×œ××•×ª × ×ª×•× ×™ ×—×™×™×©× ×™×)**
```python
assert len(row.sensors) > 0           # ×™×© ×œ×¤×—×•×ª ×—×™×™×©×Ÿ ××—×“
assert sensor.id >= 0                 # ID ×©×œ ×—×™×™×©×Ÿ ×ª×§×™×Ÿ
assert len(sensor.intensity) > 0     # ×™×© × ×ª×•× ×™ ××™× ×˜× ×¡×™×‘×™×•×ª
```

**×œ××” ×–×” ×§×¨×™×˜×™?**
- ×œ×œ× ×—×™×™×©× ×™× â†’ ××™×Ÿ ××” ×œ×”×¦×™×’ ×‘-waterfall
- ID ×©×œ×™×œ×™ â†’ ×©×’×™××” ×œ×•×’×™×ª, × ×ª×•×Ÿ ×¤×’×•×
- intensity ×¨×™×§ â†’ ××™×Ÿ × ×ª×•× ×™ ×’×¨×£ ×‘×¤×•×¢×œ

#### 3. **Data Volume Validation (×•×•×œ×™×“×¦×™×” ×©×œ ×›××•×ª × ×ª×•× ×™×)**
```python
assert len(all_rows) > 0              # ×§×™×‘×œ× ×• × ×ª×•× ×™× ×‘×›×œ×œ
assert waterfall_response.status_code == 208  # Playback ×”×•×©×œ× ×ª×§×™×Ÿ
```

### ğŸ”´ ×œ××” ×”×˜×¡×˜ ×”×–×” × ×—×™×¥?

#### ×ª×¨×—×™×©×™ ×›×©×œ ×œ×œ× ×”×˜×¡×˜:
1. **UI Crash**: ×× ×™×© × ×ª×•× ×™× ×¤×’×•××™×, ×”-UI ×™×ª×¨×¡×§ ×‘× ×™×¡×™×•×Ÿ ×œ×”×¦×™×’ waterfall
2. **Wrong Timeline**: ×¡×“×¨ ×œ× × ×›×•×Ÿ â†’ ×”××©×ª××© ×¨×•××” ××™×¨×•×¢×™× ×‘×¡×“×¨ ×©×’×•×™
3. **Data Loss Detection**: ××–×”×” ×—×•×¨×™× ×‘× ×ª×•× ×™× ×œ×¤× ×™ ×©×”××©×ª××© ××’×œ×”
4. **Performance Issues**: × ×ª×•× ×™× ×›×¤×•×œ×™× ×’×•×¨××™× ×œ×¢×•××¡ ××™×•×ª×¨

#### Business Impact:
- **×××™× ×•×ª**: ×œ×§×•×—×•×ª ×¡×•××›×™× ×©×”×”×™×¡×˜×•×¨×™×” ××“×•×™×§×ª
- **Forensics**: × ×™×ª×•×— ××™×¨×•×¢×™× ×“×•×¨×© × ×ª×•× ×™× ××•×©×œ××™×
- **Compliance**: ×‘×¨×’×•×œ×¦×™×”, × ×“×¨×©×ª ×©×œ××•×ª ××•×—×œ×˜×ª

### ğŸ’» ××™×š ×××©×™× ××ª ×–×” ×‘×§×•×“?

#### **××¨×›×™×˜×§×˜×•×¨×”**:
```
tests/integration/api/test_historic_playback_flow.py
    â”‚
    â”œâ”€> FocusServerAPI.config_task()        # ×©×œ×‘ 1: ×”×’×“×¨×ª task
    â”œâ”€> Loop: FocusServerAPI.get_waterfall() # ×©×œ×‘ 2: ××™×¡×•×£ × ×ª×•× ×™×
    â””â”€> Validation Logic                     # ×©×œ×‘ 3: ×‘×“×™×§×•×ª integrity
```

#### **×§×•×“ ××œ×**:
```python
import pytest
import time
from datetime import datetime
from typing import List

class TestHistoricPlaybackDataIntegrity:
    """
    Test Suite: Historic Playback Data Integrity
    Purpose: Ensure all data returned from historic playback is valid
    """
    
    @pytest.fixture
    def historic_time_range(self):
        """
        Generate a 5-minute time range for testing.
        Returns: (start_time, end_time) in yymmddHHMMSS format
        """
        # Use known good recording time range
        return ("250101120000", "250101120500")  # 5 minutes
    
    @pytest.fixture
    def historic_config(self, historic_time_range):
        """
        Build config payload for historic playback task.
        """
        start_time, end_time = historic_time_range
        return {
            "displayTimeAxisDuration": 10,
            "nfftSelection": 1024,
            "displayInfo": {"height": 1000},
            "channels": {"min": 0, "max": 50},
            "frequencyRange": {"min": 0, "max": 500},
            "start_time": start_time,
            "end_time": end_time,
            "view_type": 0  # Historic playback mode
        }
    
    def test_historic_playback_data_integrity(
        self,
        focus_server_api,
        historic_config,
        logger
    ):
        """
        Test: Historic Playback Data Integrity Validation
        
        Steps:
        1. Configure historic task (5-minute range)
        2. Poll GET /waterfall until completion (status 208)
        3. Validate EVERY row for:
           - Timestamp ordering (start <= end)
           - Sequential timestamps (no gaps/overlaps)
           - Sensor data presence (len > 0)
           - Valid sensor IDs (>= 0)
           - Non-empty intensity arrays
        4. Track statistics and report
        
        Expected: ALL integrity checks pass, no corrupted data
        """
        task_id = f"integrity_test_{int(time.time())}"
        
        # ============================================
        # STEP 1: Configure Historic Task
        # ============================================
        logger.info(f"Configuring historic task: {task_id}")
        response = focus_server_api.config_task(
            task_id=task_id,
            config_payload=historic_config
        )
        
        assert response.status == "Config received successfully", \
            f"Failed to configure task: {response.status}"
        
        # ============================================
        # STEP 2: Collect All Data Blocks
        # ============================================
        all_rows = []
        last_timestamp = 0
        timestamp_set = set()  # Track duplicates
        
        max_attempts = 100  # Prevent infinite loop
        
        for attempt in range(1, max_attempts + 1):
            logger.debug(f"Polling waterfall (attempt {attempt})...")
            
            waterfall_response = focus_server_api.get_waterfall(
                task_id=task_id,
                max_rows=20
            )
            
            # ============================================
            # HANDLE RESPONSE STATUS
            # ============================================
            if waterfall_response.status_code == 201:
                # Data available
                if not waterfall_response.data:
                    time.sleep(2.0)
                    continue
                
                # Process each block
                for block in waterfall_response.data:
                    for row in block.rows:
                        all_rows.append(row)
                        
                        # ============================================
                        # VALIDATION 1: Timestamp Ordering
                        # ============================================
                        assert row.startTimestamp <= row.endTimestamp, \
                            f"ROW {len(all_rows)}: start > end " \
                            f"({row.startTimestamp} > {row.endTimestamp})"
                        
                        # ============================================
                        # VALIDATION 2: Sequential Order
                        # ============================================
                        assert row.startTimestamp >= last_timestamp, \
                            f"ROW {len(all_rows)}: timestamps not sequential " \
                            f"(last={last_timestamp}, current={row.startTimestamp})"
                        
                        # ============================================
                        # VALIDATION 3: No Duplicates
                        # ============================================
                        timestamp_key = (row.startTimestamp, row.endTimestamp)
                        assert timestamp_key not in timestamp_set, \
                            f"ROW {len(all_rows)}: duplicate timestamp detected " \
                            f"({timestamp_key})"
                        timestamp_set.add(timestamp_key)
                        
                        # Update tracker
                        last_timestamp = row.endTimestamp
                        
                        # ============================================
                        # VALIDATION 4: Sensor Data Presence
                        # ============================================
                        assert len(row.sensors) > 0, \
                            f"ROW {len(all_rows)}: no sensor data found"
                        
                        # ============================================
                        # VALIDATION 5: Sensor Validity
                        # ============================================
                        for sensor_idx, sensor in enumerate(row.sensors):
                            # Check sensor ID
                            assert sensor.id >= 0, \
                                f"ROW {len(all_rows)}, SENSOR {sensor_idx}: " \
                                f"invalid sensor ID ({sensor.id})"
                            
                            # Check intensity data
                            assert len(sensor.intensity) > 0, \
                                f"ROW {len(all_rows)}, SENSOR {sensor_idx}: " \
                                f"empty intensity array"
                        
                        # Log progress every 50 rows
                        if len(all_rows) % 50 == 0:
                            logger.info(f"Validated {len(all_rows)} rows...")
            
            elif waterfall_response.status_code == 208:
                # Playback complete
                logger.info("Historic playback completed (status 208)")
                break
            
            elif waterfall_response.status_code == 206:
                # Processing, wait
                time.sleep(2.0)
            
            else:
                pytest.fail(
                    f"Unexpected status code: {waterfall_response.status_code}"
                )
        
        # ============================================
        # STEP 3: Final Assertions
        # ============================================
        assert len(all_rows) > 0, \
            "No rows collected during playback! Check if recording exists."
        
        # ============================================
        # STEP 4: Statistics & Report
        # ============================================
        total_sensors = sum(len(row.sensors) for row in all_rows)
        avg_sensors_per_row = total_sensors / len(all_rows)
        
        time_range = (
            all_rows[-1].endTimestamp - all_rows[0].startTimestamp
        ) / 1_000_000  # Convert microseconds to seconds
        
        logger.info("=" * 60)
        logger.info("HISTORIC PLAYBACK DATA INTEGRITY TEST: PASSED âœ…")
        logger.info("=" * 60)
        logger.info(f"Total rows validated: {len(all_rows)}")
        logger.info(f"Total sensors processed: {total_sensors}")
        logger.info(f"Avg sensors per row: {avg_sensors_per_row:.1f}")
        logger.info(f"Time range covered: {time_range:.1f} seconds")
        logger.info(f"No corrupted data detected")
        logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×¦×¤×•×™×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×” 1: "×œ××” ×œ× ××¡×¤×™×§ ×œ×‘×“×•×§ ×¨×§ ××ª ×”×¡×˜×˜×•×¡ ×§×•×“?"**
**×ª×©×•×‘×”**: 
```
×¡×˜×˜×•×¡ ×§×•×“ 208 ××•××¨ ×©-playback ×”×¡×ª×™×™×, ××‘×œ ×œ× ××‘×˜×™×— ×©×”× ×ª×•× ×™× ×ª×§×™× ×™×.
×“×•×’××”: ××¤×©×¨ ×œ×§×‘×œ status 208 ××‘×œ ×¢×:
- × ×ª×•× ×™× ×œ× ××¡×•×“×¨×™× ×›×¨×•× ×•×œ×•×’×™×ª
- ×—×™×™×©× ×™× ×¢× intensity ×¨×™×§
- timestamps ×›×¤×•×œ×™×

×”×˜×¡×˜ ×©×œ× ×• ×¢×•×©×” DEEP VALIDATION ×¢×œ ×›×œ row.
```

#### **×©××œ×” 2: "××” ×§×•×¨×” ×× ×”×˜×¡×˜ × ×›×©×œ?"**
**×ª×©×•×‘×”**:
```
1. ××–×”×” ×‘×“×™×•×§ ××™×¤×” ×”×‘×¢×™×” (row number, sensor index)
2. ××¡×¤×§ assertion message ××¤×•×¨×˜
3. ×××¤×©×¨ debug ××”×™×¨
4. ××•× ×¢ deployment ×©×œ version ×¤×’×•××”

×“×•×’××ª ×©×’×™××”:
"ROW 47, SENSOR 3: empty intensity array"
â†’ ××™×“ ×™×•×“×¢×™× ×œ×—×¤×© ×‘×¢×™×” ×‘×—×™×™×©×Ÿ 3, row 47
```

#### **×©××œ×” 3: "×›××” ×–××Ÿ ×”×˜×¡×˜ ×œ×•×§×—?"**
**×ª×©×•×‘×”**:
```
- Playback ×©×œ 5 ×“×§×•×ª: ~30-60 ×©× ×™×•×ª
- Validation ×©×œ ×›×œ row: ~1ms per row
- ×¡×”"×› ×–××Ÿ ×¨×™×¦×”: 1-2 ×“×§×•×ª

×–×” ××•×¤×˜×™××œ×™ ×›×™:
1. ×¨×§ 5 ×“×§×•×ª ×”×™×¡×˜×•×¨×™×” (×œ× ×©×¢×•×ª)
2. Validation ×‘××§×‘×™×œ ×œ××™×¡×•×£
3. Early exit ×× ×™×© ×©×’×™××”
```

#### **×©××œ×” 4: "××™×š ×‘×•×—×¨×™× ××ª ×˜×•×•×— ×”×–××Ÿ ×œ×˜×¡×˜?"**
**×ª×©×•×‘×”**:
```python
# ××¡×˜×¨×˜×’×™×™×ª ×‘×—×™×¨×”:
def choose_test_time_range():
    # Option 1: Known good recording (×”×›×™ ××•××œ×¥)
    return query_last_successful_recording()
    
    # Option 2: Recent recording
    return get_recordings_from_last_hour()
    
    # Option 3: Fixed test data
    return ("250101120000", "250101120500")

# ×œ××” 5 ×“×§×•×ª?
# - ××¡×¤×™×§ × ×ª×•× ×™× ×œ×•×•×œ×™×“×¦×™×” (100+ rows)
# - ×œ× ××¨×•×š ××“×™ (××”×™×¨ ×œ×¨×•×¥)
# - ××›×¡×” ×›××” channels
```

#### **×©××œ×” 5: "××” ×¢×•×©×™× ×× ×™×© gaps ×‘× ×ª×•× ×™×?"**
**×ª×©×•×‘×”**:
```
×”×˜×¡×˜ ××–×”×” gaps ×“×¨×š:
1. Sequential timestamp check
2. Expected vs actual row count

×× ×™×© gap â†’ ×”×˜×¡×˜ ×™×–×”×” ×©×”×–××Ÿ ×§×¤×¥ ×™×•×ª×¨ ××“×™.

××‘×œ: gaps ×œ×’×™×˜×™××™×™× ××:
- ×”×™×” ×”×¤×¡×§×” ×‘×”×§×œ×˜×”
- ×—×™×™×©×Ÿ ×”×™×” offline

×œ×›×Ÿ ×‘×•×“×§×™×:
âœ… Timestamps ××¡×•×“×¨×™× (××¤×™×œ×• ×¢× gaps)
âŒ Timestamps ×œ× ××¡×•×“×¨×™× (×‘×¢×™×”!)
```

---

## ×˜×¡×˜ #2: PZ-13812 - Verify Recordings Have Complete Metadata

### ğŸ¯ ××” ×”××˜×¨×” ×©×œ ×”×˜×¡×˜?
**×”××˜×¨×”**: ×œ×•×•×“× ×©×›×œ ×”×”×§×œ×˜×•×ª ×‘-MongoDB ××›×™×œ×•×ª ××ª ×›×œ ×©×“×•×ª ×”-metadata ×”× ×“×¨×©×™× - ×œ×œ× ×©×“×•×ª ×—×¡×¨×™×, null, ××• ×¨×™×§×™×.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### 1. **Required Fields Presence**
```python
required_fields = ["uuid", "start_time", "end_time", "path", "node"]

for recording in sample:
    for field in required_fields:
        assert field in recording        # ×”×©×“×” ×§×™×™×
        assert recording[field] is not None  # ×œ× null
        assert recording[field] != ""    # ×œ× ×¨×™×§
```

**×œ××” ×›×œ ×©×“×” ×§×¨×™×˜×™?**

| Field | Purpose | What Happens If Missing |
|-------|---------|-------------------------|
| `uuid` | Unique identifier | Cannot reference recording |
| `start_time` | Recording start timestamp | Cannot query by time |
| `end_time` | Recording end timestamp | Cannot calculate duration |
| `path` | File location | Cannot load raw data |
| `node` | Recording node ID | Cannot filter by source |

#### 2. **Value Validation**
```python
# Time fields must be positive
assert recording["start_time"] > 0
assert recording["end_time"] > 0

# String fields must be non-empty
assert len(recording["uuid"]) > 0
assert len(recording["path"]) > 0
```

#### 3. **Sample Size Validation**
```python
sample = list(recordings.find().sort("start_time", -1).limit(10))
assert len(sample) >= 10, "Not enough recordings for testing"
```

**×œ××” 10 recordings?**
- ××¡×¤×™×§ ×’×“×•×œ ×œ×–×”×•×ª patterns
- ×œ× ××“×’×•× ××ª ×›×œ ×”-DB (×™×¢×™×œ)
- Statistical confidence

### ğŸ”´ ×œ××” ×”×˜×¡×˜ ×”×–×” × ×—×™×¥?

#### ×ª×¨×—×™×©×™ ×›×©×œ:
1. **POST /recordings_in_time_range fails**
   ```python
   # ×× ×—×¡×¨ start_time ××• end_time:
   query = {"start_time": {"$gte": start, "$lte": end}}
   # â†’ Query returns nothing or crashes
   ```

2. **Cannot Load Raw Data**
   ```python
   # ×× ×—×¡×¨ path:
   file_path = recording["path"]  # KeyError!
   data = load_prp2_file(file_path)  # Cannot proceed
   ```

3. **UUID Collision**
   ```python
   # ×× uuid ×¨×™×§ ××• null â†’ ×›×¤×™×œ×•×™×•×ª
   recordings = {"": recording1, "": recording2}
   # â†’ Data corruption
   ```

### ğŸ’» ××™×š ×××©×™× ××ª ×–×” ×‘×§×•×“?

#### **File Location**:
```
tests/integration/infrastructure/test_mongodb_data_quality.py
```

#### **Full Implementation**:
```python
import pytest
from pymongo import MongoClient
from typing import List, Dict, Any

class TestMongoDBDataQuality:
    """
    Test Suite: MongoDB Data Quality
    Focus: Metadata completeness and integrity
    """
    
    @pytest.fixture
    def mongo_client(self, config_manager):
        """
        Establish MongoDB connection.
        Yields: Connected MongoClient instance
        """
        mongo_config = config_manager.get_mongodb_config()
        
        client = MongoClient(
            host=mongo_config["host"],
            port=mongo_config["port"],
            username=mongo_config["username"],
            password=mongo_config["password"],
            serverSelectionTimeoutMS=5000  # 5 sec timeout
        )
        
        # Test connection
        client.admin.command('ping')
        
        yield client
        
        # Cleanup
        client.close()
    
    @pytest.fixture
    def recordings_collection(self, mongo_client, config_manager):
        """
        Get recordings collection.
        Returns: MongoDB collection object
        """
        db_name = config_manager.get("mongodb.database_name")
        db = mongo_client[db_name]
        return db["recordings"]
    
    def test_recordings_have_all_required_metadata(
        self,
        recordings_collection,
        logger
    ):
        """
        Test: Verify Recordings Have Complete Metadata
        
        Purpose:
        Ensure all recordings in MongoDB contain all required metadata
        fields populated (not null/empty).
        
        Business Impact:
        - Missing metadata breaks history playback
        - Empty UUIDs cause data corruption
        - Missing paths prevent raw data loading
        
        Steps:
        1. Sample 10 recent recordings
        2. Verify all required fields exist
        3. Verify no null or empty values
        4. Verify time values are positive
        
        Expected: All recordings have complete, valid metadata
        """
        
        # ============================================
        # STEP 1: Get Sample Recordings
        # ============================================
        SAMPLE_SIZE = 10
        
        logger.info(f"Fetching {SAMPLE_SIZE} recent recordings...")
        
        sample = list(
            recordings_collection.find()
            .sort("start_time", -1)  # Most recent first
            .limit(SAMPLE_SIZE)
        )
        
        assert len(sample) >= SAMPLE_SIZE, \
            f"Insufficient recordings: found {len(sample)}, " \
            f"expected {SAMPLE_SIZE}. Database may be empty."
        
        logger.info(f"âœ“ Retrieved {len(sample)} recordings for validation")
        
        # ============================================
        # STEP 2: Define Required Fields
        # ============================================
        required_fields = {
            # Field name: (expected_type, validation_function)
            "uuid": (str, lambda v: len(v) > 0),
            "start_time": ((int, float), lambda v: v > 0),
            "end_time": ((int, float), lambda v: v > 0),
            "path": (str, lambda v: len(v) > 0),
            "node": (str, lambda v: len(v) > 0)
        }
        
        # ============================================
        # STEP 3: Validate Each Recording
        # ============================================
        validation_errors = []
        
        for idx, recording in enumerate(sample, 1):
            logger.info(f"\nValidating recording {idx}/{len(sample)}:")
            logger.info(f"  UUID: {recording.get('uuid', 'MISSING')}")
            
            # Check each required field
            for field_name, (expected_type, validator) in required_fields.items():
                
                # ----------------------------------------
                # Check 1: Field Exists
                # ----------------------------------------
                if field_name not in recording:
                    error = f"Recording {idx}: Missing field '{field_name}'"
                    validation_errors.append(error)
                    logger.error(f"  âŒ {error}")
                    continue
                
                value = recording[field_name]
                
                # ----------------------------------------
                # Check 2: Not None
                # ----------------------------------------
                if value is None:
                    error = f"Recording {idx}: Field '{field_name}' is None"
                    validation_errors.append(error)
                    logger.error(f"  âŒ {error}")
                    continue
                
                # ----------------------------------------
                # Check 3: Not Empty (strings)
                # ----------------------------------------
                if isinstance(expected_type, type) and expected_type == str:
                    if value == "":
                        error = f"Recording {idx}: Field '{field_name}' is empty"
                        validation_errors.append(error)
                        logger.error(f"  âŒ {error}")
                        continue
                
                # ----------------------------------------
                # Check 4: Correct Type
                # ----------------------------------------
                if isinstance(expected_type, tuple):
                    type_ok = isinstance(value, expected_type)
                else:
                    type_ok = isinstance(value, expected_type)
                
                if not type_ok:
                    error = (
                        f"Recording {idx}: Field '{field_name}' has wrong type "
                        f"(got {type(value).__name__}, expected {expected_type})"
                    )
                    validation_errors.append(error)
                    logger.error(f"  âŒ {error}")
                    continue
                
                # ----------------------------------------
                # Check 5: Custom Validation
                # ----------------------------------------
                try:
                    is_valid = validator(value)
                    if not is_valid:
                        error = (
                            f"Recording {idx}: Field '{field_name}' "
                            f"failed validation (value={value})"
                        )
                        validation_errors.append(error)
                        logger.error(f"  âŒ {error}")
                        continue
                except Exception as e:
                    error = (
                        f"Recording {idx}: Field '{field_name}' "
                        f"validation error: {e}"
                    )
                    validation_errors.append(error)
                    logger.error(f"  âŒ {error}")
                    continue
                
                # Success
                logger.info(f"  âœ“ Field '{field_name}': OK")
            
            # ----------------------------------------
            # Additional Logic Checks
            # ----------------------------------------
            if "start_time" in recording and "end_time" in recording:
                if recording["start_time"] >= recording["end_time"]:
                    error = (
                        f"Recording {idx}: start_time >= end_time "
                        f"({recording['start_time']} >= {recording['end_time']})"
                    )
                    validation_errors.append(error)
                    logger.error(f"  âŒ {error}")
        
        # ============================================
        # STEP 4: Final Assertions
        # ============================================
        if validation_errors:
            error_summary = "\n".join(f"  - {err}" for err in validation_errors)
            pytest.fail(
                f"\n\nMetadata validation FAILED! "
                f"Found {len(validation_errors)} errors:\n{error_summary}"
            )
        
        # ============================================
        # STEP 5: Success Report
        # ============================================
        logger.info("\n" + "=" * 60)
        logger.info("METADATA COMPLETENESS TEST: PASSED âœ…")
        logger.info("=" * 60)
        logger.info(f"Validated {len(sample)} recordings")
        logger.info(f"All recordings have complete metadata")
        logger.info(f"Required fields checked: {', '.join(required_fields.keys())}")
        logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×¦×¤×•×™×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×” 1: "×œ××” ×œ×‘×“×•×§ ×¨×§ 10 recordings ×•×œ× ××ª ×›×•×œ×?"**
**×ª×©×•×‘×”**:
```
Sampling Strategy:
1. ×™×¢×™×œ×•×ª: ×‘×“×™×§×ª ×›×œ ×”-DB (1000+ recordings) ×ª×™×§×— ×“×§×•×ª
2. ××™×™×¦×’: 10 recordings ××—×¨×•× ×™× ××™×™×¦×’×™× ××ª ×”××¦×‘ ×”× ×•×›×—×™
3. Early Detection: ×× ×™×© ×‘×¢×™×”, ×›×›×œ ×”× ×¨××” × ×’×œ×” ××•×ª×” ×‘-10 ×”×¨××©×•× ×™×

××‘×œ: ××¤×©×¨ ×œ×”×•×¡×™×£ flag ×œ×‘×“×™×§×” ××œ××” ×‘CI/CD:
```python
@pytest.mark.full_scan
def test_all_recordings_metadata():
    # Scan entire collection
    sample = recordings_collection.find()
```

#### **×©××œ×” 2: "××” ×× ×™×© recording ×œ×’×™×˜×™××™ ×‘×œ×™ end_time?"**
**×ª×©×•×‘×”**:
```python
# Recordings ×—×™×™× (Live) ×™×›×•×œ×™× ×œ×”×™×•×ª ×‘×œ×™ end_time
# ×”×˜×¡×˜ ×¦×¨×™×š ×œ×”×ª×—×©×‘ ×‘×–×”:

if "deleted" in recording and recording["deleted"] == False:
    if "end_time" not in recording or recording["end_time"] is None:
        # This might be a LIVE recording - check age
        age_hours = (now - recording["start_time"]) / 3600
        if age_hours > 24:
            # Stale recording - probably crashed
            logger.warning(f"Stale recording: {recording['uuid']}")
        else:
            # Live recording - OK
            continue
```

#### **×©××œ×” 3: "××™×š ××˜×¤×œ×™× ×‘-recordings ×©× ××—×§×•?"**
**×ª×©×•×‘×”**:
```python
# Option 1: Skip deleted recordings
sample = recordings_collection.find({"deleted": False})

# Option 2: Test deleted recordings separately
@pytest.mark.parametrize("deleted_status", [True, False])
def test_metadata_by_deletion_status(deleted_status):
    sample = recordings_collection.find({"deleted": deleted_status})
    # Validate...

# ×”××œ×¦×”: ×‘×“×•×§ ×¨×§ non-deleted ×›×™ deleted ×™×›×•×œ×™× ×œ×”×™×•×ª ×—×œ×§×™×™×
```

---

## ×˜×¡×˜ #3: PZ-13811 - Validate Recordings Document Schema

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×›×œ document ×‘-`recordings` collection ×™×© ××ª ×›×œ ×”×©×“×•×ª ×”× ×“×¨×©×™× ×¢× ×”×˜×™×¤×•×¡×™× ×”× ×›×•× ×™×**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Schema Definition**
```python
EXPECTED_SCHEMA = {
    "uuid": str,
    "start_time": (int, float),   # Unix epoch
    "end_time": (int, float),     # Unix epoch  
    "path": str,                  # File path
    "node": str,                  # Node identifier
    "sensor_min": (int, float),   # Optional
    "sensor_max": (int, float)    # Optional
}
```

#### **Type Validation**
```python
recording = recordings.find_one(sort=[("start_time", -1)])

# Check each field type
assert isinstance(recording["uuid"], str)
assert isinstance(recording["start_time"], (int, float))
assert isinstance(recording["end_time"], (int, float))
```

#### **Logical Validation**
```python
# Time logic
assert recording["start_time"] < recording["end_time"]

# Path format
assert recording["path"].endswith(".prp2") or \
       recording["path"].endswith(".segy")

# Sensor range
if "sensor_min" in recording and "sensor_max" in recording:
    assert recording["sensor_min"] <= recording["sensor_max"]
```

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **Type Mismatch = Runtime Errors**
```python
# ×× start_time ×”×•× string ×‘××§×•× number:
start_time = recording["start_time"]  # "2025-01-01"
if start_time > threshold:            # TypeError!
```

#### **Schema Drift Detection**
```
×–××Ÿ â†’ ××™×©×”×• ××©× ×” ××ª ×”×§×•×“
     â†’ ××ª×—×™×œ ×œ×›×ª×•×‘ ×©×“×” ×—×“×© ×‘×¤×•×¨××˜ ×©×•× ×”
     â†’ ×”×˜×¡×˜ ×§×•×œ×˜ ××ª ×–×” ××™×™×“
```

### ğŸ’» Implementation

```python
def test_recording_schema_validation(
    self,
    recordings_collection,
    logger
):
    """
    Test: Validate Recordings Document Schema
    
    Purpose: Verify document structure and field types
    """
    
    # Get one recent recording
    recording = recordings_collection.find_one(
        sort=[("start_time", -1)]
    )
    
    assert recording is not None, "No recordings found in database"
    
    logger.info(f"Validating schema for recording: {recording.get('uuid')}")
    
    # ============================================
    # Required Fields Check
    # ============================================
    required_fields = ["uuid", "start_time", "end_time", "path"]
    
    for field in required_fields:
        assert field in recording, f"Missing required field: {field}"
        logger.info(f"âœ“ Field '{field}' present")
    
    # ============================================
    # Type Validation
    # ============================================
    assert isinstance(recording["uuid"], str), \
        f"uuid must be string, got {type(recording['uuid'])}"
    
    assert isinstance(recording["start_time"], (int, float)), \
        f"start_time must be number, got {type(recording['start_time'])}"
    
    assert isinstance(recording["end_time"], (int, float)), \
        f"end_time must be number, got {type(recording['end_time'])}"
    
    assert isinstance(recording["path"], str), \
        f"path must be string, got {type(recording['path'])}"
    
    logger.info("âœ“ All field types correct")
    
    # ============================================
    # Logical Validation
    # ============================================
    assert recording["start_time"] < recording["end_time"], \
        f"Invalid time range: {recording['start_time']} >= {recording['end_time']}"
    
    logger.info("âœ“ Time range logic valid")
    
    # ============================================
    # Optional Fields
    # ============================================
    if "sensor_min" in recording:
        assert isinstance(recording["sensor_min"], (int, float))
        logger.info(f"âœ“ Optional field 'sensor_min' valid: {recording['sensor_min']}")
    
    if "sensor_max" in recording:
        assert isinstance(recording["sensor_max"], (int, float))
        logger.info(f"âœ“ Optional field 'sensor_max' valid: {recording['sensor_max']}")
    
    if "sensor_min" in recording and "sensor_max" in recording:
        assert recording["sensor_min"] <= recording["sensor_max"], \
            "sensor_min > sensor_max"
        logger.info("âœ“ Sensor range logic valid")
    
    logger.info("\n" + "=" * 60)
    logger.info("RECORDING SCHEMA VALIDATION: PASSED âœ…")
    logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "××” ×”×”×‘×“×œ ×‘×™×Ÿ ×”×˜×¡×˜ ×”×–×” ×œ-PZ-13812?"**
**×ª×©×•×‘×”**:
```
PZ-13811 (Schema Validation):
- ×‘×•×“×§ STRUCTURE (××™×œ×• ×©×“×•×ª ×§×™×™××™×)
- ×‘×•×“×§ TYPES (×”×× ×”××¡×¤×¨×™× ×‘×××ª ××¡×¤×¨×™×)
- ×“×•×’××” 1 recording

PZ-13812 (Metadata Completeness):
- ×‘×•×“×§ VALUES (×”×× ×™×© ×¢×¨×›×™× ××• null)
- ×‘×•×“×§ EMPTINESS (×”×× strings ×¨×™×§×™×)
- ×“×•×’××” 10 recordings

×©× ×™×”× ××©×œ×™××™×:
âœ… Schema â†’ "×™×© ×©×“×” start_time ××˜×™×¤×•×¡ number"
âœ… Completeness â†’ "start_time ×œ× null ×•×œ× 0"
```

---

## ×˜×¡×˜ #4: PZ-13810 - Verify Critical MongoDB Indexes Exist

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×›×œ ×”××™× ×“×§×¡×™× ×”×§×¨×™×˜×™×™× ×§×™×™××™× ×¢×œ `recordings` collection - ×›×“×™ ×œ×”×‘×˜×™×— ×‘×™×¦×•×¢×™× ××”×™×¨×™×**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Required Indexes**
```python
REQUIRED_INDEXES = [
    "start_time_1",    # For time-based queries
    "end_time_1",      # For range queries
    "uuid_1",          # For lookups by ID
    "_id_"             # Default MongoDB index
]
```

#### **Why Each Index Matters**

| Index | Query Type | Without Index Performance |
|-------|------------|---------------------------|
| `start_time_1` | `find({"start_time": {$gte: X}})` | O(n) - Full collection scan |
| `end_time_1` | `find({"end_time": {$lte: Y}})` | O(n) - Slow |
| `uuid_1` | `find({"uuid": "abc123"})` | O(n) - Very slow |

**With Indexes**: O(log n) - Fast!

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **Performance Crisis Without Indexes**
```python
# Scenario: Query recordings in time range
# Collection size: 10,000 recordings

# WITHOUT index on start_time:
query_time = 5000ms  # Scans all 10k docs
# â†’ POST /recordings_in_time_range is SLOW
# â†’ Users wait 5+ seconds
# â†’ Bad UX

# WITH index on start_time:
query_time = 50ms    # Uses index
# â†’ Users get results instantly
```

#### **Real-World Impact**
```
×× ××™×Ÿ indexes:
1. History playback ×™×™×§×— ×“×§×•×ª ×‘××§×•× ×©× ×™×•×ª
2. MongoDB CPU spike â†’ server overload
3. Concurrent users â†’ timeout errors
4. Production incident

×”×˜×¡×˜ ××•× ×¢ ××ª ×–×”!
```

### ğŸ’» Implementation

```python
def test_mongodb_indexes_exist_and_optimal(
    self,
    recordings_collection,
    logger
):
    """
    Test: Verify Critical MongoDB Indexes Exist
    
    Purpose: Ensure performance-critical indexes exist
    
    Impact: Missing indexes cause:
    - Slow queries (O(n) instead of O(log n))
    - High CPU usage on MongoDB
    - Timeout errors for users
    - Poor production performance
    """
    
    # ============================================
    # STEP 1: Get All Indexes
    # ============================================
    indexes = list(recordings_collection.list_indexes())
    index_names = [idx['name'] for idx in indexes]
    
    logger.info(f"Found {len(indexes)} indexes on 'recordings' collection:")
    for idx in indexes:
        logger.info(f"  - {idx['name']}: {idx['key']}")
    
    # ============================================
    # STEP 2: Define Required Indexes
    # ============================================
    required_indexes = {
        "start_time_1": {
            "field": "start_time",
            "type": "ascending",
            "reason": "Time-based queries (POST /recordings_in_time_range)"
        },
        "end_time_1": {
            "field": "end_time",
            "type": "ascending",
            "reason": "Range queries for playback"
        },
        "uuid_1": {
            "field": "uuid",
            "type": "ascending",
            "reason": "Unique recording lookups"
        }
    }
    
    # ============================================
    # STEP 3: Verify Each Required Index
    # ============================================
    missing_indexes = []
    
    for idx_name, idx_info in required_indexes.items():
        if idx_name in index_names:
            logger.info(f"âœ“ Index '{idx_name}' exists")
            logger.info(f"  Purpose: {idx_info['reason']}")
        else:
            missing_indexes.append({
                "name": idx_name,
                "field": idx_info["field"],
                "reason": idx_info["reason"]
            })
            logger.error(f"âŒ Missing index: {idx_name}")
            logger.error(f"   Field: {idx_info['field']}")
            logger.error(f"   Impact: {idx_info['reason']} will be SLOW")
    
    # ============================================
    # STEP 4: Assert No Missing Indexes
    # ============================================
    if missing_indexes:
        error_msg = "\n\nCRITICAL: Missing MongoDB indexes detected!\n\n"
        error_msg += "Missing indexes will cause severe performance degradation.\n"
        error_msg += "Users will experience slow queries and timeouts.\n\n"
        error_msg += "Missing indexes:\n"
        
        for idx in missing_indexes:
            error_msg += f"  - {idx['name']} on field '{idx['field']}'\n"
            error_msg += f"    Impact: {idx['reason']}\n"
        
        error_msg += "\nTo fix, run:\n"
        for idx in missing_indexes:
            error_msg += (
                f"  db.recordings.createIndex("
                f"{{\"{idx['field']}\": 1}}, "
                f"{{name: \"{idx['name']}\"}})\n"
            )
        
        pytest.fail(error_msg)
    
    # ============================================
    # STEP 5: Optional - Check Index Statistics
    # ============================================
    logger.info("\n" + "=" * 60)
    logger.info("Checking index usage statistics...")
    
    # Get collection stats
    stats = recordings_collection.database.command("collStats", "recordings")
    
    if "indexSizes" in stats:
        logger.info("\nIndex sizes:")
        for idx_name, size_bytes in stats["indexSizes"].items():
            size_mb = size_bytes / (1024 * 1024)
            logger.info(f"  {idx_name}: {size_mb:.2f} MB")
    
    # ============================================
    # SUCCESS
    # ============================================
    logger.info("\n" + "=" * 60)
    logger.info("MONGODB INDEXES VALIDATION: PASSED âœ…")
    logger.info("=" * 60)
    logger.info(f"All {len(required_indexes)} required indexes exist")
    logger.info("Query performance is optimal")
    logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "××™×š ×™×•×¦×¨×™× ××ª ×”-indexes ×× ×”× ×—×¡×¨×™×?"**
**×ª×©×•×‘×”**:
```python
# Method 1: MongoDB Shell
db.recordings.createIndex({"start_time": 1}, {name: "start_time_1"})
db.recordings.createIndex({"end_time": 1}, {name: "end_time_1"})
db.recordings.createIndex({"uuid": 1}, {name: "uuid_1", unique: true})

# Method 2: Python (automated setup)
from pymongo import ASCENDING, IndexModel

def setup_indexes(collection):
    indexes = [
        IndexModel([("start_time", ASCENDING)], name="start_time_1"),
        IndexModel([("end_time", ASCENDING)], name="end_time_1"),
        IndexModel([("uuid", ASCENDING)], name="uuid_1", unique=True)
    ]
    collection.create_indexes(indexes)
    logger.info("Indexes created successfully")

# Method 3: CI/CD Pipeline
# â†’ ×‘onboarding ×©×œ ×¡×‘×™×‘×” ×—×“×©×”, ×¨×¥ script ×©×™×•×¦×¨ indexes
```

#### **×©××œ×”: "××” ×”-overhead ×©×œ indexes?"**
**×ª×©×•×‘×”**:
```
Trade-offs:
âœ… Pros:
- Queries ×¤×™ 100-1000 ×™×•×ª×¨ ××”×™×¨×•×ª
- ×¤×—×•×ª CPU usage
- ×™×•×ª×¨ concurrent users

âŒ Cons:
- ×ª×•×¤×¡ ××§×•× (×›×œ index ~1-5% ××’×•×“×œ ×”-collection)
- Inserts ××¢×˜ ×™×•×ª×¨ ××™×˜×™×™× (×¦×¨×™×š ×œ×¢×“×›×Ÿ ×’× ××ª ×”-index)

Bottom Line:
×”×‘enefits ×’×“×•×œ×™× ×‘×”×¨×‘×” ××”-cost.
×‘×œ×™ indexes â†’ production unusable.
```

#### **×©××œ×”: "××™×œ×• indexes × ×•×¡×¤×™× ×›×“××™ ×œ×©×§×•×œ?"**
**×ª×©×•×‘×”**:
```python
# Compound indexes for common queries
db.recordings.createIndex(
    {"start_time": 1, "end_time": 1},
    {name: "time_range_compound"}
)
# â†’ ××”×™×¨ ×™×•×ª×¨ ×œrange×§×©×ª×™×

# Index on deleted flag
db.recordings.createIndex(
    {"deleted": 1},
    {name: "deleted_1"}
)
# â†’ ××”×™×¨ ×œ×¡× ×Ÿ recordings ×©×œ× × ××—×§×•

# Partial index (only for non-deleted)
db.recordings.createIndex(
    {"start_time": 1},
    {
        name: "start_time_active",
        partialFilterExpression: {"deleted": false}
    }
)
# â†’ ×—×•×¡×š ××§×•×, ×¨×œ×•×•× ×˜×™ ×¨×§ ×œ-active recordings
```

---

## ×˜×¡×˜ #5: PZ-13809 - Verify Required MongoDB Collections Exist

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×›×œ ×”-collections ×”× ×“×¨×©×™× ×§×™×™××™× ×‘-MongoDB database - ×‘×“×™×§×ª ×ª×©×ª×™×ª ×‘×¡×™×¡×™×ª**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Required Collections**
```python
REQUIRED_COLLECTIONS = [
    "recordings",   # Main metadata storage
    "node4",        # Node-specific data
    "tasks",        # Task management
    "jobs"          # Job queue
]
```

#### **What Each Collection Does**

| Collection | Purpose | What Breaks If Missing |
|------------|---------|------------------------|
| `recordings` | Stores recording metadata | Cannot query history, playback fails |
| `node4` | Node-specific recording info | Cannot identify recording source |
| `tasks` | Active task tracking | Cannot manage waterfall tasks |
| `jobs` | Job processing queue | Cannot process async jobs |

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **×ª×¨×—×™×©×™ ×›×©×œ**
```python
# Scenario: Deployment to new environment
# ×× ×œ× ×¨×¥ setup script â†’ collections ×—×¡×¨×™×

# User tries: POST /recordings_in_time_range
db = client["prisma"]
recordings = db["recordings"]  # Collection doesn't exist!
results = recordings.find(...)  # Returns nothing

# Error: "No recordings found"
# â†’ User thinks system is broken
```

#### **Early Detection**
```
×”×˜×¡×˜ ×¨×¥ FIRST ×‘test suite:
1. ×× collections ×—×¡×¨×™× â†’ ×˜×¡×˜ × ×›×©×œ ××™×“
2. ×œ× ××‘×–×‘×–×™× ×–××Ÿ ×¢×œ ×˜×¡×˜×™× × ×•×¡×¤×™×
3. ×‘×¨×•×¨ ××” ×œ×ª×§×Ÿ
```

### ğŸ’» Implementation

```python
def test_required_collections_exist(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: Verify Required MongoDB Collections Exist
    
    Purpose: Validate database setup and infrastructure
    
    Priority: CRITICAL
    This test runs FIRST - if it fails, other tests will also fail
    """
    
    # ============================================
    # STEP 1: Connect to Database
    # ============================================
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    
    logger.info(f"Connected to database: {db_name}")
    
    # ============================================
    # STEP 2: List All Collections
    # ============================================
    existing_collections = db.list_collection_names()
    
    logger.info(f"Found {len(existing_collections)} collections:")
    for col in existing_collections:
        logger.info(f"  - {col}")
    
    # ============================================
    # STEP 3: Define Required Collections
    # ============================================
    required_collections = {
        "recordings": "Main recording metadata storage",
        "node4": "Node-specific recording information",
        "tasks": "Active task management",
        "jobs": "Asynchronous job processing queue"
    }
    
    # ============================================
    # STEP 4: Check Each Required Collection
    # ============================================
    missing_collections = []
    
    for col_name, purpose in required_collections.items():
        if col_name in existing_collections:
            # Collection exists - verify accessible
            collection = db[col_name]
            
            try:
                count = collection.count_documents({})
                logger.info(f"âœ“ Collection '{col_name}' exists ({count} documents)")
                logger.info(f"  Purpose: {purpose}")
            except Exception as e:
                logger.error(f"âŒ Collection '{col_name}' exists but not accessible: {e}")
                missing_collections.append({
                    "name": col_name,
                    "reason": f"Access error: {e}"
                })
        else:
            logger.error(f"âŒ Collection '{col_name}' MISSING")
            logger.error(f"   Purpose: {purpose}")
            missing_collections.append({
                "name": col_name,
                "reason": "Collection does not exist"
            })
    
    # ============================================
    # STEP 5: Assert No Missing Collections
    # ============================================
    if missing_collections:
        error_msg = "\n\nCRITICAL: Required MongoDB collections are missing!\n\n"
        error_msg += "This indicates incomplete database setup.\n"
        error_msg += "Focus Server will NOT function correctly.\n\n"
        error_msg += "Missing collections:\n"
        
        for col in missing_collections:
            error_msg += f"  - {col['name']}: {col['reason']}\n"
        
        error_msg += "\nTo fix:\n"
        error_msg += "1. Run database setup script:\n"
        error_msg += "   python scripts/setup_mongodb.py\n"
        error_msg += "2. Or create collections manually:\n"
        for col in missing_collections:
            error_msg += f"   db.createCollection('{col['name']}')\n"
        
        pytest.fail(error_msg)
    
    # ============================================
    # SUCCESS
    # ============================================
    logger.info("\n" + "=" * 60)
    logger.info("MONGODB COLLECTIONS VALIDATION: PASSED âœ…")
    logger.info("=" * 60)
    logger.info(f"All {len(required_collections)} required collections exist")
    logger.info("Database infrastructure is ready")
    logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "×”×× MongoDB ×™×•×¦×¨ collections ××•×˜×•××˜×™×ª?"**
**×ª×©×•×‘×”**:
```python
# ×›×Ÿ, ××‘×œ ×–×” ×œ× ×¨×¦×•×™ ×œproduction!

# Auto-creation (implicit):
db = client["prisma"]
recordings = db["recordings"]  # Collection created NOW
recordings.insert_one({"uuid": "test"})  # First insert creates it

# Problem:
# - No schema validation
# - No indexes
# - Wrong settings

# Best Practice (explicit):
db.create_collection(
    "recordings",
    validator={  # Schema validation
        "$jsonSchema": {
            "required": ["uuid", "start_time", "end_time"],
            "properties": {
                "uuid": {"bsonType": "string"},
                "start_time": {"bsonType": ["int", "double"]},
                "end_time": {"bsonType": ["int", "double"]}
            }
        }
    }
)

# Then create indexes:
db.recordings.create_index("start_time")
```

---

## ×˜×¡×˜ #6: PZ-13705 - Historical vs Live Recordings Classification

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×¡×•×•×’ ××ª ×›×œ ×”×”×§×œ×˜×•×ª ×‘-MongoDB ×œ×¤×™ lifecycle stage ×•×œ×–×”×•×ª ×‘×¢×™×•×ª ×‘× ×™×”×•×œ ××—×–×•×¨ ×—×™×™ ×”× ×ª×•× ×™×**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Recording States**

| State | Criteria | Expected % | What It Means |
|-------|----------|------------|---------------|
| **Historical** | `start_time` exists AND `end_time` exists AND `deleted=False` | ~99% | Completed, available recordings |
| **Live** | `start_time` exists AND `end_time=None` AND `deleted=False` | <1% | Currently recording (recent only) |
| **Deleted** | `deleted=True` | <1% | Marked for cleanup |
| **Stale** | Age >24h AND `end_time=None` AND `deleted=False` | 0% | Crashed recordings (BUG!) |
| **Invalid** | `start_time` missing | 0% | Corrupted data (BUG!) |

#### **Validation Logic**
```python
# Historical
historical = db.recordings.count_documents({
    "start_time": {"$exists": True},
    "end_time": {"$ne": None},
    "deleted": False
})

# Live  
live = db.recordings.count_documents({
    "start_time": {"$exists": True},
    "end_time": None,
    "deleted": False
})

# Deleted
deleted = db.recordings.count_documents({
    "deleted": True
})

# Invalid (should be 0!)
invalid = db.recordings.count_documents({
    "start_time": {"$exists": False}
})

# Stale Detection (should be 0!)
now = time.time()
stale_threshold = now - (24 * 3600)  # 24 hours ago

stale = db.recordings.count_documents({
    "start_time": {"$lt": stale_threshold},
    "end_time": None,
    "deleted": False
})
```

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **×ª×¨×—×™×© 1: Stale Recordings**
```python
# ×‘×¢×™×”: recording ×”×ª×—×™×œ ×œ×¤× ×™ 3 ×™××™×, ××™×Ÿ end_time, ×œ× deleted

# ×¡×™×‘×•×ª ××¤×©×¨×™×•×ª:
1. Focus Server crashed mid-recording
2. Recording process hung
3. end_time ×œ× × ×›×ª×‘ ×œDB

# ×”×©×¤×¢×”:
- Looks like "Live" but actually dead
- Wastes storage
- Confuses users
- Indicates reliability issue

# ×”×˜×¡×˜ ××–×”×”:
if stale_count > 0:
    logger.warning(f"Found {stale_count} stale recordings!")
    # â†’ ×× ×—× ×• ×™×•×“×¢×™× ×©×™×© ×‘×¢×™×”
```

#### **×ª×¨×—×™×© 2: Missing Metadata**
```python
# ×× ×™×© recordings ×‘×œ×™ start_time:

# ×–×” ××•××¨:
- Data corruption
- Bug in recording creation
- Database integrity issue

# ×”×˜×¡×˜ ×›×•×©×œ:
assert invalid_count == 0, f"Found {invalid_count} invalid recordings!"
# â†’ ×—×•×¡× deployment
```

#### **×ª×¨×—×™×© 3: Cleanup Service Validation**
```python
# ×‘×•×“×§×™× ×©Cleanup Service ×¢×•×‘×“:

deleted_with_endtime = db.recordings.count_documents({
    "deleted": True,
    "end_time": {"$ne": None}
})

deleted_without_endtime = db.recordings.count_documents({
    "deleted": True,
    "end_time": None
})

# ×× deleted_without_endtime ×’×‘×•×”:
# â†’ Recordings × ××—×§×• ×‘×–××Ÿ ×”×§×œ×˜×” (×œ×¤× ×™ ×©×”×¡×ª×™×™××•)
# â†’ ×–×” OK, ××‘×œ ××¢× ×™×™×Ÿ ×œ×“×¢×ª
```

### ğŸ’» Implementation (××§×•×¦×¨ - ×”×§×•×“ ×××•×“ ××¨×•×š)

```python
def test_historical_vs_live_recordings(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: Historical vs Live Recordings Classification
    
    Purpose:
    - Validate recording lifecycle management
    - Detect stale/crashed recordings
    - Verify cleanup service functionality
    - Ensure data integrity
    
    Business Impact:
    - Stale recordings indicate system reliability issues
    - Invalid metadata breaks history playback
    - Proper classification required for data retention policies
    """
    
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    recordings = db["recordings"]
    
    # ============================================
    # STEP 1: Count Total Recordings
    # ============================================
    total = recordings.count_documents({})
    logger.info(f"Total recordings in DB: {total}")
    
    assert total > 0, "No recordings found! Database may be empty."
    
    # ============================================
    # STEP 2: Classify by State
    # ============================================
    
    # Historical (completed)
    historical = recordings.count_documents({
        "start_time": {"$exists": True},
        "end_time": {"$ne": None},
        "deleted": False
    })
    
    # Live (in-progress)
    live = recordings.count_documents({
        "start_time": {"$exists": True},
        "end_time": None,
        "deleted": False
    })
    
    # Deleted
    deleted = recordings.count_documents({"deleted": True})
    
    # Invalid (missing start_time)
    invalid = recordings.count_documents({
        "start_time": {"$exists": False}
    })
    
    # ============================================
    # STEP 3: Detect Stale Recordings
    # ============================================
    now = time.time()
    stale_threshold = now - (24 * 3600)
    
    stale = recordings.count_documents({
        "start_time": {"$lt": stale_threshold},
        "end_time": None,
        "deleted": False
    })
    
    # ============================================
    # STEP 4: Calculate Percentages
    # ============================================
    percentages = {
        "historical": (historical / total) * 100,
        "live": (live / total) * 100,
        "deleted": (deleted / total) * 100,
        "invalid": (invalid / total) * 100,
        "stale": (stale / total) * 100
    }
    
    # ============================================
    # STEP 5: Log Classification
    # ============================================
    logger.info("\n" + "=" * 60)
    logger.info("RECORDING CLASSIFICATION RESULTS")
    logger.info("=" * 60)
    logger.info(f"Historical (completed):  {historical:>6} ({percentages['historical']:>5.1f}%)")
    logger.info(f"Live (in-progress):      {live:>6} ({percentages['live']:>5.1f}%)")
    logger.info(f"Deleted (cleanup):       {deleted:>6} ({percentages['deleted']:>5.1f}%)")
    logger.info(f"Invalid (missing data):  {invalid:>6} ({percentages['invalid']:>5.1f}%)")
    logger.info(f"Stale (crashed):         {stale:>6} ({percentages['stale']:>5.1f}%)")
    logger.info("=" * 60)
    
    # ============================================
    # STEP 6: CRITICAL Assertions
    # ============================================
    
    # No invalid recordings allowed
    assert invalid == 0, \
        f"Found {invalid} recordings without start_time! Data corruption detected."
    
    # Classification integrity
    classified_total = historical + live + deleted
    assert classified_total == total, \
        f"Classification mismatch: {classified_total} vs {total}"
    
    # Historical should be majority
    assert percentages["historical"] > 50.0, \
        f"Historical recordings only {percentages['historical']:.1f}% " \
        f"(expected >50%). Indicates cleanup or data loss issues."
    
    # ============================================
    # STEP 7: WARNING Assertions
    # ============================================
    
    if stale > 0:
        logger.warning(
            f"\nâš ï¸  WARNING: Found {stale} stale recordings!\n"
            f"   These are recordings >24h old without end_time.\n"
            f"   Possible causes:\n"
            f"   - Focus Server crashed during recording\n"
            f"   - Recording process hung\n"
            f"   - Bug in end_time writing logic\n"
        )
        
        # Log samples
        stale_samples = list(recordings.find({
            "start_time": {"$lt": stale_threshold},
            "end_time": None,
            "deleted": False
        }).limit(3))
        
        for rec in stale_samples:
            age_hours = (now - rec["start_time"]) / 3600
            logger.warning(
                f"   Stale: {rec['uuid']} (age: {age_hours:.1f} hours)"
            )
    
    # ============================================
    # STEP 8: Success
    # ============================================
    logger.info("\n" + "=" * 60)
    logger.info("RECORDING LIFECYCLE VALIDATION: PASSED âœ…")
    logger.info("=" * 60)
    logger.info(f"âœ“ No invalid recordings")
    logger.info(f"âœ“ Classification integrity verified")
    logger.info(f"âœ“ Historical recordings are majority")
    if stale == 0:
        logger.info(f"âœ“ No stale recordings detected")
    logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "××” ×¢×•×©×™× ×× ××•×¦××™× Stale recordings?"**
**×ª×©×•×‘×”**:
```python
# Option 1: Mark as deleted
def cleanup_stale_recordings(db, stale_threshold):
    result = db.recordings.update_many(
        {
            "start_time": {"$lt": stale_threshold},
            "end_time": None,
            "deleted": False
        },
        {
            "$set": {"deleted": True, "deletion_reason": "stale"}
        }
    )
    logger.info(f"Marked {result.modified_count} stale recordings as deleted")

# Option 2: Fix end_time (if we can infer it)
def fix_stale_recordings(db):
    stale_recs = db.recordings.find({
        "start_time": {"$exists": True},
        "end_time": None,
        "deleted": False
    })
    
    for rec in stale_recs:
        # Try to infer end_time from file system
        file_path = rec.get("path")
        if file_path and os.path.exists(file_path):
            file_mtime = os.path.getmtime(file_path)
            db.recordings.update_one(
                {"_id": rec["_id"]},
                {"$set": {"end_time": file_mtime}}
            )

# Option 3: Investigate root cause
# â†’ Check Focus Server logs
# â†’ Find crash time
# â†’ Fix underlying bug
```

#### **×©××œ×”: "×œ××” 24 ×©×¢×•×ª ×–×” ×”-threshold ×œ-Stale?"**
**×ª×©×•×‘×”**:
```
×”×’×“×¨×ª 24 ×©×¢×•×ª ×”×™× heuristic:

- Recordings ×¨×’×™×œ×™×: 1-2 ×©×¢×•×ª
- Long recordings: ×¢×“ 12 ×©×¢×•×ª
- ×× recording ×¢×“×™×™×Ÿ "Live" ××—×¨×™ 24h â†’ ×›× ×¨××” crashed

××‘×œ: ××¤×©×¨ ×œ×”×ª××™× ×œ×¤×™ use case:
```python
# For short recordings (minutes)
STALE_THRESHOLD_HOURS = 1

# For long-term monitoring (days)
STALE_THRESHOLD_HOURS = 48

# ×”×˜×¡×˜ ×¦×¨×™×š ×œ×”×™×•×ª configurable:
stale_threshold_hours = config_manager.get(
    "data_quality.stale_threshold_hours",
    default=24
)
```

#### **×©××œ×”: "××™×š ×”×˜×¡×˜ ××˜×¤×œ ×‘-deleted recordings ×‘×œ×™ end_time?"**
**×ª×©×•×‘×”**:
```python
# ×ª×¨×—×™×©: recording × ××—×§ ×‘×–××Ÿ ×©×¢×“×™×™×Ÿ ×”×™×” Live

# ×–×” LEGITIMATE behavior:
# 1. User starts recording
# 2. Realizes it's wrong
# 3. Deletes it immediately
# 4. Recording never got end_time â†’ OK!

# ×”×˜×¡×˜ ×œ× × ×›×©×œ ×¢×œ ×–×”:
deleted_without_endtime = db.recordings.count_documents({
    "deleted": True,
    "end_time": None
})

if deleted_without_endtime > 0:
    logger.info(
        f"â„¹ï¸  {deleted_without_endtime} deleted recordings missing end_time.\n"
        f"   This is OK - they were deleted while still recording."
    )
    # â†’ ×¨×§ INFO, ×œ× WARNING ××• ERROR
```

---

## ×˜×¡×˜ #7: PZ-13686 - MongoDB Indexes Validation

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×™×© indexes ××•×¤×˜×™××œ×™×™× ×¢×œ `node4` collection ×œ×× ×™×¢×ª performance degradation**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

#### **Expected Indexes on node4**
```python
EXPECTED_INDEXES_NODE4 = {
    "start_time_1": {
        "field": "start_time",
        "type": "ascending",
        "critical": True
    },
    "end_time_1": {
        "field": "end_time", 
        "type": "ascending",
        "critical": True
    },
    "uuid_1": {
        "field": "uuid",
        "type": "ascending",
        "unique": True,
        "critical": True
    },
    "deleted_1": {
        "field": "deleted",
        "type": "ascending",
        "critical": False  # Nice to have
    }
}
```

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **Performance Impact Without Indexes**
```python
# Query: Find all recordings in time range
recordings = node4.find({
    "start_time": {"$gte": start},
    "end_time": {"$lte": end},
    "deleted": False
})

# WITHOUT indexes:
# - Full collection scan: O(n)
# - For 10,000 recordings: ~5000ms
# - MongoDB CPU: 80%+
# - Multiple concurrent users: timeout

# WITH indexes:
# - Index scan: O(log n)
# - For 10,000 recordings: ~50ms
# - MongoDB CPU: <5%
# - Scales to 100+ concurrent users
```

### ğŸ’» Implementation

```python
def test_mongodb_indexes_exist_and_optimal(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: MongoDB Indexes Validation on node4
    
    Purpose: Ensure critical indexes exist for efficient queries
    
    Critical Queries That Need Indexes:
    1. Time-based lookups (history playback)
    2. UUID lookups (recording retrieval)
    3. Deleted flag filtering (active recordings only)
    """
    
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    node4 = db["node4"]
    
    # ============================================
    # STEP 1: List Current Indexes
    # ============================================
    indexes = list(node4.list_indexes())
    index_details = {}
    
    for idx in indexes:
        index_details[idx['name']] = {
            "key": idx['key'],
            "unique": idx.get('unique', False)
        }
    
    logger.info(f"Found {len(indexes)} indexes on node4:")
    for name, details in index_details.items():
        unique_marker = " [UNIQUE]" if details['unique'] else ""
        logger.info(f"  - {name}: {details['key']}{unique_marker}")
    
    # ============================================
    # STEP 2: Verify Required Indexes
    # ============================================
    required_indexes = ["start_time_1", "end_time_1", "uuid_1"]
    missing = []
    
    for idx_name in required_indexes:
        if idx_name not in index_details:
            missing.append(idx_name)
            logger.error(f"âŒ Missing critical index: {idx_name}")
        else:
            logger.info(f"âœ“ Index {idx_name} exists")
    
    # ============================================
    # STEP 3: Verify UUID is Unique
    # ============================================
    if "uuid_1" in index_details:
        if index_details["uuid_1"]["unique"]:
            logger.info("âœ“ UUID index is UNIQUE (prevents duplicates)")
        else:
            logger.warning("âš ï¸  UUID index exists but is NOT unique!")
            logger.warning("   Recommendation: Recreate as unique index")
    
    # ============================================
    # STEP 4: Performance Analysis (Optional)
    # ============================================
    logger.info("\nPerformance Analysis:")
    
    # Estimate index efficiency
    total_docs = node4.count_documents({})
    logger.info(f"Total documents in node4: {total_docs}")
    
    if total_docs > 1000:
        # Large collection - indexes are CRITICAL
        if missing:
            logger.error(
                f"ğŸš¨ CRITICAL: {len(missing)} indexes missing on "
                f"large collection ({total_docs} docs)!"
            )
            logger.error("   Queries will be VERY slow!")
    
    # ============================================
    # ASSERTIONS
    # ============================================
    assert not missing, \
        f"Missing critical indexes on node4: {', '.join(missing)}\n" \
        f"This will cause severe performance degradation.\n" \
        f"Create indexes with:\n" + \
        "\n".join(f"  db.node4.createIndex({{\"{idx.replace('_1', '')}\": 1}})" 
                  for idx in missing)
    
    logger.info("\n" + "=" * 60)
    logger.info("NODE4 INDEXES VALIDATION: PASSED âœ…")
    logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "××” ×”×”×‘×“×œ ×‘×™×Ÿ index ×¢×œ `recordings` ×•-`node4`?"**
**×ª×©×•×‘×”**:
```
×©× ×™ collections, ××•×ª× indexes:

recordings collection:
- Main metadata store
- Used by API: POST /recordings_in_time_range
- Needs: start_time, end_time, uuid indexes

node4 collection:
- Node-specific recording data
- Used by: Baby Analyzer, recording lookup by node
- Needs: same indexes + node-specific fields

×œ××” ×©× ×™ collections?
- Separation of concerns
- Different access patterns
- node4 might have additional node-specific fields
```

#### **×©××œ×”: "××™×š ××•×“×“×™× ××ª ×”-performance impact ×©×œ indexes?"**
**×ª×©×•×‘×”**:
```python
# Test WITHOUT index (controlled test only!)
import time

# Measure query time
start = time.time()
results = collection.find({"start_time": {"$gte": threshold}})
list(results)  # Force execution
duration_no_index = time.time() - start

# Create index
collection.create_index("start_time")

# Measure again
start = time.time()
results = collection.find({"start_time": {"$gte": threshold}})
list(results)
duration_with_index = time.time() - start

speedup = duration_no_index / duration_with_index
logger.info(f"Speedup with index: {speedup}x faster")

# Typical results:
# - Small collection (100 docs): 2-5x faster
# - Medium collection (1000 docs): 10-50x faster
# - Large collection (10000+ docs): 100-1000x faster
```

---

## ×˜×¡×˜ #8: PZ-13685 - Recordings Metadata Completeness

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×›×œ recording ×‘-`node4` ×™×© metadata ××œ× - ×‘×“×™×§×” ××§×‘×™×œ×” ×œ-PZ-13812 ××‘×œ ×¢×œ `node4`**.

### ğŸ“‹ ××” ×‘×“×™×•×§ ×‘×•×“×§×™×?

```python
# Sample recordings from node4
sample = list(node4.find().limit(100))

for recording in sample:
    # Required fields
    assert "uuid" in recording and recording["uuid"]
    assert "start_time" in recording and recording["start_time"] > 0
    assert "end_time" in recording and recording["end_time"] > 0
    assert "deleted" in recording and isinstance(recording["deleted"], bool)
    
    # Logical validation
    assert recording["start_time"] < recording["end_time"]
```

### ğŸ”´ ×œ××” × ×—×™×¥?
**×–×”×” ×œ-PZ-13812 ××‘×œ ×¢×œ `node4` collection**. ×× ×™×© ×‘×¢×™×•×ª metadata ×‘-node4:
- Baby Analyzer ×œ× ×™×•×›×œ ×œ××¦×•× recordings
- Node-specific queries ×™×›×©×œ×•
- Recording attribution (××™×–×” node ×”×§×œ×™×˜) ×ª×™×¢×œ×

### ğŸ’» Implementation

```python
def test_recordings_have_all_required_metadata(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: Recordings Metadata Completeness on node4
    
    Similar to PZ-13812 but validates node4 collection
    """
    
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    node4 = db["node4"]
    
    SAMPLE_SIZE = 100
    sample = list(node4.find().limit(SAMPLE_SIZE))
    
    assert len(sample) >= 10, \
        f"Insufficient data in node4: {len(sample)} documents"
    
    logger.info(f"Validating {len(sample)} recordings from node4...")
    
    required_fields = ["uuid", "start_time", "end_time", "deleted"]
    missing_metadata_count = 0
    
    for idx, rec in enumerate(sample, 1):
        for field in required_fields:
            if field not in rec or rec[field] is None:
                missing_metadata_count += 1
                logger.error(
                    f"Recording {idx}: Missing or null field '{field}'"
                )
        
        # Time validation
        if "start_time" in rec and "end_time" in rec:
            if rec["start_time"] >= rec["end_time"]:
                missing_metadata_count += 1
                logger.error(
                    f"Recording {idx}: Invalid time range "
                    f"(start >= end)"
                )
    
    assert missing_metadata_count == 0, \
        f"Found {missing_metadata_count} metadata issues in node4"
    
    logger.info("âœ… All recordings in node4 have complete metadata")
```

---

## ×˜×¡×˜ #9: PZ-13684 - node4 Schema Validation

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©×”-schema ×©×œ documents ×‘-`node4` ×ª×§×™×Ÿ - parallel ×œ-PZ-13811**.

### ğŸ“‹ ××” ×‘×•×“×§×™×?

```python
EXPECTED_SCHEMA_NODE4 = {
    "uuid": str,
    "start_time": (int, float),
    "end_time": (int, float),
    "deleted": bool,
    "path": str,  # Optional
    "node": str   # Optional - node identifier
}
```

### ğŸ’» Implementation

```python
def test_node4_schema_validation(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: node4 Schema Validation
    
    Validates document structure and field types in node4 collection
    """
    
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    node4 = db["node4"]
    
    # Sample documents
    sample_size = min(100, node4.count_documents({}))
    sample = list(node4.find().limit(sample_size))
    
    assert len(sample) > 0, "node4 collection is empty"
    
    logger.info(f"Validating schema for {len(sample)} documents...")
    
    schema_errors = []
    
    for idx, doc in enumerate(sample, 1):
        # uuid
        if "uuid" not in doc:
            schema_errors.append(f"Doc {idx}: Missing 'uuid'")
        elif not isinstance(doc["uuid"], str):
            schema_errors.append(
                f"Doc {idx}: 'uuid' is {type(doc['uuid'])}, expected str"
            )
        
        # start_time
        if "start_time" in doc:
            if not isinstance(doc["start_time"], (int, float)):
                schema_errors.append(
                    f"Doc {idx}: 'start_time' is {type(doc['start_time'])}"
                )
        
        # end_time
        if "end_time" in doc:
            if not isinstance(doc["end_time"], (int, float)):
                schema_errors.append(
                    f"Doc {idx}: 'end_time' is {type(doc['end_time'])}"
                )
        
        # deleted
        if "deleted" in doc:
            if not isinstance(doc["deleted"], bool):
                schema_errors.append(
                    f"Doc {idx}: 'deleted' should be bool, "
                    f"got {type(doc['deleted'])}"
                )
    
    if schema_errors:
        error_msg = "\n".join(schema_errors)
        pytest.fail(f"Schema validation failed:\n{error_msg}")
    
    logger.info("âœ… node4 schema validation passed")
```

---

## ×˜×¡×˜ #10: PZ-13683 - MongoDB Collections Exist

### ğŸ¯ ××” ×”××˜×¨×”?
**×‘×“×™×§×ª ×ª×©×ª×™×ª - ×œ×•×•×“× ×©-collections ×‘×¡×™×¡×™×™× ×§×™×™××™× (×–×”×” ×œ-PZ-13809 ××‘×œ ×¢× ×¨×©×™××” ××—×¨×ª)**.

### ğŸ“‹ ××” ×‘×•×“×§×™×?

```python
REQUIRED_COLLECTIONS = [
    "base_paths",   # GUID mapping
    "node2",        # Node 2 data
    "node4"         # Node 4 data (main)
]
```

### ğŸ’» Implementation

```python
def test_required_collections_exist(
    self,
    mongo_client,
    config_manager,
    logger
):
    """
    Test: MongoDB Collections Exist (base_paths, node2, node4)
    """
    
    db_name = config_manager.get("mongodb.database_name")
    db = mongo_client[db_name]
    
    existing = db.list_collection_names()
    
    required = ["base_paths", "node2", "node4"]
    missing = [col for col in required if col not in existing]
    
    if missing:
        pytest.fail(
            f"Missing collections: {', '.join(missing)}\n"
            f"Database setup incomplete!"
        )
    
    logger.info(f"âœ… All required collections exist: {', '.join(required)}")
```

---

## ×˜×¡×˜ #11: PZ-13599 - Postgres Connectivity and Catalogs

### ğŸ¯ ××” ×”××˜×¨×”?
**×œ×•×•×“× ×©××¤×©×¨ ×œ×”×ª×—×‘×¨ ×œ-PostgreSQL ×•×©×›×œ system catalogs × ×’×™×©×™×**.

### ğŸ“‹ ××” ×‘×•×“×§×™×?

#### **1. Basic Connectivity**
```python
import psycopg2

# Test connection
conn = psycopg2.connect(
    host=config["postgres"]["host"],
    port=config["postgres"]["port"],
    dbname=config["postgres"]["database"],
    user=config["postgres"]["username"],
    password=config["postgres"]["password"]
)

# Test simple query
cursor = conn.cursor()
cursor.execute("SELECT 1")
result = cursor.fetchone()
assert result == (1,), "Basic query failed"
```

#### **2. System Catalogs Accessibility**
```python
# Required system tables/views for monitoring
REQUIRED_CATALOGS = [
    "pg_stat_activity",    # Active connections monitoring
    "pg_database",         # Database list
    "pg_namespace",        # Schema list
    "pg_tables"            # Table list
]

for catalog in REQUIRED_CATALOGS:
    cursor.execute(f"SELECT COUNT(*) FROM {catalog}")
    count = cursor.fetchone()[0]
    logger.info(f"âœ“ Catalog '{catalog}' accessible ({count} rows)")
```

### ğŸ”´ ×œ××” × ×—×™×¥?

#### **×ª×¨×—×™×©: PostgreSQL Monitoring**
```python
# ×‘×œ×™ ×’×™×©×” ×œ-pg_stat_activity:
# â†’ Cannot monitor active connections
# â†’ Cannot detect long-running queries
# â†’ Cannot kill problematic queries

# ×‘×œ×™ ×’×™×©×” ×œ-pg_database:
# â†’ Cannot list databases
# â†’ Cannot check DB sizes
# â†’ Cannot validate DB exists before connecting
```

### ğŸ’» Implementation

```python
import pytest
import psycopg2

class TestPostgresConnectivity:
    """
    Test Suite: PostgreSQL Infrastructure
    """
    
    @pytest.fixture
    def postgres_connection(self, config_manager):
        """
        Establish PostgreSQL connection.
        """
        pg_config = config_manager.get_postgres_config()
        
        conn = psycopg2.connect(
            host=pg_config["host"],
            port=pg_config["port"],
            dbname=pg_config["database"],
            user=pg_config["username"],
            password=pg_config["password"],
            connect_timeout=5
        )
        
        yield conn
        
        conn.close()
    
    def test_postgres_connectivity_and_catalogs(
        self,
        postgres_connection,
        logger
    ):
        """
        Test: Postgres Connectivity and Catalogs
        
        Purpose: Validate DB connectivity and system catalog accessibility
        
        Steps:
        1. Test basic connectivity (SELECT 1)
        2. Verify access to pg_stat_activity
        3. Verify access to pg_database
        4. Verify access to pg_namespace
        
        Expected: All checks pass, no permission errors
        """
        
        cursor = postgres_connection.cursor()
        
        # ============================================
        # STEP 1: Basic Connectivity
        # ============================================
        logger.info("Testing basic PostgreSQL connectivity...")
        
        cursor.execute("SELECT 1 AS test")
        result = cursor.fetchone()
        
        assert result == (1,), \
            f"Basic query failed: expected (1,), got {result}"
        
        logger.info("âœ“ Basic connectivity OK")
        
        # ============================================
        # STEP 2: Test System Catalogs
        # ============================================
        required_catalogs = {
            "pg_stat_activity": "Monitor active connections",
            "pg_database": "List databases",
            "pg_namespace": "List schemas",
            "pg_tables": "List tables"
        }
        
        logger.info("\nVerifying system catalog access:")
        
        for catalog, purpose in required_catalogs.items():
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {catalog}")
                count = cursor.fetchone()[0]
                
                logger.info(f"âœ“ {catalog}: {count} rows ({purpose})")
            
            except psycopg2.Error as e:
                pytest.fail(
                    f"Cannot access system catalog '{catalog}'\n"
                    f"Purpose: {purpose}\n"
                    f"Error: {e}\n"
                    f"This indicates insufficient permissions."
                )
        
        # ============================================
        # STEP 3: Database Version Check
        # ============================================
        cursor.execute("SELECT version()")
        version = cursor.fetchone()[0]
        logger.info(f"\nPostgreSQL version: {version}")
        
        # ============================================
        # STEP 4: Active Connections Check
        # ============================================
        cursor.execute("""
            SELECT COUNT(*) 
            FROM pg_stat_activity 
            WHERE state = 'active'
        """)
        active_conns = cursor.fetchone()[0]
        logger.info(f"Active connections: {active_conns}")
        
        # ============================================
        # SUCCESS
        # ============================================
        logger.info("\n" + "=" * 60)
        logger.info("POSTGRES CONNECTIVITY TEST: PASSED âœ…")
        logger.info("=" * 60)
        logger.info("âœ“ Database connectivity verified")
        logger.info("âœ“ All system catalogs accessible")
        logger.info("âœ“ Permissions sufficient for monitoring")
        logger.info("=" * 60)
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "×œ××” ×¦×¨×™×š PostgreSQL ×‘×›×œ×œ? ×œ× ×¨×§ MongoDB?"**
**×ª×©×•×‘×”**:
```
××¨×›×™×˜×§×˜×•×¨×ª Focus Server:

MongoDB:
- Recording metadata (fast queries)
- Time-series indexing
- Document storage

PostgreSQL:
- Relational data (users, settings, configs)
- Transactions (ACID compliance)
- Complex queries with JOINs
- Reports and analytics

×©× ×™×”× ××©×œ×™××™×:
- MongoDB â†’ fast reads, simple queries
- PostgreSQL â†’ complex logic, consistency
```

#### **×©××œ×”: "××” ×× ××™×Ÿ ×”×¨×©××•×ª ×œ-pg_stat_activity?"**
**×ª×©×•×‘×”**:
```sql
-- ×”×‘×¢×™×”: User doesn't have permissions

-- Fix 1: Grant permissions
GRANT SELECT ON pg_stat_activity TO focus_user;

-- Fix 2: Use restricted view
CREATE VIEW focus_stat_activity AS
SELECT pid, usename, application_name, state, query
FROM pg_stat_activity
WHERE usename = current_user;

GRANT SELECT ON focus_stat_activity TO focus_user;

-- ×”×˜×¡×˜ ×¦×¨×™×š ×œ×”×ª××™×:
try:
    cursor.execute("SELECT COUNT(*) FROM pg_stat_activity")
except psycopg2.ProgrammingError:
    # Try restricted view
    cursor.execute("SELECT COUNT(*) FROM focus_stat_activity")
```

---

## ×˜×¡×˜ #12: PZ-13598 - Mongo Collections and Schema (Parent Test)

### ğŸ¯ ××” ×”××˜×¨×”?
**×˜×¡×˜-×¢×œ (parent) ×©××¨×™×¥ ××¡×¤×¨ sub-tests ×œ×•×•×œ×™×“×¦×™×” ××§×™×¤×” ×©×œ MongoDB**.

### ğŸ“‹ ××” ×‘×•×“×§×™×?

```python
# This is a TEST SUITE, not a single test
class TestMongoDBDataQuality:
    """
    Parent Test Suite: MongoDB Data Quality
    
    Sub-tests:
    1. Collections exist
    2. Schema validation (node4)
    3. Metadata completeness
    4. Indexes validation
    5. Soft delete implementation
    """
    
    def test_required_collections_exist(self):
        # PZ-13809
        pass
    
    def test_node4_schema_validation(self):
        # PZ-13811
        pass
    
    def test_recordings_have_all_required_metadata(self):
        # PZ-13812
        pass
    
    def test_mongodb_indexes_exist_and_optimal(self):
        # PZ-13810
        pass
```

### ğŸ”´ ×œ××” × ×—×™×¥?
**×–×” ×”-umbrella test ×©×××’×“ ××ª ×›×œ ×‘×“×™×§×•×ª ×”-MongoDB quality**.

### ğŸ’» Implementation

```python
import pytest

@pytest.mark.mongodb
@pytest.mark.data_quality
class TestMongoDBDataQuality:
    """
    Test Suite: MongoDB Data Quality (PZ-13598)
    
    Purpose: Comprehensive MongoDB validation covering:
    - Infrastructure (collections, indexes)
    - Schema integrity (field types, structure)
    - Data quality (completeness, validity)
    - Soft delete implementation
    
    This is the parent test that groups all MongoDB quality tests.
    
    Run all MongoDB tests:
        pytest -m mongodb -v
    
    Run only this suite:
        pytest tests/integration/infrastructure/test_mongodb_data_quality.py -v
    """
    
    def test_mongodb_connection(self, mongo_client, logger):
        """
        Prerequisite: Verify MongoDB is accessible
        """
        # Test connection
        mongo_client.admin.command('ping')
        logger.info("âœ… MongoDB connection successful")
    
    def test_required_collections_exist(self, ...):
        """Sub-test: PZ-13809"""
        # Implementation from PZ-13809
        pass
    
    def test_node4_schema_validation(self, ...):
        """Sub-test: PZ-13811"""
        # Implementation from PZ-13811
        pass
    
    def test_recordings_have_all_required_metadata(self, ...):
        """Sub-test: PZ-13812"""
        # Implementation from PZ-13812
        pass
    
    def test_mongodb_indexes_exist_and_optimal(self, ...):
        """Sub-test: PZ-13810"""
        # Implementation from PZ-13810
        pass
    
    def test_soft_delete_implementation(self, ...):
        """
        Additional Test: Soft Delete Logic
        
        Verify:
        - Deleted flag exists and is boolean
        - Deleted recordings are excluded from queries
        - Cleanup service marks recordings correctly
        """
        # Implementation...
        pass
    
    def test_detect_illegal_inserts(self, ...):
        """
        Additional Test: Schema Enforcement
        
        Try to insert invalid data and verify it's rejected
        """
        # Implementation...
        pass
```

### ğŸ” ×©××œ×•×ª ×•×”×ª×©×•×‘×•×ª

#### **×©××œ×”: "×œ××” ×¦×¨×™×š parent test? ×œ××” ×œ× ×¨×§ individual tests?"**
**×ª×©×•×‘×”**:
```
××¨×’×•×Ÿ ×”×™×¨×¨×›×™:

PZ-13598 (Parent)
  â”œâ”€ PZ-13809: Collections Exist
  â”œâ”€ PZ-13810: Indexes Exist
  â”œâ”€ PZ-13811: Schema Validation
  â””â”€ PZ-13812: Metadata Completeness

×™×ª×¨×•× ×•×ª:
1. ×¨×™×¦×” ×××•×¨×’× ×ª: pytest -m mongodb
2. ×ª×™×¢×•×“ ×‘×¨×•×¨: ×›×œ ×”×‘×“×™×§×•×ª ×‘××§×•× ××—×“
3. Shared fixtures: ×›×•×œ× ××©×ª××©×™× ×‘××•×ª×• mongo_client
4. Progress tracking: "5/5 MongoDB tests passed"
```

#### **×©××œ×”: "××™×š ××¨×™×¦×™× ×¨×§ ×§×‘×•×¦×” ××—×ª ×©×œ ×˜×¡×˜×™×?"**
**×ª×©×•×‘×”**:
```bash
# Run all MongoDB tests
pytest -m mongodb -v

# Run only infrastructure tests (collections, indexes)
pytest -m "mongodb and infrastructure" -v

# Run only data quality tests (schema, metadata)
pytest -m "mongodb and data_quality" -v

# Run specific test file
pytest tests/integration/infrastructure/test_mongodb_data_quality.py -v

# Run specific test
pytest tests/integration/infrastructure/test_mongodb_data_quality.py::TestMongoDBDataQuality::test_node4_schema_validation -v

# Run with detailed output
pytest -m mongodb -v -s --log-cli-level=INFO
```

---

## ×˜×¡×˜ #13: Data Quality - Additional Tests Summary

×”×˜×¡×˜×™× ×©×œ× ×¤×•×¨×˜×• ×‘×¤×™×¨×•×˜ ××œ× (×›×™ ×”× ×“×•××™× ×××•×“ ×œ×§×•×“××™×):

### PZ-13684: node4 Schema Validation
- **×–×”×” ×œ-PZ-13811** ××‘×œ ×¢×œ `node4` ×‘××§×•× `recordings`
- ×‘×•×“×§ ×©×›×œ document ×™×© ×©×“×•×ª × ×›×•× ×™× ××”×˜×™×¤×•×¡ ×”× ×›×•×Ÿ

### PZ-13685: Recordings Metadata Completeness  
- **×–×”×” ×œ-PZ-13812** ××‘×œ ×¢×œ `node4`
- ×‘×•×“×§ ×©××™×Ÿ null ××• ×¢×¨×›×™× ×¨×™×§×™×

### PZ-13686: MongoDB Indexes Validation
- **×–×”×” ×œ-PZ-13810** ××‘×œ ××ª××§×“ ×‘-`node4` indexes
- ×‘×•×“×§ start_time, end_time, uuid indexes

### PZ-13683: MongoDB Collections Exist
- **variant ×©×œ PZ-13809** ×¢× ×¨×©×™××ª collections ××—×¨×ª
- ×‘×•×“×§: base_paths, node2, node4

---

## ğŸ“š ×¡×™×›×•× ×•×”××œ×¦×•×ª ×œ×¤×’×™×©×”

### ×˜×™×¤×™× ×œ×¤×’×™×©×”

#### 1. **×”×›× ×” ×× ×˜×œ×™×ª**
```
×œ×¤× ×™ ×”×¤×’×™×©×”:
âœ… ×§×¨× ××ª ×”×ª×§×¦×™×¨ ×©×œ ×›×œ ×˜×¡×˜ (3-4 ××©×¤×˜×™×)
âœ… ×”×‘×Ÿ ××ª ×”×§×˜×’×•×¨×™×•×ª (MongoDB infra, Data quality, etc.)
âœ… ×ª×“×¢ ×œ×”×¡×‘×™×¨ "×œ××”" ×›×œ ×˜×¡×˜ ×—×©×•×‘
```

#### 2. **×©××œ×•×ª × ×¤×•×¦×•×ª ×©×™×™×©××œ×•**
```
Q: "××™×–×” ×˜×¡×˜×™× ×”×›×™ ×§×¨×™×˜×™×™×?"
A: "PZ-13809 (Collections Exist) ×•-PZ-13867 (Data Integrity) 
    ×›×™ ×‘×œ×¢×“×™×”× ×›×œ×•× ×œ× ×™×¢×‘×•×“."

Q: "×›××” ×–××Ÿ ×œ×¨×•×¥ ××ª ×›×œ ×”×˜×¡×˜×™×?"
A: "5-7 ×“×§×•×ª total. MongoDB tests ××”×™×¨×™× (seconds),
    Historic playback tests ×™×•×ª×¨ ××¨×•×›×™× (1-2 min ×›×œ ××—×“)."

Q: "××” ×× ×˜×¡×˜ × ×›×©×œ ×‘production?"
A: "×™×© severity levels:
    - CRITICAL = block deployment
    - WARNING = alert team, don't block
    - INFO = log for analysis"
```

#### 3. **×”×¦×’ ×¢×¨×š ×¢×¡×§×™**
```
×›×œ ×˜×¡×˜ ×¢× ×” ×¢×œ:
1. ××” ×–×” ××•× ×¢? (bugs, downtime, data loss)
2. ××” ×–×” ×—×•×¡×š? (debug time, support tickets)
3. ××” ×–×” ××©×¤×¨? (reliability, user trust)
```

---

## ğŸ“Š ×˜×‘×œ×ª ×¡×™×›×•× ××”×™×¨ - ×›×œ ×”×˜×¡×˜×™×

| # | Jira ID | ×©× ×”×˜×¡×˜ | ×§×˜×’×•×¨×™×” | Priority | ×–××Ÿ ×¨×™×¦×” | Automation Status |
|---|---------|---------|----------|----------|----------|-------------------|
| 1 | PZ-13867 | Historic Playback Data Integrity | Data Quality | High | ~2 min | âœ… Automated |
| 2 | PZ-13812 | Recordings Complete Metadata | MongoDB | Medium | ~10 sec | âœ… Automated |
| 3 | PZ-13811 | Recordings Schema Validation | MongoDB | High | ~5 sec | âœ… Automated |
| 4 | PZ-13810 | Critical Indexes Exist | MongoDB | Medium | ~3 sec | âœ… Automated |
| 5 | PZ-13809 | Required Collections Exist | MongoDB | Medium | ~2 sec | âœ… Automated |
| 6 | PZ-13705 | Historical vs Live Classification | Data Lifecycle | Medium | ~15 sec | âœ… Automated |
| 7 | PZ-13686 | node4 Indexes Validation | MongoDB | Medium | ~3 sec | âœ… Automated |
| 8 | PZ-13685 | node4 Metadata Completeness | MongoDB | Medium | ~10 sec | âœ… Automated |
| 9 | PZ-13684 | node4 Schema Validation | MongoDB | Medium | ~5 sec | âœ… Automated |
| 10 | PZ-13683 | Collections Exist (base_paths/nodes) | MongoDB | Medium | ~2 sec | âœ… Automated |
| 11 | PZ-13599 | Postgres Connectivity | PostgreSQL | Medium | ~5 sec | âœ… Automated |
| 12 | PZ-13598 | Mongo Collections and Schema (Parent) | MongoDB | Medium | ~30 sec | âœ… Automated |
| 13 | - | Additional Tests Summary | Various | - | - | - |

**×¡×”"×› ×–××Ÿ ×¨×™×¦×”**: ~5-7 ×“×§×•×ª  
**×¡×”"×› ×˜×¡×˜×™× ××•×˜×•××˜×™×™×**: 12 (100%)

---

## ğŸ¯ ××¤×ª ×“×¨×›×™× - ××™×š ×œ×”×¦×™×’ ×‘×¤×’×™×©×”

### **×¤×ª×™×—×” (2 ×“×§×•×ª)**
```
"×™×© ×œ× ×• 13 ×˜×¡×˜×™× ×©××›×¡×™× ××ª ×›×œ ××—×–×•×¨ ×”×—×™×™× ×©×œ ×”× ×ª×•× ×™×:
- Infrastructure (collections, indexes, connectivity)
- Data Quality (schema, metadata, integrity)
- Lifecycle Management (historical, live, deleted)

×›×œ ×”×˜×¡×˜×™× ××•×˜×•××˜×™×™× ×•×¨×¦×™× ×‘-CI/CD."
```

### **×—×œ×•×§×” ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª (×”×¦×’ ×›×š)**

#### **×§×˜×’×•×¨×™×” 1: MongoDB Infrastructure (×§×¨×™×˜×™!)**
```
×˜×¡×˜×™× ×©×‘×œ×¢×“×™×”× ×”××¢×¨×›×ª ×œ× ×ª×¢×‘×•×“:

âœ… PZ-13809/13683: Collections Exist
   â†’ ×‘×•×“×§ ×©-recordings, node4, tasks, jobs ×§×™×™××™×
   â†’ ×× ×—×¡×¨×™× â†’ Focus Server crashes

âœ… PZ-13810/13686: Indexes Exist
   â†’ ×‘×•×“×§ indexes ×¢×œ start_time, end_time, uuid
   â†’ ×× ×—×¡×¨×™× â†’ Queries slow (5 sec â†’ timeout)
   â†’ ×‘×™×¦×•×¢×™× ×¤×™ 100-1000 ×™×•×ª×¨ ×˜×•×‘×™× ×¢× indexes

âœ… PZ-13598: Parent Test
   â†’ ××¨×™×¥ ××ª ×›×œ ×‘×“×™×§×•×ª ×”-infrastructure ×‘×™×—×“
```

#### **×§×˜×’×•×¨×™×” 2: Schema & Type Safety**
```
×˜×¡×˜×™× ×©××•× ×¢×™× runtime errors:

âœ… PZ-13811/13684: Schema Validation
   â†’ ×‘×•×“×§ ×©×›×œ field ×”×•× ××”×˜×™×¤×•×¡ ×”× ×›×•×Ÿ
   â†’ ×× start_time ×”×•× string â†’ TypeError!
   â†’ ××•× ×¢ schema drift

âœ… PZ-13812/13685: Metadata Completeness
   â†’ ×‘×•×“×§ ×©××™×Ÿ null ××• ×¢×¨×›×™× ×¨×™×§×™×
   â†’ ×× uuid ×¨×™×§ â†’ data corruption
   â†’ ×× path ×—×¡×¨ â†’ cannot load recording
```

#### **×§×˜×’×•×¨×™×” 3: Data Integrity & Quality**
```
×˜×¡×˜×™× ×©××‘×˜×™×—×™× × ×ª×•× ×™× ×ª×§×™× ×™×:

âœ… PZ-13867: Historic Playback Data Integrity
   â†’ ×‘×•×“×§ ×©×›×œ row ×‘playback ×ª×§×™×Ÿ:
     - Timestamps ××¡×•×“×¨×™× ×›×¨×•× ×•×œ×•×’×™×ª
     - ××™×Ÿ × ×ª×•× ×™× ×—×¡×¨×™×
     - sensor data complete
   â†’ ×× × ×›×©×œ â†’ UI crashes, wrong timeline

âœ… PZ-13705: Historical vs Live Classification
   â†’ ××¡×•×•×’ recordings: Historical, Live, Deleted, Stale
   â†’ ××–×”×” crashed recordings (>24h without end_time)
   â†’ ××•×•×“× cleanup service ×¢×•×‘×“
```

#### **×§×˜×’×•×¨×™×” 4: PostgreSQL Infrastructure**
```
âœ… PZ-13599: Postgres Connectivity
   â†’ ×‘×•×“×§ connection + system catalogs
   â†’ × ×“×¨×© ×œ××¢×§×‘ ××—×¨ connections
   â†’ × ×“×¨×© ×œ× ×™×”×•×œ transactions
```

---

## ğŸ’¡ ×ª×©×•×‘×•×ª ××•×›× ×•×ª ×œ×©××œ×•×ª ×§×©×•×ª

### **×©××œ×”: "×œ××” ×™×© ×›×¤×™×œ×•×™×•×ª? PZ-13810 ×•-PZ-13686 ×©× ×™×”× ×‘×•×“×§×™× indexes!"**
**×ª×©×•×‘×” ××¦×•×™× ×ª**:
```
× ×›×•×Ÿ! ×™×© ×“××™×•×Ÿ ××‘×œ ×¢×œ collections ×©×•× ×™×:

PZ-13810: Indexes on recordings collection
- ××™×•×¢×“ ×œ-API calls (POST /recordings_in_time_range)
- ××©××© ××ª ×”-UI ×•×”-REST API

PZ-13686: Indexes on node4 collection
- ××™×•×¢×“ ×œ-Baby Analyzer queries
- ××©××© ×œ×–×™×”×•×™ recordings ×œ×¤×™ node

×œ××” ×©× ×™ collections?
- Separation of concerns
- Different access patterns
- node4 ×™×›×•×œ ×œ×”×›×™×œ metadata × ×•×¡×£ specific ×œnode

××¤×©×¨ ×œ××—×“? ×›×Ÿ, ××‘×œ ×–×” architectural decision.
```

### **×©××œ×”: "××™×š ××ª×” ×‘×•×—×¨ ××” priority - High vs Medium?"**
**×ª×©×•×‘×” ××¦×•×™× ×ª**:
```
×™×© ×œ×™ methodology:

CRITICAL (block production):
- Cannot function without (Collections, Indexes)
- Data corruption risk (Schema validation)

HIGH:
- User-facing impact (Historic playback integrity)
- Performance degradation (Missing indexes)

MEDIUM:
- Operational concerns (Metadata completeness)
- Monitoring & observability (Postgres connectivity)

LOW:
- Nice to have
- Optional features

×‘×¤×•×¢×œ, ×× ×™ ××¨×™×¥ ××ª ×›×•×œ× ×‘CI/CD,
××‘×œ priority ×§×•×‘×¢:
- CRITICAL â†’ FAIL build
- HIGH â†’ WARN + require approval
- MEDIUM â†’ LOG + continue
```

### **×©××œ×”: "××” ×ª×¢×©×” ×× ×™×© ×œ×š 100 ×˜×¡×˜×™×? ××™×š ×ª× ×”×œ?"**
**×ª×©×•×‘×” ××¦×•×™× ×ª**:
```
××¨×’×•×Ÿ ×”×™×¨×¨×›×™ ×¢× pytest marks:

@pytest.mark.critical
@pytest.mark.mongodb
@pytest.mark.infrastructure
def test_collections_exist():
    ...

×¨×™×¦×” ×—×›××”:
# Before deployment
pytest -m critical -v  # Only critical tests (fast)

# Nightly CI/CD
pytest -m "mongodb or postgres" -v  # All DB tests

# Full regression
pytest -v  # Everything

# Specific category
pytest -m "data_quality and high" -v

×¤×¨×œ×•×œ:
pytest -n 8  # Run 8 tests in parallel (pytest-xdist)

××¡×˜×¨×˜×’×™×”:
- Critical tests: <30 seconds total
- Full suite: <10 minutes total
- If > 10 min â†’ optimize or parallelize
```

### **×©××œ×”: "××” ×× ×˜×¡×˜ × ×›×©×œ ×‘production?"**
**×ª×©×•×‘×” ××¦×•×™× ×ª**:
```
×™×© ×œ×™ action plan:

STEP 1: Severity Assessment
- CRITICAL failure â†’ rollback deployment
- HIGH failure â†’ investigate immediately
- MEDIUM failure â†’ log + monitor

STEP 2: Root Cause Analysis
# ×”×˜×¡×˜ ×›×ª×•×‘ ×›×š ×©×”×•× × ×•×ª×Ÿ context:
AssertionError: Missing index 'start_time_1' on node4
  Impact: Queries will be slow (O(n) instead of O(log n))
  Fix: db.node4.createIndex({"start_time": 1})

â†’ ×‘×¨×•×¨ ××” ×”×‘×¢×™×” ×•××” ×”×¤×ª×¨×•×Ÿ!

STEP 3: Hotfix
- ×× ××¤×©×¨ ×œ×ª×§×Ÿ ×‘××”×™×¨×•×ª (missing index) â†’ fix
- ×× bug ×‘×§×•×“ â†’ rollback + fix in dev

STEP 4: Prevent Recurrence
- ×”×•×¡×£ ×˜×¡×˜ ×œCI/CD ×× ×—×¡×¨
- Document the issue
- Add to monitoring/alerts
```

---

## ğŸ“‹ Checklist ×œ×¤×’×™×©×”

### **×œ×¤× ×™ ×”×¤×’×™×©×”**
- [ ] ×§×¨× ××ª ×”×¡×™×›×•× ×©×œ ×›×œ 13 ×˜×¡×˜×™× (×¢××•×“ ×”×¨××©×•×Ÿ)
- [ ] ×”×›×Ÿ 3 ×“×•×’×××•×ª ×§×•×“ (1 MongoDB, 1 Data Integrity, 1 Postgres)
- [ ] ×ª×¨×’×œ ×”×¡×‘×¨ ××—×“ ×‘×§×•×œ ×¨× (×‘×—×¨ PZ-13867 ××• PZ-13705)
- [ ] ×”×›×Ÿ laptop ×¢× ×”×§×•×“ ×¤×ª×•×— (×œ×”×¨××•×ª implementation ×× ×™×‘×§×©×•)

### **×‘××”×œ×š ×”×¤×’×™×©×”**
- [ ] ×”×ª×—×œ ×¢× overview (2 ×“×§×•×ª)
- [ ] ×”×¦×’ ×˜×‘×œ×ª ×¡×™×›×•× (visual)
- [ ] ×¦×œ×•×œ ×œ×“×•×’××” ××¤×•×¨×˜×ª ××—×ª (5 ×“×§×•×ª)
- [ ] ×¢× ×” ×¢×œ ×©××œ×•×ª ×¢× ×§×•×“ + ×”×¡×‘×¨ (×œ× ×¨×§ ×ª×™××•×¨×™×”)
- [ ] ×ª××™×“ ×§×©×¨ ×œ-business value (×œ××” ×–×” ×—×©×•×‘ ×œ×œ×§×•×—?)

### **××—×¨×™ ×”×¤×’×™×©×”**
- [ ] ×©×œ×— ××ª ×”××¡××š ×”×–×” ×›×¡×™×›×•×
- [ ] ×¨×©×•× ×©××œ×•×ª ×©×œ× ×™×“×¢×ª ×œ×¢× ×•×ª ×¢×œ×™×”×Ÿ
- [ ] ×¢×“×›×Ÿ ××ª ×”××¡××š ×œ×¤×™ feedback
- [ ] ×”×•×¡×£ ×˜×¡×˜×™× × ×•×¡×¤×™× ×× ×”×¦×™×¢×•

---

## ğŸ“ Key Takeaways - ××¡×¨×™× ××¨×›×–×™×™×

### **1. Coverage (×›×™×¡×•×™ ××§×™×£)**
```
âœ… Infrastructure: Collections, Indexes, Connectivity
âœ… Schema: Field types, Document structure
âœ… Data Quality: Completeness, Integrity
âœ… Lifecycle: Historical, Live, Deleted classification
âœ… Performance: Index validation, Query optimization
```

### **2. Automation (××•×˜×•××¦×™×” ××œ××”)**
```
âœ… 100% automated tests
âœ… Integrated in CI/CD pipeline
âœ… Fast execution (5-7 minutes total)
âœ… Clear error messages with fix suggestions
âœ… Pytest markers for selective execution
```

### **3. Business Value (×¢×¨×š ×¢×¡×§×™)**
```
âœ… Prevents production incidents (data corruption, crashes)
âœ… Ensures performance (indexes = 100x faster queries)
âœ… Enables forensics (historical data integrity)
âœ… Reduces debug time (clear assertions + logging)
âœ… Builds customer trust (reliable system)
```

### **4. Best Practices (×©×™×˜×•×ª ×¢×‘×•×“×” ××•××œ×¦×•×ª)**
```
âœ… Production-grade code (PEP8, type hints, docstrings)
âœ… Comprehensive assertions (not just status codes)
âœ… Detailed logging (context for debugging)
âœ… Fixtures for reusability (DRY principle)
âœ… Clear test naming (descriptive, purposeful)
```

---

## ğŸ“ ××™×© ×§×©×¨ ×•××©××‘×™×

### **×§×‘×¦×™× ×¨×œ×•×•× ×˜×™×™×**
```
tests/integration/api/test_historic_playback_flow.py
tests/integration/infrastructure/test_mongodb_data_quality.py
tests/integration/infrastructure/test_postgres_connectivity.py
config/environments.yaml
```

### **×¤×§×•×“×•×ª ×©×™××•×©×™×•×ª**
```bash
# Run all tests
pytest -v

# Run MongoDB tests only
pytest -m mongodb -v

# Run critical tests only
pytest -m critical -v

# Run specific test
pytest tests/integration/infrastructure/test_mongodb_data_quality.py::TestMongoDBDataQuality::test_required_collections_exist -v

# Run with detailed logging
pytest -v -s --log-cli-level=INFO

# Generate HTML report
pytest --html=report.html --self-contained-html
```

### **Monitoring & Alerts**
```python
# ×× ×˜×¡×˜ × ×›×©×œ ×‘CI/CD:
1. Check pytest output for exact error
2. Look at assertion message (contains fix suggestion)
3. Check logs in artifacts
4. If infrastructure issue â†’ check MongoDB/Postgres status
5. If data issue â†’ check recent deployments/migrations
```

---

## ğŸ† ×¡×™×›×•× ×¡×•×¤×™

**××ª×” ××›×•×¡×” ×œ×—×œ×•×˜×™×Ÿ ×œ×¤×’×™×©×”!**

×™×© ×œ×š:
- âœ… **× ×™×ª×•×— ××¤×•×¨×˜** ×©×œ ×›×œ 13 ×˜×¡×˜×™×
- âœ… **×ª×©×•×‘×•×ª ××•×›× ×•×ª** ×œ×›×œ ×©××œ×” ××¤×©×¨×™×ª
- âœ… **×§×•×“ ×œ×“×•×’××”** ×œ×™×™×©×•×
- âœ… **×”×¡×‘×¨×™× ×¢×¡×§×™×™×** ×œ××” ×›×œ ×˜×¡×˜ × ×—×™×¥
- âœ… **××¡×˜×¨×˜×’×™×•×ª** ×œ× ×™×”×•×œ ×˜×¡×˜×™×
- âœ… **Troubleshooting guides** ×œ×›×©×œ×™×

**×”××¤×ª×— ×œ×”×¦×œ×—×”**:
1. **×”×‘×Ÿ ××ª ×”-"×œ××”"** - ×œ× ×¨×§ ×”-"××”"
2. **×“×‘×¨ ×‘×©×¤×” ×¢×¡×§×™×ª** - ×œ× ×¨×§ ×˜×›× ×™×ª
3. **×”×¨××” ×§×•×“** - ××œ ×ª×¡×ª×¤×§ ×‘×”×¡×‘×¨×™×
4. **×ª×Ÿ ×“×•×’×××•×ª** - ××ª×¨×—×™×©×™× ×××™×ª×™×™×
5. **×”×™×” ×‘×™×˜×—×•×Ÿ** - ××ª×” ××›×™×¨ ××ª ×”×—×•××¨!

---

**×‘×”×¦×œ×—×” ×‘×¤×’×™×©×”! ğŸš€**

*×”××¡××š ×”×–×” × ×•×¦×¨ ×‘××™×•×—×“ ×¢×‘×•×¨×š ×•××›×¡×” ×›×œ ×¤×¨×˜ ×§×˜×Ÿ.*  
*×× ×™×© ×©××œ×•×ª × ×•×¡×¤×•×ª ×œ×¤× ×™ ×”×¤×’×™×©×” - ×ª×’×™×“!*


